import{_ as i,c as l,a0 as e,o as t}from"./chunks/framework.OqxnQCTf.js";const r="/llm-study/assets/img.CyP8Bmrj.png",n="/llm-study/assets/img_1.svqVYqCs.png",b=JSON.parse('{"title":"RAG","description":"","frontmatter":{},"headers":[],"relativePath":"AI场景/知识库/RAG/README.md","filePath":"AI场景/知识库/RAG/README.md"}'),o={name:"AI场景/知识库/RAG/README.md"};function h(s,a,d,p,u,c){return t(),l("div",null,a[0]||(a[0]=[e(`<h1 id="rag" tabindex="-1">RAG <a class="header-anchor" href="#rag" aria-label="Permalink to &quot;RAG&quot;">​</a></h1><p>当我们将大模型应用于实际业务场景时会发现，通用的基础大模型基本无法满足实际业务需求，主要有以下几方面原因：</p><ul><li>知识的局限性：模型自身的知识完全源于它的训练数据，而现有的主流大模型（ChatGPT、文心一言、通义千问…）的训练集基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备。</li><li>幻觉问题：所有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种幻觉问题的区分是比较困难的，因为它要求使用者自身具备相应领域的知识。</li><li>数据安全性：对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身的私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍。</li></ul><p>而RAG是解决上述问题的有效方案，RAG（检索增强生成）通过将检索与生成结合，突破了传统语言模型的局限</p><hr><p>一句话总结：</p><p>RAG（中文为检索增强生成） = 检索技术 + LLM 提示</p><p>RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答</p><hr><p>完整的RAG应用流程主要包含两个阶段：</p><ul><li>数据准备阶段：数据提取——&gt;文本分割——&gt;向量化（embedding）——&gt;数据入库</li><li>应用阶段：用户提问——&gt;数据检索（召回）——&gt;注入Prompt——&gt;LLM生成答案</li></ul><h2 id="数据准备阶段" tabindex="-1">数据准备阶段 <a class="header-anchor" href="#数据准备阶段" aria-label="Permalink to &quot;数据准备阶段&quot;">​</a></h2><p>数据准备一般是一个离线的过程，主要是将私域数据向量化后构建索引并存入数据库的过程。主要包括：数据提取、文本分割、向量化、数据入库等环节</p><h3 id="数据提取" tabindex="-1">数据提取 <a class="header-anchor" href="#数据提取" aria-label="Permalink to &quot;数据提取&quot;">​</a></h3><ul><li>数据加载：包括多格式数据加载、不同数据源获取等，根据数据自身情况，将数据处理为同一个范式。</li><li>数据处理：包括数据过滤、压缩、格式化等。</li><li>元数据获取：提取数据中关键信息，例如文件名、Title、时间等 。</li></ul><h3 id="文本分割" tabindex="-1">文本分割 <a class="header-anchor" href="#文本分割" aria-label="Permalink to &quot;文本分割&quot;">​</a></h3><ol><li>文本分割主要考虑两个因素：</li><li>embedding模型的Tokens限制情况；</li><li>语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下：</li></ol><ul><li>句分割：以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。</li><li>固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度（例如256/512个tokens），这种切分方式会损失很多语义信息，一般通过在头尾增加一定冗余量来缓解。</li></ul><h3 id="向量化-embedding" tabindex="-1">向量化（embedding） <a class="header-anchor" href="#向量化-embedding" aria-label="Permalink to &quot;向量化（embedding）&quot;">​</a></h3><p>向量化是一个将文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果。</p><p>目前常见的embedding模型如下所示，这些embedding模型基本能满足大部分需求，但对于特殊场景（例如涉及一些罕见专有词或字等）或者想进一步优化效果，则可以选择开源Embedding模型微调或直接训练适合自己场景的Embedding模型。</p><ul><li>ChatGPT-Embedding</li><li>ERNIE-Embedding V1</li><li>M3E</li><li>BGE</li></ul><h3 id="数据入库" tabindex="-1">数据入库 <a class="header-anchor" href="#数据入库" aria-label="Permalink to &quot;数据入库&quot;">​</a></h3><p>数据向量化后构建索引，并写入数据库的过程可以概述为数据入库过程，适用于RAG场景的数据库包括：<code>FAISS</code>、<code>Chromadb</code>、<code>ES</code>、<code>milvus</code>等。一般可以根据业务场景、硬件、性能需求等多因素综合考虑，选择合适的数据库。</p><h2 id="应用阶段" tabindex="-1">应用阶段 <a class="header-anchor" href="#应用阶段" aria-label="Permalink to &quot;应用阶段&quot;">​</a></h2><p>在应用阶段，可以根据用户的提问，通过高效的检索方法，召回与提问最相关的知识，并融入Prompt</p><p>大模型参考当前提问和相关知识，生成相应的答案</p><p>关键环节包括：数据检索、注入Prompt等</p><h3 id="数据检索" tabindex="-1">数据检索 <a class="header-anchor" href="#数据检索" aria-label="Permalink to &quot;数据检索&quot;">​</a></h3><p>常见的数据检索方法包括：相似性检索、全文检索等，根据检索效果，一般可以选择多种检索方式融合，提升召回率。</p><ul><li>相似性检索：即计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括：余弦相似性、欧氏距离、曼哈顿距离等。</li><li>全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。</li></ul><h3 id="注入prompt" tabindex="-1">注入Prompt <a class="header-anchor" href="#注入prompt" aria-label="Permalink to &quot;注入Prompt&quot;">​</a></h3><p>在RAG场景中，Prompt一般包括任务描述、背景知识（检索得到）、任务指令（一般是用户提问）等 ，根据任务场景和大模型性能，也可以在Prompt中适当加入其他指令优化大模型的输出</p><p>一个简单知识问答场景的Prompt如下所示：</p><div class="language-markdown vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">markdown</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">【任务描述】</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">假如你是一个专业的客服机器人，请参考【背景知识】，回答【问题】</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">【背景知识】</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{content} // 数据检索得到的相关文本</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">【问题】</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">石头扫地机器人P10的续航时间是多久？</span></span></code></pre></div><h1 id="高级rag技术" tabindex="-1">高级RAG技术 <a class="header-anchor" href="#高级rag技术" aria-label="Permalink to &quot;高级RAG技术&quot;">​</a></h1><p><img src="`+r+'" alt="img.png"></p><h2 id="_1-分块-向量化" tabindex="-1">1. 分块 &amp; 向量化 <a class="header-anchor" href="#_1-分块-向量化" aria-label="Permalink to &quot;1. 分块 &amp; 向量化&quot;">​</a></h2><ul><li>分块：对数据进行分块————将初始文档拆分为一定大小的块，而不会失去其含义。有许多文本拆分器实现能够完成此任务，块的大小取决于所使用的嵌入模型以及模型需要使用 token 的容量。如基于 BERT 的句子转换器，最多需要 512 个 token，OpenAI ada-002 能够处理更长的序列，如 8191 个 token，但这里的折衷是 LLM 有足够的上下文来推理</li><li>向量化：选择一个搜索优化的模型来嵌入块。有很多选项，比如 bge-large 或 E5 嵌入系列</li></ul><h2 id="_2-搜索索引" tabindex="-1">2. 搜索索引 <a class="header-anchor" href="#_2-搜索索引" aria-label="Permalink to &quot;2. 搜索索引&quot;">​</a></h2><h3 id="_2-1-向量存储索引" tabindex="-1">2.1 向量存储索引 <a class="header-anchor" href="#_2-1-向量存储索引" aria-label="Permalink to &quot;2.1 向量存储索引&quot;">​</a></h3><ul><li>向量存储索引：为了实现1w+元素规模的高效检索，搜索索引应该采用向量索引，比如 faiss、nmslib 以及 annoy。这些工具基于近似最近邻居算法，如聚类、树结构或HNSW算法（LlamaIndex 支持多种向量存储索引，同时也兼容其他简单的索引类型，如列表索引、树索引和关键词表索引）</li><li>分层索引：在大型数据库的情况下，一个有效的方法是创建两个索引——一个由摘要组成，另一个由文档块组成，然后分两步进行搜索，首先通过摘要过滤掉相关文档，然后只在这个相关组内搜索</li></ul><h3 id="_2-2-假设性问题和-hyde" tabindex="-1">2.2. 假设性问题和 HyDE <a class="header-anchor" href="#_2-2-假设性问题和-hyde" aria-label="Permalink to &quot;2.2. 假设性问题和 HyDE&quot;">​</a></h3><ul><li>假设性问题：让 LLM 为每个块生成一个问题，并将这些问题嵌入到向量中，在运行时对这个问题向量的索引执行查询搜索（将块向量替换为索引中的问题向量），然后在检索后路由到原始文本块并将它们作为 LLM 获取答案的上下文发送</li><li>HyDE：要求 LLM 在给定查询的情况下生成一个假设的响应，然后将其向量与查询向量一起使用来提高搜索质量</li></ul><h3 id="_2-3-内容增强" tabindex="-1">2.3. 内容增强 <a class="header-anchor" href="#_2-3-内容增强" aria-label="Permalink to &quot;2.3. 内容增强&quot;">​</a></h3><p>将相关的上下文组合起来供 LLM 推理，获得更好的搜索质量</p><ul><li>一种是围绕较小的检索块的句子扩展上下文</li><li>另一种是首先在检索过程中获取较小的块，然后如果前 k 个检索到的块中有超过 n 个块链接到同一个父节点（较大的块），将这个父节点替换成给 LLM 的上下文——工作原理类似于自动将一些检索到的块合并到一个更大的父块中</li></ul><h3 id="_2-4-融合检索或混合搜索" tabindex="-1">2.4. 融合检索或混合搜索： <a class="header-anchor" href="#_2-4-融合检索或混合搜索" aria-label="Permalink to &quot;2.4. 融合检索或混合搜索：&quot;">​</a></h3><p>结合传统的基于关键字的搜索（稀疏检索算法，如 tf-idf 或搜索行业标准 BM25）和现代语义或向量搜索，并将其结果组合在一个检索结果中</p><h2 id="_3-重排-reranking-和过滤-filtering" tabindex="-1">3. 重排（reranking）和过滤（filtering） <a class="header-anchor" href="#_3-重排-reranking-和过滤-filtering" aria-label="Permalink to &quot;3. 重排（reranking）和过滤（filtering）&quot;">​</a></h2><p>使用上述任何算法获得了检索结果，然后通过过滤、重排或一些转换来完善</p><blockquote><p>在 LlamaIndex 中，有各种可用的后处理器，根据相似性分数、关键字、元数据过滤掉结果，或使用其他模型（如 LLM）、sentence-transformer 交叉编码器，Cohere 重新排名接口或者基于元数据重排它们</p></blockquote><p>将检索到的上下文提供给 LLM 以获得结果答案之前的最后一步</p><hr><blockquote><p>接下来，将讨论更高级的 RAG 技术，比如查询转换和路由。这些技术涉及到大语言模型的使用，代表了一种更复杂的逻辑思维——在 RAG 流程中融合了 LLM 的推理能力</p></blockquote><h2 id="_4-查询转换" tabindex="-1">4. 查询转换 <a class="header-anchor" href="#_4-查询转换" aria-label="Permalink to &quot;4. 查询转换&quot;">​</a></h2><p>查询转换是一系列技术，使用 LLM 作为推理引擎来修改用户输入以提高检索质量 <img src="'+n+'" alt="img_1.png"></p><p><strong>对于复杂的查询，大语言模型能够将其拆分为多个子查询。</strong> 比如，</p><ul><li>当你问：“在 Github 上，Langchain 和 LlamaIndex 这两个框架哪个更受欢迎？”</li></ul><p>一般不太可能直接在语料库找到它们的比较，所以将这个问题分解为两个更简单、具体的合理的子查询：</p><ul><li>&quot;Langchain 在 Github 上有多少星？&quot;</li><li>&quot;Llamaindex 在 Github 上有多少星？&quot; 这些子查询会并行执行，检索到的信息随后被汇总到一个 LLM 提示词中</li></ul><h2 id="_5-聊天引擎" tabindex="-1">5. 聊天引擎 <a class="header-anchor" href="#_5-聊天引擎" aria-label="Permalink to &quot;5. 聊天引擎&quot;">​</a></h2><p>通过查询压缩技术解决的，将聊天上下文与用户查询一起考虑在内</p><h2 id="_6-查询路由" tabindex="-1">6. 查询路由 <a class="header-anchor" href="#_6-查询路由" aria-label="Permalink to &quot;6. 查询路由&quot;">​</a></h2><p>定义查询路由器包括设置它可以做出的选择，如果是涉及到关联操作，这些查询还可能被发送到子链或其他智能体</p><p>通常是总结、对某些数据索引执行搜索或尝试许多不同的路由，然后将它们的输出综合到一个答案中</p><blockquote><p>比如文档智能体都有两个工具：向量存储索引和摘要索引，它根据路由查询决定使用哪一个。对于顶级智能体来说，所有文档智能体都是其工具</p></blockquote><h2 id="_7-响应合成" tabindex="-1">7. 响应合成 <a class="header-anchor" href="#_7-响应合成" aria-label="Permalink to &quot;7. 响应合成&quot;">​</a></h2><p>响应合成的主要方法有：</p><ul><li>通过将检索到的上下文逐块发送到 LLM 来优化答案</li><li>概括检索到的上下文，以适应提示</li><li>根据不同的上下文块生成多个答案，然后将它们连接或概括起来</li></ul><h2 id="未来发展方向" tabindex="-1">未来发展方向 <a class="header-anchor" href="#未来发展方向" aria-label="Permalink to &quot;未来发展方向&quot;">​</a></h2><ul><li><strong>多模态RAG</strong>：结合图像、音频等数据进行检索和生成</li><li><strong>高效检索</strong>：优化检索速度，适应实时应用</li><li><strong>自适应知识库</strong>：让模型自动识别并且补充知识库</li></ul><h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><ol><li><a href="https://zhuanlan.zhihu.com/p/675509396" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/675509396</a></li></ol>',74)]))}const g=i(o,[["render",h]]);export{b as __pageData,g as default};
