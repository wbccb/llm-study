import{_ as e,c as t,a0 as l,o as r}from"./chunks/framework.OqxnQCTf.js";const m=JSON.parse('{"title":"内容","description":"","frontmatter":{},"headers":[],"relativePath":"基础原理/AI框架&微调&底层原理/3.AI应用开发&微调.md","filePath":"基础原理/AI框架&微调&底层原理/3.AI应用开发&微调.md"}'),i={name:"基础原理/AI框架&微调&底层原理/3.AI应用开发&微调.md"};function o(n,a,s,h,c,d){return r(),t("div",null,a[0]||(a[0]=[l('<h1 id="内容" tabindex="-1">内容 <a class="header-anchor" href="#内容" aria-label="Permalink to &quot;内容&quot;">​</a></h1><ol><li>基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤；</li><li>针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等；</li><li>开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等；</li><li>开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。</li></ol><h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><ol><li><a href="https://github.com/datawhalechina/self-llm" target="_blank" rel="noreferrer">https://github.com/datawhalechina/self-llm</a></li></ol>',4)]))}const L=e(i,[["render",o]]);export{m as __pageData,L as default};
