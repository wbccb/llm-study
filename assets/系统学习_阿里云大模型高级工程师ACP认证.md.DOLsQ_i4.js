import{_ as l,c as e,a0 as s,j as i,o as n}from"./chunks/framework.OqxnQCTf.js";const E=JSON.parse('{"title":"阿里云大模型高级工程师ACP认证","description":"","frontmatter":{},"headers":[],"relativePath":"系统学习/阿里云大模型高级工程师ACP认证.md","filePath":"系统学习/阿里云大模型高级工程师ACP认证.md"}'),t={name:"系统学习/阿里云大模型高级工程师ACP认证.md"};function h(p,a,r,k,o,d){return n(),e("div",null,a[0]||(a[0]=[s('<h1 id="阿里云大模型高级工程师acp认证" tabindex="-1">阿里云大模型高级工程师ACP认证 <a class="header-anchor" href="#阿里云大模型高级工程师acp认证" aria-label="Permalink to &quot;阿里云大模型高级工程师ACP认证&quot;">​</a></h1><blockquote><p><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning" target="_blank" rel="noreferrer">https://github.com/AlibabaCloudDocs/aliyun_acp_learning</a></p></blockquote><h2 id="_2-1-大模型基础知识" tabindex="-1">2.1 大模型基础知识 <a class="header-anchor" href="#_2-1-大模型基础知识" aria-label="Permalink to &quot;2.1 大模型基础知识&quot;">​</a></h2><p>大模型的问答工作流程</p><ul><li>输入文本Token化</li><li>Token向量化</li><li>大模型推理</li><li>输出Token：影响大模型生成内容的参数<code>temperature</code>和<code>top_p</code></li><li>输出文本</li></ul><h3 id="_2-1-1-temperature-调整候选token集合的概率分布" tabindex="-1">2.1.1 temperature 调整候选Token集合的概率分布 <a class="header-anchor" href="#_2-1-1-temperature-调整候选token集合的概率分布" aria-label="Permalink to &quot;2.1.1 temperature 调整候选Token集合的概率分布&quot;">​</a></h3><ul><li>想要明确的答案：temperature设置越小</li><li>想要创意多样的答案：temperature设置越大</li></ul><h3 id="_2-1-2-top-p-控制候选token集合的采样范围" tabindex="-1">2.1.2 top_p 控制候选Token集合的采样范围 <a class="header-anchor" href="#_2-1-2-top-p-控制候选token集合的采样范围" aria-label="Permalink to &quot;2.1.2 top_p 控制候选Token集合的采样范围&quot;">​</a></h3><ul><li>top_p越大：候选范围越广，适合多样性答案</li><li>top_p越小：候选范围越小，适合代码生成需要明确答案的场景</li><li>top_p取极小值：理论上模型只选择概率最高的token，输出非常稳定</li></ul><h3 id="_2-1-3-上下文工程-context-engineering" tabindex="-1">2.1.3 上下文工程（Context Engineering） <a class="header-anchor" href="#_2-1-3-上下文工程-context-engineering" aria-label="Permalink to &quot;2.1.3 上下文工程（Context Engineering）&quot;">​</a></h3><ul><li>RAG</li><li>Prompt</li><li>Tool</li><li>Memory</li></ul><h2 id="_2-3-prompt" tabindex="-1">2.3 Prompt <a class="header-anchor" href="#_2-3-prompt" aria-label="Permalink to &quot;2.3 Prompt&quot;">​</a></h2><ul><li>任务目标：明确要求大模型完成什么任务，让大模型专注具体目标</li><li>上下文：任务的背景信息，比如操作流程、任务场景等，明确大模型理解讨论的范围</li><li>角色：大模型扮演的角色，或者强调大模型应该使用的语气、写作风格等，明确大模型回应的预期情感</li><li>受众：明确大模型针对的特定受众，约束大模型的应答风格</li><li>样例：让大模型参考的具体案例，大模型会从中抽象出实现方案、需要注意的具体格式等信息</li><li>输出格式：明确指定输出的格式、输出类型、枚举值的范围。通常也会明确指出不需要输出的内容和不期望的信息，可以结合样例来进一步明确输出的格式和输出方法</li></ul><h2 id="_2-4-rag自动化评测" tabindex="-1">2.4 RAG自动化评测 <a class="header-anchor" href="#_2-4-rag自动化评测" aria-label="Permalink to &quot;2.4 RAG自动化评测&quot;">​</a></h2><p>评估纬度：</p><ul><li>召回质量：RAG系统是否检索到了正确且相关的文档片段</li><li>答案忠实度：生成的答案是否完全基于检索到的上下文，没有幻觉</li><li>答案相关性：生成的答案是否准确回答了用户的问题</li><li>上下文利用率/效率：大模型是否有效利用了所有提供给它的上下文信息</li></ul><h3 id="_2-4-1-流行的rag评测框架" tabindex="-1">2.4.1 流行的RAG评测框架 <a class="header-anchor" href="#_2-4-1-流行的rag评测框架" aria-label="Permalink to &quot;2.4.1 流行的RAG评测框架&quot;">​</a></h3><h4 id="_2-4-1-1-ragas" tabindex="-1">2.4.1.1 Ragas <a class="header-anchor" href="#_2-4-1-1-ragas" aria-label="Permalink to &quot;2.4.1.1 Ragas&quot;">​</a></h4><p>在评测时，Ragas 会调用一个大模型作为评测专家来阅读你的问题、RAG检索到的上下文和生成的答案，然后根据预设的指标给出分数。Ragas的评估指标高度契合RAG系统的痛点，主要包括：</p><ul><li>整体回答质量的评估： <ul><li>Answer Correctness，用于评估 RAG 应用生成答案的准确度。</li></ul></li><li>生成环节的评估： <ul><li>Answer Relevancy，用于评估 RAG 应用生成的答案是否与问题相关。</li><li>Faithfulness，用于评估 RAG 应用生成的答案和检索到的参考资料的事实一致性。</li></ul></li><li>召回阶段的评估： <ul><li>Context Precision，用于评估 contexts 中与准确答案相关的条目是否排名靠前、占比高（信噪比）。</li><li>Context Recall，用于评估有多少相关参考资料被检索到，越高的得分意味着更少的相关参考资料被遗漏。</li></ul></li></ul><h4 id="_2-4-1-2-trulens" tabindex="-1">2.4.1.2 TruLens <a class="header-anchor" href="#_2-4-1-2-trulens" aria-label="Permalink to &quot;2.4.1.2 TruLens&quot;">​</a></h4><p>专注于评估的<code>可观测性</code></p><ul><li>告诉你RAG哪里可能有问题</li><li>还能帮你追溯RAG整个运行过程，包括每次运行的输入、输出、中间步骤、调用的大模型、检索召回的上下文等等；并提供一系列<code>反馈函数</code>来自动化评估这些运行</li></ul><p>可以和<code>langChain</code>、<code>LlamaIndex</code>无缝集成，并提供一套可视化工具，展示每次RAG调用的详细过程</p><p>提供细粒度的反馈函数，从多个角度评估RAG性能，帮助你快速定位问题，评估指标包括：</p><ul><li>Context Relevance：评估召回的知识是否跟问题相关。</li><li>Groundedness：生成的答案是否是来自于检索召回的知识</li><li>Answer Relevance：生成的答案是否与问题相关</li></ul><h4 id="_2-4-1-3-deepeval" tabindex="-1">2.4.1.3 DeepEval <a class="header-anchor" href="#_2-4-1-3-deepeval" aria-label="Permalink to &quot;2.4.1.3 DeepEval&quot;">​</a></h4><p>将传统的软件开发测试理念引入到RAG应用中，在RAG功能开发前就编写评估测试，采用单元测试和测试驱动开发TDD的思想，提供一个专门的测试框架，让你能够编写单元测试一样，为RAG系统创建LLM评估测试</p><p>支持为每个测试用例设置明确的通过/失败的阀值，轻松集成到你的CI/CD流程，从而实现自动化回归测试</p><h4 id="_2-4-1-4-其他的开源评测工具" tabindex="-1">2.4.1.4 其他的开源评测工具 <a class="header-anchor" href="#_2-4-1-4-其他的开源评测工具" aria-label="Permalink to &quot;2.4.1.4 其他的开源评测工具&quot;">​</a></h4><p>比如LlamaIndex和LangChain这些主流RAG开发框架本身也有内置评估工具</p><h4 id="_2-4-1-5-自定义评测框架" tabindex="-1">2.4.1.5 自定义评测框架 <a class="header-anchor" href="#_2-4-1-5-自定义评测框架" aria-label="Permalink to &quot;2.4.1.5 自定义评测框架&quot;">​</a></h4><p>在某些特定场景下，你可能需要更灵活、更贴合业务的评估方式，这时你也可以选择自定义评测框架</p><blockquote><p>请注意：领域专家的深度参与是评测系统乃至AI应用成功的关键。使用自动化评测框架并非要让机器彻底取代人工判断，而是旨在为评测提效。前面介绍的许多自动化框架（如Ragas）会利用大模型来充当“评委”，对RAG表现进行初步判断。然而，只有领域专家才能提供那些最宝贵的、真正反映业务需求的高价值“正确答案”（Ground Truth），并对用户提问和RAG回答的正确性进行权威性的审定。</p></blockquote><p>案例一：当用户询问“报销的流程是什么？”，你们的业务专家可能会要求“好的报销答案，必须明确包含报销单链接和二级主管审批信息。”或者“好的答案，必须引导用户到费用系统提交报销申请。”，这样才能满足“流程合规”的要求。</p><h4 id="_2-4-1-6-评测示例讲解" tabindex="-1">2.4.1.6 评测示例讲解 <a class="header-anchor" href="#_2-4-1-6-评测示例讲解" aria-label="Permalink to &quot;2.4.1.6 评测示例讲解&quot;">​</a></h4><blockquote><p><a href="https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_4_%E8%87%AA%E5%8A%A8%E5%8C%96%E8%AF%84%E6%B5%8B%E7%AD%94%E7%96%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0.ipynb" target="_blank" rel="noreferrer">https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/大模型ACP认证教程/p2_构造大模型问答系统/2_4_自动化评测答疑机器人的表现.ipynb</a></p></blockquote><h2 id="_2-5-评测不理想-优化rag具体步骤" tabindex="-1">2.5 评测不理想-&gt;优化RAG具体步骤 <a class="header-anchor" href="#_2-5-评测不理想-优化rag具体步骤" aria-label="Permalink to &quot;2.5 评测不理想-&gt;优化RAG具体步骤&quot;">​</a></h2><h3 id="_2-5-1-切片方式的优化" tabindex="-1">2.5.1. 切片方式的优化 <a class="header-anchor" href="#_2-5-1-切片方式的优化" aria-label="Permalink to &quot;2.5.1. 切片方式的优化&quot;">​</a></h3><ul><li>如果你刚开始接触 RAG，建议先使用默认的句子切片方法，它在大多数场景下都能提供不错的效果</li><li>当你发现检索结果不够理想时，可以尝试： <ul><li>处理长文档且需要保持上下文？试试句子窗口切片</li><li>文档逻辑性强、内容专业？语义切片可能会有帮助</li><li>模型总是报 Token 超限？Token 切片可以帮你精确控制</li><li>处理 Markdown 文档？别忘了有专门的 Markdown 切片</li></ul></li></ul><h3 id="_2-5-2-embedding模型" tabindex="-1">2.5.2. Embedding模型 <a class="header-anchor" href="#_2-5-2-embedding模型" aria-label="Permalink to &quot;2.5.2. Embedding模型&quot;">​</a></h3><ul><li>新版本的Embedding模型通常能带来更好的效果（如text-embedding-v3比v2表现更好）</li><li>在实践中，单纯升级Embedding模型就可能显著提升检索质量</li></ul><h3 id="_2-5-3-embedding存储数据库" tabindex="-1">2.5.3. Embedding存储数据库 <a class="header-anchor" href="#_2-5-3-embedding存储数据库" aria-label="Permalink to &quot;2.5.3. Embedding存储数据库&quot;">​</a></h3><ul><li>开发测试时使用内存向量存储</li><li>小规模应用可以使用本地向量数据库</li><li>生产环境推荐使用云服务，可根据具体需求选择合适的服务类型</li></ul><h3 id="_2-5-4-检索召回阶段" tabindex="-1">2.5.4. 检索召回阶段 <a class="header-anchor" href="#_2-5-4-检索召回阶段" aria-label="Permalink to &quot;2.5.4. 检索召回阶段&quot;">​</a></h3><ul><li>在执行检索前，很多用户问题描述是不完整、甚至有歧义的，你需要想办法还原用户真实意图，以便提升检索效果。</li><li>在执行检索后，你可能会发现存在一些无关的信息，需要想办法减少无关信息，避免干扰下一步的答案生成。</li></ul><h4 id="_2-5-4-1-问题改写" tabindex="-1">2.5.4.1 问题改写 <a class="header-anchor" href="#_2-5-4-1-问题改写" aria-label="Permalink to &quot;2.5.4.1 问题改写&quot;">​</a></h4><ul><li>使用大模型扩充用户问题</li><li>将单一查询改写为多步骤查询</li><li>用假设文档来增强检索（HyDE）</li></ul><blockquote><p>在向量检索的基础上，我们还可以添加标签过滤来提升检索精度。这种方式类似于图书馆既有书名检索，又有分类编号系统，能让检索更精准</p></blockquote><h4 id="_2-5-4-2-标签提取有两个关键场景" tabindex="-1">2.5.4.2 标签提取有两个关键场景 <a class="header-anchor" href="#_2-5-4-2-标签提取有两个关键场景" aria-label="Permalink to &quot;2.5.4.2 标签提取有两个关键场景&quot;">​</a></h4><ul><li>建立索引时，从文档切片中提取结构化标签</li><li>检索时，从用户问题中提取对应的标签进行过滤</li></ul><p>当我们建立索引时，可以将这些标签与文档切片一起存储。这样在检索时，比如用户问&quot;张伟是哪个部门的&quot;，我们可以：</p>',52),i("ul",null,[i("li",{"key:":"","人名,":"","value:":"",张伟:""},"从问题中提取人名标签"),i("li",null,'先用标签过滤出所有包含"张伟"的文档切片'),i("li",null,"再用向量相似度检索找出最相关的内容")],-1),s(`<p>这种&quot;标签过滤+向量检索&quot;的组合方式，能大幅提升检索的准确性。特别是在处理结构化程度较高的企业文档时，这个方法效果更好。</p><h4 id="_2-5-4-3-重排序" tabindex="-1">2.5.4.3 重排序 <a class="header-anchor" href="#_2-5-4-3-重排序" aria-label="Permalink to &quot;2.5.4.3 重排序&quot;">​</a></h4><p>增加召回数量，利用重排序获取分数更高的片段</p><h4 id="_2-5-4-4-生成答案阶段" tabindex="-1">2.5.4.4 生成答案阶段 <a class="header-anchor" href="#_2-5-4-4-生成答案阶段" aria-label="Permalink to &quot;2.5.4.4 生成答案阶段&quot;">​</a></h4><p>即使上面几个步骤都优化好了，你仍然会遇到：</p><ul><li>没有检索到相关信息，大模型捏造答案。</li><li>检索到了相关信息，但是大模型没有按照要求生成答案。</li><li>检索到了相关信息，大模型也给出了答案，但是你希望 AI 给出更全面的答案。</li></ul><p>这个能做的是：</p><ul><li>选择合适的大模型</li><li>充分优化prompt内容</li><li>调整大模型参数</li><li>调优大模型</li></ul><blockquote><p>GraphRAG 技术巧妙地结合了检索增强生成（RAG）和查询聚焦摘要（QFS）的优点，为处理大规模文本数据提供了一个强大的解决方案。它把两种技术的特长融合在一起：RAG 擅长找出精确的细节信息，而 QFS 则更善于理解和总结文章的整体内容。通过这种结合，GraphRAG 既能准确回答具体问题，又能处理需要深入理解的复杂查询，特别适合用来构建智能问答系统。 如果你想深入了解如何实际运用 GraphRAG，可以参考 LlamaIndex 提供的详细教程：使用 LlamaIndex 构建 GraphRAG 应用</p></blockquote><h2 id="_2-6-agent" tabindex="-1">2.6 Agent <a class="header-anchor" href="#_2-6-agent" aria-label="Permalink to &quot;2.6 Agent&quot;">​</a></h2><p>智能体的工作原理：</p><ol><li>工具模块</li><li>记忆模块</li><li>计划能力</li><li>行动能力</li></ol><h3 id="_2-6-1-意图识别" tabindex="-1">2.6.1 意图识别 <a class="header-anchor" href="#_2-6-1-意图识别" aria-label="Permalink to &quot;2.6.1 意图识别&quot;">​</a></h3><p>对用户的问题进行意图识别后，你就可以让答疑机器人先识别问题的类型，再使用不同的提示词和工作流程来回答问题</p><ul><li>节省资源：对于检查文档错误的问题，大模型其实可以直接回复，并不需要检索参考资料，之前的实现存在资源浪费。</li><li>避免误解：之前的实现每次会检索参考资料，这些被召回的相关文本段可能会干扰大模型理解问题，导致答非所问。</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> ask_llm_route</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(question):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    question_type </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> get_question_type(question)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;问题：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">question</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">类型：</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">question_type</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    reviewer_prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    【角色背景】</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    你是文档纠错专家，负责找出文档中或网页内容的明显错误</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    【任务要求】</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    - 你需要言简意赅的回复。</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    - 如果没有明显问题，请直接回复没有问题</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    【输入如下】</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    translator_prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">   【任务要求】</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    你是一名翻译专家，你要识别不同语言的文本，并翻译为中文。</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    【输入如下】</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> question_type </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;文档审查&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> llm.invoke(reviewer_prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> question)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    elif</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> question_type </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;公司内部文档查询&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> rag.ask(question, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">query_engine</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">query_engine)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    elif</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> question_type </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;内容翻译&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> llm.invoke(translator_prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> question)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;未能识别问题类型，请重新输入。&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">query_engine </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">rag.create_query_engine(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">index</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">rag.load_index())</span></span></code></pre></div><h3 id="_2-6-2-多智能体" tabindex="-1">2.6.2 多智能体 <a class="header-anchor" href="#_2-6-2-多智能体" aria-label="Permalink to &quot;2.6.2 多智能体&quot;">​</a></h3><p>Multi-Agent系统有多种设计思路，本教程将介绍一个由</p><ul><li>一个Planner Agent：根据用户的输入内容，选择要将任务分发给哪个Agent或Agent组合完成任务</li><li>若干个负责执行工具函数的Agent：根据Planner Agent分发的任务，执行属于自己的工具函数</li><li>一个Summary Agent组成的Multi-Agent系统：根据用户的输入，以及执行工具函数的Agent的输出，生成总结并返回给用户</li></ul><p>组成的多智能体系统</p><h3 id="_2-7-微调" tabindex="-1">2.7 微调 <a class="header-anchor" href="#_2-7-微调" aria-label="Permalink to &quot;2.7 微调&quot;">​</a></h3>`,21)]))}const g=l(t,[["render",h]]);export{E as __pageData,g as default};
