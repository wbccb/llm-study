import{_ as e,c as i,a0 as l,o as r}from"./chunks/framework.OqxnQCTf.js";const u=JSON.parse('{"title":"基本概念","description":"","frontmatter":{},"headers":[],"relativePath":"基础原理/深度学习/基本概念.md","filePath":"基础原理/深度学习/基本概念.md"}'),t={name:"基础原理/深度学习/基本概念.md"};function n(o,a,d,p,c,s){return r(),i("div",null,a[0]||(a[0]=[l('<h1 id="基本概念" tabindex="-1">基本概念 <a class="header-anchor" href="#基本概念" aria-label="Permalink to &quot;基本概念&quot;">​</a></h1><h2 id="知识蒸馏" tabindex="-1">知识蒸馏 <a class="header-anchor" href="#知识蒸馏" aria-label="Permalink to &quot;知识蒸馏&quot;">​</a></h2><p>知识蒸馏通常用于 <code>大模型</code> -&gt; <code>小模型</code></p><ol><li>定义：让轻量级学生模型模仿教师模型的输出分布，实现模型压缩或者性能迁移</li><li>典型场景：将GPT-4级别的模型压缩为更小参数的版本，或者适配特定硬件（如移动端）</li></ol><h2 id="大模型时代的迁移学习技术" tabindex="-1">大模型时代的迁移学习技术 <a class="header-anchor" href="#大模型时代的迁移学习技术" aria-label="Permalink to &quot;大模型时代的迁移学习技术&quot;">​</a></h2><p>预训练-微调范式（Pretraining-Finetuning）</p><p>核心流程：</p><ul><li>预训练阶段：在大规模通用数据（如互联网文本）上训练基础模型（如BERT、GPT）。</li><li>微调阶段：在特定任务数据（如医疗问答）上更新部分参数，适配下游任务。</li></ul><p>典型应用：</p><ul><li>文本分类：BERT微调后用于新闻主题识别。</li><li>代码生成：Codex基于GPT-3在代码数据上微调</li></ul><h2 id="多模态大模型" tabindex="-1">多模态大模型 <a class="header-anchor" href="#多模态大模型" aria-label="Permalink to &quot;多模态大模型&quot;">​</a></h2><p>多模态大模型是指能够同时处理和理解多种数据模态（如文本、图像、音频、视频、传感器数据等）的大规模深度学习模型</p><p>其核心是通过统一架构（如Transformer）实现跨模态信息的对齐、融合与推理，从而模仿人类多感官协同认知的能力。</p><p>典型代表包括</p><ul><li>CLIP（OpenAI）：对齐图像与文本的跨模态表示。</li><li>DALL-E：根据文本生成高质量图像。</li><li>Flamingo（DeepMind）：融合文本与视频的对话模型</li></ul><h2 id="强化学习" tabindex="-1">强化学习 <a class="header-anchor" href="#强化学习" aria-label="Permalink to &quot;强化学习&quot;">​</a></h2><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，其核心思想是让一个智能体（agent）通过与环境（environment）的不断交互，基于获得的奖励（reward）和惩罚信号来学习如何采取行动，从而在长期内最大化累计回报（return）。这种学习过程类似于人类或动物在试错中逐步改进行为的方式</p>',17)]))}const m=e(t,[["render",n]]);export{u as __pageData,m as default};
