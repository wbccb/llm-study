import{_ as e,c as l,a0 as i,o as t}from"./chunks/framework.OqxnQCTf.js";const u=JSON.parse('{"title":"基本概念","description":"","frontmatter":{},"headers":[],"relativePath":"docs/深度学习/基本概念.md","filePath":"docs/深度学习/基本概念.md"}'),o={name:"docs/深度学习/基本概念.md"};function r(n,a,d,s,c,p){return t(),l("div",null,a[0]||(a[0]=[i('<h1 id="基本概念" tabindex="-1">基本概念 <a class="header-anchor" href="#基本概念" aria-label="Permalink to &quot;基本概念&quot;">​</a></h1><h2 id="知识蒸馏" tabindex="-1">知识蒸馏 <a class="header-anchor" href="#知识蒸馏" aria-label="Permalink to &quot;知识蒸馏&quot;">​</a></h2><p>知识蒸馏通常用于 <code>大模型</code> -&gt; <code>小模型</code></p><ol><li>定义：让轻量级学生模型模仿教师模型的输出分布，实现模型压缩或者性能迁移</li><li>典型场景：将GPT-4级别的模型压缩为更小参数的版本，或者适配特定硬件（如移动端）</li></ol><h2 id="大模型时代的迁移学习技术" tabindex="-1">大模型时代的迁移学习技术 <a class="header-anchor" href="#大模型时代的迁移学习技术" aria-label="Permalink to &quot;大模型时代的迁移学习技术&quot;">​</a></h2><p>预训练-微调范式（Pretraining-Finetuning）</p><p>核心流程：</p><ul><li>预训练阶段：在大规模通用数据（如互联网文本）上训练基础模型（如BERT、GPT）。</li><li>微调阶段：在特定任务数据（如医疗问答）上更新部分参数，适配下游任务。</li></ul><p>典型应用：</p><ul><li>文本分类：BERT微调后用于新闻主题识别。</li><li>代码生成：Codex基于GPT-3在代码数据上微调</li></ul><h2 id="多模态大模型" tabindex="-1">多模态大模型 <a class="header-anchor" href="#多模态大模型" aria-label="Permalink to &quot;多模态大模型&quot;">​</a></h2><p>多模态大模型是指能够同时处理和理解多种数据模态（如文本、图像、音频、视频、传感器数据等）的大规模深度学习模型</p><p>其核心是通过统一架构（如Transformer）实现跨模态信息的对齐、融合与推理，从而模仿人类多感官协同认知的能力。</p><p>典型代表包括</p><ul><li>CLIP（OpenAI）：对齐图像与文本的跨模态表示。</li><li>DALL-E：根据文本生成高质量图像。</li><li>Flamingo（DeepMind）：融合文本与视频的对话模型</li></ul>',15)]))}const m=e(o,[["render",r]]);export{u as __pageData,m as default};
