import socket
from imp import SEARCH_ERROR
from logging import exception

import gradio as gr
import webbrowser

import jieba
import requests
from sentence_transformers import SentenceTransformer
import chromadb
from datetime import datetime
import os
import logging
from pdfminer.high_level import extract_text_to_fp
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
import time

from rank_bm25 import BM25Okapi
import numpy as np
from dotenv import load_dotenv

import requests
from requests.adapters import HTTPAdapter
import hashlib



# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")

SEARCH_ENGINE = "google"  # å¯æ ¹æ®éœ€è¦æ”¹ä¸ºå…¶ä»–æœç´¢å¼•æ“


# æ·»åŠ é‡è¯•æ¬¡æ•°
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ é‡è¯•æ¬¡æ•°
session = requests.Session()


# LLM->ç”ŸæˆText Embeddings
EMBED_MODEL = SentenceTransformer("all-MIniLM-L6-v2")
# å‘é‡æ•°æ®åº“
CHROMA_CLIENT = chromadb.PersistentClient(
    path="./chroma_db",
    settings=chromadb.Settings(anonymized_telemetry=False)
)
## å‘é‡æ•°æ®åº“->è·å–å¯¹åº”çš„é›†åˆ
COLLECTION = CHROMA_CLIENT.get_or_create_collection("rag_docs")

## langchainæ–‡æœ¬åˆ‡å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=40,
    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ï¼š", " ", ""]  # æŒ‰è‡ªç„¶è¯­è¨€ç»“æ„åˆ†å‰²
)

# ä¿®æ”¹å¸ƒå±€éƒ¨åˆ†ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„æ ‡ç­¾é¡µ
with gr.Blocks(
        title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
        css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        width: 100vw;
        height: 100vh;
    }

    .gradio-container {
        max-width: 100% !important;
        width: 100% !important;
        margin: 0 !important;
        padding: 0 1% !important;
        color: var(--text-color);
        background-color: var(--bg-color);
        min-height: 100vh;
    }

    /* ç¡®ä¿æ ‡ç­¾å†…å®¹æ’‘æ»¡ */
    .tabs.svelte-710i53 {
        margin: 0 !important;
        padding: 0 !important;
        width: 100% !important;
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    .theme-toggle:hover {
        background: var(--hover-color);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: var(--bg-color);
        width: 100%;
    }

    .right-panel {
        height: 100vh;
        background: var(--bg-color);
        width: 100%;
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: var(--panel-bg);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: var(--panel-bg) !important;
        color: var(--text-color) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: var(--primary-color) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-1px);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 80vh;
        height: 80vh !important;
        overflow-y: auto;
        background: var(--bg-color);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
    }

    .chat-message.user {
        background: var(--chat-user-bg);
        margin-left: 32px;
        border-top-right-radius: 4px;
    }

    .chat-message.assistant {
        background: var(--chat-assistant-bg);
        margin-right: 32px;
        border-top-left-radius: 4px;
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: var(--error-color) !important;
    }

    /* APIé…ç½®æç¤ºæ ·å¼ */
    .api-info {
        margin-top: 10px;
        padding: 10px;
        border-radius: 5px;
        background: var(--panel-bg);
        border: 1px solid var(--border-color);
    }

    /* æ–°å¢: æ•°æ®å¯è§†åŒ–å¡ç‰‡æ ·å¼ */
    .model-card {
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        border: 1px solid var(--border-color);
        margin-bottom: 16px;
    }

    .model-card h3 {
        margin-top: 0;
        border-bottom: 1px solid var(--border-color);
        padding-bottom: 8px;
    }

    .model-item {
        display: flex;
        margin-bottom: 8px;
    }

    .model-item .label {
        flex: 1;
        font-weight: 500;
    }

    .model-item .value {
        flex: 2;
    }

    /* æ•°æ®è¡¨æ ¼æ ·å¼ */
    .chunk-table {
        border-radius: 8px;
        overflow: hidden;
        border: 1px solid var(--border-color);
    }

    .chunk-table th, .chunk-table td {
        border: 1px solid var(--border-color);
        padding: 8px;
    }

    .chunk-detail-box {
        min-height: 200px;
        padding: 16px;
        background: var(--panel-bg);
        border-radius: 8px;
        border: 1px solid var(--border-color);
        font-family: monospace;
        white-space: pre-wrap;
        overflow-y: auto;
    }
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

    with gr.Tabs() as tabs:
        # ç¬¬ä¸€ä¸ªé€‰é¡¹å¡ï¼šé—®ç­”å¯¹è¯
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ“ä½œé¢æ¿ - è°ƒæ•´æ¯”ä¾‹ä¸ºåˆé€‚çš„å¤§å°
                with gr.Column(scale=5, elem_classes="left-panel"):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        file_input = gr.File(
                            label="ä¸Šä¼ PDFæ–‡æ¡£",
                            file_types=[".pdf"],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False,
                            lines=2
                        )
                        file_list = gr.Textbox(
                            label="å·²å¤„ç†æ–‡ä»¶",
                            interactive=False,
                            lines=3,
                            elem_classes="file-list"
                        )

                    # å°†é—®é¢˜è¾“å…¥åŒºç§»è‡³å·¦ä¾§é¢æ¿åº•éƒ¨
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            # æ·»åŠ è”ç½‘å¼€å…³
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢",
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€é…ç½®SERPAPI_KEYï¼‰"
                            )

                            # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                            model_choice = gr.Dropdown(
                                choices=["ollama", "siliconflow"],
                                value="ollama",
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                            )

                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button",
                                                  scale=1)

                    # æ·»åŠ APIé…ç½®æç¤ºä¿¡æ¯
                    api_info = gr.HTML(
                        """
                        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
                            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
                            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
                        </div>
                        """
                    )

                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")

                    # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=600,  # å¢åŠ é«˜åº¦
                        elem_classes="chat-container",
                        show_label=False
                    )

                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)

        # ç¬¬äºŒä¸ªé€‰é¡¹å¡ï¼šåˆ†å—å¯è§†åŒ–
        with gr.TabItem("ğŸ“Š åˆ†å—å¯è§†åŒ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ’¡ ç³»ç»Ÿæ¨¡å‹ä¿¡æ¯")

                    # æ˜¾ç¤ºç³»ç»Ÿæ¨¡å‹ä¿¡æ¯å¡ç‰‡
                    models_info = get_system_models_info()
                    with gr.Group(elem_classes="model-card"):
                        gr.Markdown("### æ ¸å¿ƒæ¨¡å‹ä¸æŠ€æœ¯")

                        for key, value in models_info.items():
                            with gr.Row():
                                gr.Markdown(f"**{key}**:", elem_classes="label")
                                gr.Markdown(f"{value}", elem_classes="value")

                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ“„ æ–‡æ¡£åˆ†å—ç»Ÿè®¡")
                    refresh_chunks_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ†å—æ•°æ®", variant="primary")
                    chunks_status = gr.Markdown("ç‚¹å‡»æŒ‰é’®æŸ¥çœ‹åˆ†å—ç»Ÿè®¡")

            # åˆ†å—æ•°æ®è¡¨æ ¼å’Œè¯¦æƒ…
            with gr.Row():
                chunks_data = gr.Dataframe(
                    headers=["æ¥æº", "åºå·", "å­—ç¬¦æ•°", "åˆ†è¯æ•°", "å†…å®¹é¢„è§ˆ"],
                    elem_classes="chunk-table",
                    interactive=False,
                    wrap=True,
                    row_count=(10, "dynamic")
                )

            with gr.Row():
                chunk_detail_text = gr.Textbox(
                    label="åˆ†å—è¯¦æƒ…",
                    placeholder="ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å®Œæ•´å†…å®¹...",
                    lines=8,
                    elem_classes="chunk-detail-box"
                )

            gr.Markdown("""
            <div class="footer-note">
                * ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡Œå¯æŸ¥çœ‹è¯¥åˆ†å—çš„å®Œæ•´å†…å®¹<br>
                * åˆ†è¯æ•°è¡¨ç¤ºä½¿ç”¨jiebaåˆ†è¯åçš„tokenæ•°é‡
            </div>
            """)

    # è°ƒæ•´åçš„åŠ è½½æç¤º
    gr.HTML("""
    <div id="loading" style="text-align:center;padding:20px;">
        <h3>ğŸ”„ ç³»ç»Ÿåˆå§‹åŒ–ä¸­ï¼Œè¯·ç¨å€™...</h3>
    </div>
    """)

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)


    # å®šä¹‰å‡½æ•°å¤„ç†äº‹ä»¶
    def clear_chat_history():
        return None, "å¯¹è¯å·²æ¸…ç©º"


    def process_chat(question, history, enable_web_search, model_choice):
        if history is None:
            history = []

        # æ›´æ–°æ¨¡å‹é€‰æ‹©ä¿¡æ¯çš„æ˜¾ç¤º
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨",
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )

        # å¦‚æœé—®é¢˜ä¸ºç©ºï¼Œä¸å¤„ç†
        if not question or question.strip() == "":
            history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
            return history, "", api_text

        # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
        history.append((question, ""))

        # æ ¹æ®é—®é¢˜ + å¤šç§é€»è¾‘å¤„ç†ï¼Œå½¢æˆå¯¹åº”çš„ç­”æ¡ˆå†…å®¹
        resp_generator = stream_answer(question, enable_web_search, model_choice)

        # æµå¼æ›´æ–°å›ç­”
        for response, status in resp_generator:
            history[-1] = (question, response)
            yield history, "", api_text


    def update_api_info(enable_web_search, model_choice):
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨",
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        return api_text


    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_multiple_pdfs,
        inputs=[file_input],
        outputs=[upload_status, file_list],
        show_progress=True
    )

    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input, api_info]
    )

    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )

    # å½“åˆ‡æ¢è”ç½‘æœç´¢æˆ–æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°APIä¿¡æ¯
    web_search_checkbox.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )

    model_choice.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )

    # æ–°å¢ï¼šåˆ†å—å¯è§†åŒ–åˆ·æ–°æŒ‰é’®äº‹ä»¶
    refresh_chunks_btn.click(
        fn=get_document_chunks,
        outputs=[chunks_data, chunks_status]
    )

    # æ–°å¢ï¼šåˆ†å—è¡¨æ ¼ç‚¹å‡»äº‹ä»¶
    chunks_data.select(
        fn=show_chunk_details,
        inputs=chunks_data,
        outputs=chunk_detail_text
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');

    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');
});
"""


# chatèŠå¤©æ¡†å¤„ç†é€»è¾‘
# æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢ã€é‡æ’åº
def stream_answer(question, enable_web_search=False, model_choice="ollama", progress=gr.Progress()):
    try:
    # 1.æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦ä¸ºç©ºï¼Œä¹Ÿå°±æ˜¯å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        try:
            collect_data = COLLECTION.get(include=["documents"])
            if not collect_data or not collect_data.get("documents") or len(collect_data.get("documents")) == 0:
                if not enable_web_search:
                    yield "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                    return
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        except Exception as e:
            if not enable_web_search:
                yield f"âš ï¸ æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}ï¼Œè¯·ç¡®ä¿å·²ä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                return
            logging.error(f"æ£€æŸ¥çŸ¥è¯†åº“æ—¶å‡ºé”™: {str(e)}")

         progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
    # 2.ä½¿ç”¨é€’å½’æœç´¢è·å–æ›´åŠ å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡


    # 3.ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬æ¥æºä¿¡æ¯
    ## ä½¿ç”¨æ£€ç´¢åˆ°çš„æ•°æ®
    ## æ£€æµ‹æœç´¢çš„ç»“æ„æ˜¯å¦å­˜åœ¨çŸ›ç›¾ï¼Œä¹Ÿå°±æ˜¯æœ‰çš„æ•°æ®è¯´1+1=2ï¼›æœ‰çš„è¯´1+1=3
    ## å¦‚æœå­˜åœ¨çŸ›ç›¾ï¼Œåˆ™è¿›è¡Œæ•°æ®

    ## ä¸Šä¸‹æ–‡æ·»åŠ queryæ—¶é—´æ•æ„Ÿæ£€æµ‹
    ## æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡

    # 4.æ ¹æ®æœ¬åœ°æ¨¡å‹è¿˜æ˜¯çº¿ä¸Šæ¨¡å¼è¿›è¡Œä¸åŒAPIçš„é€‰æ‹©
    # 5.æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åŒ…å«thinkingï¼Œæ„å»ºæ€è€ƒé“¾æ•°æ®å±•ç¤º

    # 6.è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ=>æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Š

    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"


# æ£€æŸ¥æ˜¯å¦é…ç½®äº†webæœç´¢çš„ç›¸å…³å¯†é’¥
def check_serpapi_key():
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""

# ä½¿ç”¨ SerpAPI è¿›è¡Œç½‘ç»œå·¥å…·çš„ä½¿ç”¨
def serpapi_search(query: str, num_results: int = 5):
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []

# è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®ï¼Œæ•´ç†ä¸ºç»Ÿä¸€æ ¼å¼
def _parse_serpapi_results(data: dict):
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date"),
            }
            results.append(result)
    ## è¿”å›æ•°æ®å¯èƒ½å­˜åœ¨å›¾è°±ä¿¡æ¯
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("desciption"),
            "source": "knowledge_graph"
        })
    return results;

def update_web_results(query: str, num_results: int = 5) -> list:
    results = serpapi_search(query, num_results)

    if not results:
        return []

    ## åˆ é™¤æ—§çš„æ•°æ®åº“æ•°æ®
    try:
        collection_data = COLLECTION.get(include=["metadatas"])
        total_len = len(collection_data.get("ids"))
        if collection_data and "metadatas" in collection_data:
            web_ids = []
            for i, metadata in enumerate(collection_data.get("metadatas")):
                if metadata.get("source") == "web" and i < total_len:
                    # collection_data.get("metadatas")å’Œcollection_data.get("ids")å¾—åˆ°çš„æ•°æ®é•¿åº¦éƒ½æ˜¯ä¸€æ ·çš„
                    web_ids.append(collection_data.get("ids"))

            ## åˆ é™¤æ‰¾åˆ°çš„ç½‘ç»œç»“æœ
            if web_ids:
                COLLECTION.delete(ids=web_ids)
                logging.info(f"å·²åˆ é™¤ {len(web_ids)} æ¡æ—§çš„ç½‘ç»œæœç´¢ç»“æœ")

    except Exception as e:
        logging.warning(f"åˆ é™¤æ—§çš„ç½‘ç»œæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")


    ## å°†resultsæ•´ç†åæ’å…¥åˆ°æ•°æ®åº“ä¸­
    docs = []
    metadatas = []
    ids = []
    for idx, res in enumerate(results):
        text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\n æ‘˜è¦ï¼š{res.get('snippet', '')}"
        docs.append(text)

        meta = {
            "source": "web",
            "url": res.get('url', ''),
            "title": res.get('title', ''),
            "content_hash": hashlib.md5(text.encode()).hexdigest()[:8]
        }
        metadatas.append(meta)

        ids.append(f"web_{idx}")
    embeddings = EMBED_MODEL.encode(docs)
    COLLECTION.add(ids=ids, embeddings=embeddings.tolist(), documents=docs, metadatas=metadatas)
    return results



def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice="ollama"):
    """
    å®ç°é€’å½’æ£€ç´¢ä¸è¿­ä»£æŸ¥è¯¢åŠŸèƒ½
    é€šè¿‡åˆ†æå½“å‰æŸ¥è¯¢ç»“æœï¼Œç¡®å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢

    :param initial_query: åˆå§‹æŸ¥è¯¢çš„å­—ç¬¦ä¸²
    :param max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
    :param enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
    :param model_choice: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆæœ¬åœ°æˆ–è€…åœ¨çº¿ï¼‰
    :return:
        åŒ…å«æ‰€æœ‰æ£€ç´¢å†…å®¹çš„åˆ—è¡¨
    """

    query = initial_query
    all_contexts = []
    all_doc_ids = []
    all_metadata = []

    for i in range(max_iterations):
        logging.info(f"é€’å½’æ£€ç´¢è¿­ä»£ {i + 1}/{max_iterations}ï¼Œå½“å‰æŸ¥è¯¢: {query}")

        # å¦‚æœå¯åŠ¨äº†ç½‘ç»œæŸ¥è¯¢ï¼Œå…ˆè¿›è¡Œç½‘ç»œæœç´¢
        web_results = []
        if enable_web_search and check_serpapi_key():
            web_results = update_web_results(query);

        # query -> embedding

        # query embedding ä¸ å‘é‡æ•°æ®åº“ æ¯”å¯¹ï¼Œè·å–å¯¹åº”çš„è¯­ä¹‰åˆ†æç»“æœ

        # BM25å…³é”®è¯æ£€ç´¢

        # æ··åˆæ£€ç´¢ç»“æœå¤„ç†ï¼šquery embedding ä¸ å‘é‡æ•°æ®åº“ æ¯”å¯¹ + BM25å…³é”®è¯æ£€ç´¢

        # å¯¹æ‹¿åˆ°çš„ç»“æœè¿›è¡Œé‡æ’åº

        # æ”¶é›†å½“å‰æœ€æ–°ç»“æœåˆ°è¿­ä»£ç»“æœæ•°æ®é›†

        # ä½¿ç”¨LLMåˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ï¼šç›´æ¥é—®LLM=>åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æä¾›æ–°çš„æŸ¥è¯¢é—®é¢˜ï¼Œä½¿ç”¨ä¸åŒè§’åº¦æˆ–æ›´å…·ä½“çš„å…³é”®è¯ã€‚å¦‚æœå·²ç»æœ‰å……åˆ†ä¿¡æ¯ï¼Œè¯·å›å¤'ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢'
        ## ä¸éœ€è¦=>ç»“æŸè¿­ä»£æ£€ç´¢
        ## éœ€è¦=>æ›´æ–°query


    return all_contexts, all_doc_ids, all_metadata


# æ–‡ä»¶çŠ¶æ€å¤„ç†ç®¡ç†ç±»
class FileProcessor:
    def __init__(self):
        self.processed_files = {}

    def clear_files(self):
        self.processed_files = {}

    def add_file(self, file_name):
        self.processed_files[file_name] = {
            "status": "ç­‰å¾…å¤„ç†",
            "tiemstamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "chunks": 0
        }

    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]["status"] = status
            if chunks is not None:
                self.processed_files[file_name]["chunks"] = chunks

    def get_file_list(self):
        # è¯­æ³•ï¼Œæ³¨æ„ä¸¤è¡Œä¹‹é—´æ˜¯æ²¡æœ‰é€—å·çš„
        return [
            f"{fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()


class BM25IndexManager:
    def __init__(self):
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []

    def build_index(self, documents, doc_ids):
        self.raw_corpus = documents
        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}

        self.tokenized_corpus = []
        """
         tokenized_corpus = [
            ["çŒ«", "æ˜¯", "ä¸€ç§", "åŠ¨ç‰©"],  # æ–‡æ¡£1çš„åˆ†è¯ç»“æœ
            ["ç‹—", "æ˜¯", "äººç±»", "æœ‹å‹"],  # æ–‡æ¡£2çš„åˆ†è¯ç»“æœ
            ...
        ]
        """
        for doc in documents:
            tokens = list(jieba.cut(doc))
            self.tokenized_corpus.append(tokens)

        # åˆ›å»ºBM25ç´¢å¼•
        self.bm25_index = BM25Okapi(self.tokenized_corpus)
        return True

    def search(self, query, top_k=5):

    def clear(self):
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []


BM25_MANAGER = BM25IndexManager()

# ä¸Šä¼ æ–‡ä»¶åè¿›è¡Œã€æ–‡æ¡£çš„ç´¢å¼•æ›´æ–°ã€‘
def update_bm25_index():
    try:
        ## è·å–æ‰€æœ‰æ–‡æ¡£
        all_docs = COLLECTION.get(include=["documents", "metadatas"])

        if not all_docs or not all_docs["documents"]:
            logging.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£")
            BM25_MANAGER.clear()
            return False

        doc_ids = [f"{meta.get('doc_id', 'unknown')}_{idx}" for idx, meta in enumerate(all_docs["metadatas"])]
        BM25_MANAGER.build_index(all_docs["documents"], doc_ids)
        logging.info(f"BM25ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå…±ç´¢å¼• {len(doc_ids)} ä¸ªæ–‡æ¡£")
        return True

    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {str(e)}")
        return False


def extract_text(filepath):
    output = StringIO()
    with open(filepath, "rb") as file:
        extract_text_to_fp(file, output)
    return output.getvalue()


# æ–‡æ¡£å¤„ç†æµç¨‹-PDFè§£æä¸åˆ†å—
def process_multiple_pdfs(files, progress=gr.Progress()):
    # å¼€å§‹æ¸…ç†æ—§çš„æ•°æ®
    # å¤„ç†ç›®å‰ä¸Šä¼ çš„æ‰€æœ‰æ•°æ®files
    total_files = len(files)
    processed_results = []
    total_chunks = 0

    for idx, file in enumerate(files, 1):
        try:
            # å¯è§†åŒ–å±•ç¤ºç›®å‰å¤„ç†æ–‡ä»¶åç§°
            file_name = os.path.basename(file)
            progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx/total_files}:{file_name}")

            # æ·»åŠ æ–‡ä»¶åˆ°æ–‡ä»¶ç®¡ç†ç±»ä¸­ + æå–è¯¥æ–‡ä»¶å†…å®¹
            file_processor.add_file(file_name)

            # å¯¹æå–çš„å†…å®¹è¿›è¡Œåˆ†å‰²ä¸ºchunks
            text = extract_text(file.name)
            chunks = text_splitter.split_text(text)

            if not chunks:
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©º/æ— æ³•æå–æ–‡æœ¬")

            # ä½¿ç”¨all-MIniLM-L6-v2å°†chunks->å‘é‡Text Embeddings
            doc_id = f"doc_{int(time.time())}_{idx}"
            embeddings = EMBED_MODEL.encode(chunks)

            # å°†ç”Ÿæˆçš„å‘é‡æ•°ç»„chunksä»¥åŠå¯¹åº”çš„æ–‡æ¡£æ•°æ®å­˜å…¥åˆ°chromadb
            ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
            metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]

            COLLECTION.add(
                ids=ids,
                embeddings=embeddings.tolist(),
                documents=chunks,
                metadatas=metadatas
            )

            # æ›´æ–°å½“å‰UIçš„å¤„ç†çŠ¶æ€
            total_chunks += len(chunks)
            file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
            processed_results.append(f"âœ…{file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")

        except Exception as e:
            error_msg = str(e)
            logging.error(f"å¤„ç†æ–‡ä»¶ {file_name}æ—¶å‡ºé”™ï¼š{error_msg}")
            file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
            processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")

    # éå†å®Œæˆï¼Œè¿›è¡Œä¿¡æ¯çš„æ€»ç»“
    summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
    processed_results.append(summary)

    # æ›´æ–°BM25çš„ç´¢å¼•ï¼šå¯ä»¥æ ¹æ®è¿™ä¸ªç´¢å¼•å¾—åˆ° queryä¸å„ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸å…³æ€§
    progress(0.95, desc="æ„å»ºBM25æ£€ç´¢ç´¢å¼•...")
    update_bm25_index()

    # è·å–æ›´æ–°çŠ¶æ€åçš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ¯ä¸€ä¸ªæ–‡ä»¶éƒ½æ›´æ–°äº†å¯¹åº”çš„çŠ¶æ€)
    file_list = file_processor.get_file_list()

    # è¿”å›å¤„ç†è¿‡ç¨‹çš„æ‰“å°ä¿¡æ¯
    return "\n".join(processed_results), file_list





def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0


if __name__ == "__main" :
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)

    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)

    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("ollamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨")
            exit(1)

        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False,
            height=900
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")