# 知识图谱增强检索实现分析

> graphrag/search.py


`retrieval()` 方法的主要流程主要分为几个步骤：
1. 查询重写：调用 `query_rewrite()` 方法，将用户的问题转换为 `类型关键词` 和 `实体列表`
2. 检索相关实体：通过 `get_relevants_by_keywords()` 和 `get_relevant_ents_by_types()` 方法，根据 `关键词` 和 `类型` 从知识图谱中检索实体
3. 检索相关关系：通过 `get_relevant_relations_by_txt()` 方法，基于问题文本检索关系路径，即基于语义的关系检索
4. 整合、排序结果
5. 社区报告检索：调用 `_community_retrival_()` 方法，根据检索到的实体获取相关的社区摘要


## 查询重写query_rewrite()


`ty_kwds, ents = self.query_rewrite(llm, qst, [index_name(tid) for tid in tenant_ids], kb_ids)`
- 先使用 `混合检索retrievaler.search()`  得到 `ty2ents`，也就是`Answer type pool`
- 然后拼接出 `hint_prompt`，使用 `llm-chat大模型`获取 `answer_type_keywords` 和 `entities_from_query`
  - `answer_type_keywords` ：问题的类型，从 `Answer type pool` 中获取，可能性最高的类型放在前面，不超过3个
  - `entities_from_query`：特定实体或者详细信息，从查询中获取
- 返回 `answer_type_keywords` 和 `entities_from_query`


> `answer_type_keywords` 代表着 query 的类型，比如你问的问题可能是父子关系？可能是地方名字？可能是食物？
>
> `entities_from_query` 代表着 query 所包含的实体 以及 可能有关联的实体

> 上面的流程可能过于抽象，可以参考 `graphrag/query_analyze_prompt.py` 的 `PROMPTS["minirag_query2kwd"]`


------ 
上面的流程可以使用`graphrag/query_analyze_prompt.py`一个示例展示：
- `混合检索retrievaler.search()` 得到 `TYPE_POOL`
- 拼接 `Query: {query}` + `Answer type pool:{TYPE_POOL}` 为 prompt 进行LLM的提问，要求得到 `answer_type_keywords` 和 `entities_from_query`

```python
#############################
Query: "Where is the capital of the United States?"
Answer type pool: {{
 'ORGANIZATION': ['GREENPEACE', 'RED CROSS'],
 'PERSONAL LIFE': ['DAILY WORKOUT', 'HOME COOKING'],
 'STRATEGY': ['FINANCIAL INVESTMENT', 'BUSINESS EXPANSION'],
 'SERVICE FACILITATION': ['ONLINE SUPPORT', 'CUSTOMER SERVICE TRAINING'],
 'PERSON': ['ALBERTA SMITH', 'BENJAMIN JONES'],
 'FOOD': ['PASTA CARBONARA', 'SUSHI PLATTER'],
 'EMOTION': ['HAPPINESS', 'SADNESS'],
 'PERSONAL EXPERIENCE': ['TRAVEL ADVENTURE', 'BOOK CLUB'],
 'INTERACTION': ['TEAM BUILDING', 'NETWORKING MEETUP'],
 'BEVERAGE': ['LATTE', 'GREEN TEA'],
 'PLAN': ['WEIGHT LOSS', 'CAREER DEVELOPMENT'],
 'GEO': ['PARIS', 'NEW YORK'],
 'GEAR': ['CAMERA', 'HEADPHONES'],
 'EMOJI': ['🏢', '🌍'],
 'BEHAVIOR': ['POSITIVE THINKING', 'STRESS MANAGEMENT'],
 'TONE': ['FRIENDLY', 'PROFESSIONAL'],
 'LOCATION': ['DOWNTOWN', 'SUBURBS']
}}
################
Output:
{{
  "answer_type_keywords": ["LOCATION"],
  "entities_from_query": ["capital of the United States", "Washington", "New York"]
}}
#############################
```


```python
def query_rewrite(self, llm, question, idxnms, kb_ids):
    ty2ents = trio.run(lambda: get_entity_type2sampels(idxnms, kb_ids))
    hint_prompt = PROMPTS["minirag_query2kwd"].format(query=question,
                                                        TYPE_POOL=json.dumps(ty2ents, ensure_ascii=False, indent=2))
    result = self._chat(llm, hint_prompt, [{"role": "user", "content": "Output:"}], {"temperature": .5})

    keywords_data = json_repair.loads(result)
    type_keywords = keywords_data.get("answer_type_keywords", [])
    entities_from_query = keywords_data.get("entities_from_query", [])[:5]
    return type_keywords, entities_from_query

async def get_entity_type2sampels(idxnms, kb_ids: list):
    es_res = await trio.to_thread.run_sync(lambda: settings.retrievaler.search({"knowledge_graph_kwd": "ty2ents", "kb_id": kb_ids,
                                       "size": 10000,
                                       "fields": ["content_with_weight"]},
                                      idxnms, kb_ids))
def _chat(self, llm_bdl, system, history, gen_conf):
    response = get_llm_cache(llm_bdl.llm_name, system, history, gen_conf)
    if response:
        return response
    response = llm_bdl.chat(system, history, gen_conf)
    if response.find("**ERROR**") >= 0:
        raise Exception(response)
    set_llm_cache(llm_bdl.llm_name, system, response, history, gen_conf)
    return response
```



## 检索相关实体

从 `query_rewrite()` 我们得到了 `ty_kwds查询问题的类型` + `ents查询问题相关联的实体`

然后触发`get_relevants_by_keywords()` 和 `get_relevant_ents_by_types()`
```python
ents_from_query = self.get_relevant_ents_by_keywords(ents, filters, idxnms, kb_ids, emb_mdl, ent_sim_threshold)
ents_from_types = self.get_relevant_ents_by_types(ty_kwds, filters, idxnms, kb_ids, 10000)
```

> 本质还是通过 `ES` 配置参数去检索对应的实体

**通过关键词检索相关实体**：get_relevant_ents_by_keywords

拿到 `"entities_from_query": ["capital of the United States", "Washington", "New York"]` 的相关实体

```python
filters["knowledge_graph_kwd"] = "entity"
matchDense = self.get_vector(", ".join(keywords), emb_mdl, 1024, sim_thr)
matchExprs = [matchDense]
# ES请求...
```


**通过问题类型检索相关实体**：get_relevant_ents_by_types

拿到 `"answer_type_keywords": ["LOCATION"]` 的相关实体

```python
filters["knowledge_graph_kwd"] = "entity"
filters["entity_type_kwd"] = types
matchDense = []
# ES请求...
```


## 检索相关关系

> 本质还是通过 `ES` 配置参数去检索对应的实体

拿到当前问题query相关的关系路径，比如 `"relations": [{"From": "Washington", "To": "United States", "Score": 1.08}]`

```python
qst = question
rels_from_txt = self.get_relevant_relations_by_txt(qst, filters, idxnms, kb_ids, emb_mdl, rel_sim_threshold)
```

```python
filters["knowledge_graph_kwd"] = "relation"
matchDense = self.get_vector(txt, emb_mdl, 1024, sim_thr)
matchExprs = [matchDense]
# ES请求...
```



## 整合、排序结果


### 过滤低分信息

通过 `get_relevants_by_keywords()` 和 `get_relevant_ents_by_types()` 方法检索ES，使用 `_ent_info_from_()` 进行 `sim_thr` 的过滤以及关键信息的提取

```python
def _ent_info_from_(self, es_res, sim_thr=0.3):
    res = {}
    flds = ["content_with_weight", "_score", "entity_kwd", "rank_flt", "n_hop_with_weight"]
    es_res = self.dataStore.getFields(es_res, flds)
    for _, ent in es_res.items():
        for f in flds:
            if f in ent and ent[f] is None:
                del ent[f]
        if get_float(ent.get("_score", 0)) < sim_thr:
            continue
        if isinstance(ent["entity_kwd"], list):
            ent["entity_kwd"] = ent["entity_kwd"][0]
        res[ent["entity_kwd"]] = {
            "sim": get_float(ent.get("_score", 0)),
            "pagerank": get_float(ent.get("rank_flt", 0)),
            "n_hop_ents": json.loads(ent.get("n_hop_with_weight", "[]")),
            "description": ent.get("content_with_weight", "{}")
        }
    return res
```


通过 `get_relevant_relations_by_txt()` 方法检索ES，使用 `_relation_info_from_()`进行 `sim_thr` 的过滤以及关键信息的提取

```python
def _relation_info_from_(self, es_res, sim_thr=0.3):
    res = {}
    es_res = self.dataStore.getFields(es_res, ["content_with_weight", "_score", "from_entity_kwd", "to_entity_kwd",
                                                "weight_int"])
    for _, ent in es_res.items():
        if get_float(ent["_score"]) < sim_thr:
            continue
        f, t = sorted([ent["from_entity_kwd"], ent["to_entity_kwd"]])
        if isinstance(f, list):
            f = f[0]
        if isinstance(t, list):
            t = t[0]
        res[(f, t)] = {
            "sim": get_float(ent["_score"]),
            "pagerank": get_float(ent.get("weight_int", 0)),
            "description": ent["content_with_weight"]
        }
    return res
```


### 多跳路径整合：从ents_from_query中获取


假设 `ent[sim]` = 0.92，那么
- 路径长度为 1 时，权重 = 0.92 / (2+0) = 0.46
- 路径长度为 2 时，权重 = 0.92 / (2+1) = 0.31

> 多跳衰减

```python
for _, ent in ents_from_query.items():
    nhops = ent.get("n_hop_ents", [])
    for nbr in nhops:
        path = nbr["path"]
        wts = nbr["weights"]
        for i in range(len(path) - 1):
            f, t = path[i], path[i + 1]
            if (f, t) in nhop_pathes:
                nhop_pathes[(f, t)]["sim"] += ent["sim"] / (2 + i)
            else:
                nhop_pathes[(f, t)]["sim"] = ent["sim"] / (2 + i)
            nhop_pathes[(f, t)]["pagerank"] = wts[i]
```

### 类型匹配到的实体权重翻倍

```python
for ent in ents_from_types.keys():
    if ent not in ents_from_query:
        continue
    ents_from_query[ent]["sim"] *= 2
```

### 关系relations和类型得到的实体entity进行混合评分

如果通过ES得到的relations关系中的 `两端实体` 都在 `ents_from_types` 中，则增强该权重

```python
for (f, t) in rels_from_txt.keys():
    pair = tuple(sorted([f, t]))
    s = 0
    if pair in nhop_pathes:
        s += nhop_pathes[pair]["sim"]
        del nhop_pathes[pair]
    if f in ents_from_types:
        s += 1
    if t in ents_from_types:
        s += 1
    rels_from_txt[(f, t)]["sim"] *= s + 1
```

### 将从ents_from_query中获取的多跳路径并入到rels_from_txt关系路径中

```python
for (f, t) in nhop_pathes.keys():
    s = 0
    if f in ents_from_types:
        s += 1
    if t in ents_from_types:
        s += 1
    rels_from_txt[(f, t)] = {
        "sim": nhop_pathes[(f, t)]["sim"] * (s + 1),
        "pagerank": nhop_pathes[(f, t)]["pagerank"]
    }
```


### 根据sim和pagerank进行排序

```python
ents_from_query = sorted(ents_from_query.items(), key=lambda x: x[1]["sim"] * x[1]["pagerank"], reverse=True)[
                    :ent_topn]
rels_from_txt = sorted(rels_from_txt.items(), key=lambda x: x[1]["sim"] * x[1]["pagerank"], reverse=True)[
                :rel_topn]
```


## 构建ents和relas + 社区报告检索


传入从查询问题中得到的实体，构建对应的参数进行ES的搜索，从ES查询结果中构建对应的社区报告

拼接到 `content_with_weight` 进行返回

```python
# 省略...ents_from_query=>构建ents的逻辑
# 省略...rels_from_txt=>构建relas的逻辑
return {
        "chunk_id": get_uuid(),
        "content_ltks": "",
        "content_with_weight": ents + relas + self._community_retrival_([n for n, _ in ents_from_query], filters, kb_ids, idxnms, comm_topn, max_token),
        "doc_id": "",
        "docnm_kwd": "Related content in Knowledge Graph",
        "kb_id": kb_ids,
        "important_kwd": [],
        "image_id": "",
        "similarity": 1.,
        "vector_similarity": 1.,
        "term_similarity": 0,
        "vector": [],
        "positions": [],
    }

def _community_retrival_(self, entities, condition, kb_ids, idxnms, topn, max_token):
    ## Community retrieval
    fields = ["docnm_kwd", "content_with_weight"]
    odr = OrderByExpr()
    odr.desc("weight_flt")
    fltr = deepcopy(condition)
    fltr["knowledge_graph_kwd"] = "community_report"
    fltr["entities_kwd"] = entities
    comm_res = self.dataStore.search(fields, [], fltr, [],
                                        OrderByExpr(), 0, topn, idxnms, kb_ids)
    comm_res_fields = self.dataStore.getFields(comm_res, fields)
    txts = []
    for ii, (_, row) in enumerate(comm_res_fields.items()):
        obj = json.loads(row["content_with_weight"])
        txts.append("# {}. {}\n## Content\n{}\n## Evidences\n{}\n".format(
            ii + 1, row["docnm_kwd"], obj["report"], obj["evidences"]))
        max_token -= num_tokens_from_string(str(txts[-1]))

    if not txts:
        return ""
    return "\n---- Community Report ----\n" + "\n".join(txts)
```