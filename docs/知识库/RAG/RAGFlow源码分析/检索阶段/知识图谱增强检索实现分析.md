# çŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢å®ç°åˆ†æ

> graphrag/search.py


`retrieval()` æ–¹æ³•çš„ä¸»è¦æµç¨‹ä¸»è¦åˆ†ä¸ºå‡ ä¸ªæ­¥éª¤ï¼š
1. æŸ¥è¯¢é‡å†™ï¼šè°ƒç”¨ `query_rewrite()` æ–¹æ³•ï¼Œå°†ç”¨æˆ·çš„é—®é¢˜è½¬æ¢ä¸º `ç±»å‹å…³é”®è¯` å’Œ `å®ä½“åˆ—è¡¨`
2. æ£€ç´¢ç›¸å…³å®ä½“ï¼šé€šè¿‡ `get_relevants_by_keywords()` å’Œ `get_relevant_ents_by_types()` æ–¹æ³•ï¼Œæ ¹æ® `å…³é”®è¯` å’Œ `ç±»å‹` ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å®ä½“
3. æ£€ç´¢ç›¸å…³å…³ç³»ï¼šé€šè¿‡ `get_relevant_relations_by_txt()` æ–¹æ³•ï¼ŒåŸºäºé—®é¢˜æ–‡æœ¬æ£€ç´¢å…³ç³»è·¯å¾„ï¼Œå³åŸºäºè¯­ä¹‰çš„å…³ç³»æ£€ç´¢
4. æ•´åˆã€æ’åºç»“æœ
5. ç¤¾åŒºæŠ¥å‘Šæ£€ç´¢ï¼šè°ƒç”¨ `_community_retrival_()` æ–¹æ³•ï¼Œæ ¹æ®æ£€ç´¢åˆ°çš„å®ä½“è·å–ç›¸å…³çš„ç¤¾åŒºæ‘˜è¦


## æŸ¥è¯¢é‡å†™query_rewrite()


`ty_kwds, ents = self.query_rewrite(llm, qst, [index_name(tid) for tid in tenant_ids], kb_ids)`
- å…ˆä½¿ç”¨ `æ··åˆæ£€ç´¢retrievaler.search()`  å¾—åˆ° `ty2ents`ï¼Œä¹Ÿå°±æ˜¯`Answer type pool`
- ç„¶åæ‹¼æ¥å‡º `hint_prompt`ï¼Œä½¿ç”¨ `llm-chatå¤§æ¨¡å‹`è·å– `answer_type_keywords` å’Œ `entities_from_query`
  - `answer_type_keywords` ï¼šé—®é¢˜çš„ç±»å‹ï¼Œä» `Answer type pool` ä¸­è·å–ï¼Œå¯èƒ½æ€§æœ€é«˜çš„ç±»å‹æ”¾åœ¨å‰é¢ï¼Œä¸è¶…è¿‡3ä¸ª
  - `entities_from_query`ï¼šç‰¹å®šå®ä½“æˆ–è€…è¯¦ç»†ä¿¡æ¯ï¼Œä»æŸ¥è¯¢ä¸­è·å–
- è¿”å› `answer_type_keywords` å’Œ `entities_from_query`


> `answer_type_keywords` ä»£è¡¨ç€ query çš„ç±»å‹ï¼Œæ¯”å¦‚ä½ é—®çš„é—®é¢˜å¯èƒ½æ˜¯çˆ¶å­å…³ç³»ï¼Ÿå¯èƒ½æ˜¯åœ°æ–¹åå­—ï¼Ÿå¯èƒ½æ˜¯é£Ÿç‰©ï¼Ÿ
>
> `entities_from_query` ä»£è¡¨ç€ query æ‰€åŒ…å«çš„å®ä½“ ä»¥åŠ å¯èƒ½æœ‰å…³è”çš„å®ä½“

> ä¸Šé¢çš„æµç¨‹å¯èƒ½è¿‡äºæŠ½è±¡ï¼Œå¯ä»¥å‚è€ƒ `graphrag/query_analyze_prompt.py` çš„ `PROMPTS["minirag_query2kwd"]`


------ 
ä¸Šé¢çš„æµç¨‹å¯ä»¥ä½¿ç”¨`graphrag/query_analyze_prompt.py`ä¸€ä¸ªç¤ºä¾‹å±•ç¤ºï¼š
- `æ··åˆæ£€ç´¢retrievaler.search()` å¾—åˆ° `TYPE_POOL`
- æ‹¼æ¥ `Query: {query}` + `Answer type pool:{TYPE_POOL}` ä¸º prompt è¿›è¡ŒLLMçš„æé—®ï¼Œè¦æ±‚å¾—åˆ° `answer_type_keywords` å’Œ `entities_from_query`

```python
#############################
Query: "Where is the capital of the United States?"
Answer type pool: {{
 'ORGANIZATION': ['GREENPEACE', 'RED CROSS'],
 'PERSONAL LIFE': ['DAILY WORKOUT', 'HOME COOKING'],
 'STRATEGY': ['FINANCIAL INVESTMENT', 'BUSINESS EXPANSION'],
 'SERVICE FACILITATION': ['ONLINE SUPPORT', 'CUSTOMER SERVICE TRAINING'],
 'PERSON': ['ALBERTA SMITH', 'BENJAMIN JONES'],
 'FOOD': ['PASTA CARBONARA', 'SUSHI PLATTER'],
 'EMOTION': ['HAPPINESS', 'SADNESS'],
 'PERSONAL EXPERIENCE': ['TRAVEL ADVENTURE', 'BOOK CLUB'],
 'INTERACTION': ['TEAM BUILDING', 'NETWORKING MEETUP'],
 'BEVERAGE': ['LATTE', 'GREEN TEA'],
 'PLAN': ['WEIGHT LOSS', 'CAREER DEVELOPMENT'],
 'GEO': ['PARIS', 'NEW YORK'],
 'GEAR': ['CAMERA', 'HEADPHONES'],
 'EMOJI': ['ğŸ¢', 'ğŸŒ'],
 'BEHAVIOR': ['POSITIVE THINKING', 'STRESS MANAGEMENT'],
 'TONE': ['FRIENDLY', 'PROFESSIONAL'],
 'LOCATION': ['DOWNTOWN', 'SUBURBS']
}}
################
Output:
{{
  "answer_type_keywords": ["LOCATION"],
  "entities_from_query": ["capital of the United States", "Washington", "New York"]
}}
#############################
```


```python
def query_rewrite(self, llm, question, idxnms, kb_ids):
    ty2ents = trio.run(lambda: get_entity_type2sampels(idxnms, kb_ids))
    hint_prompt = PROMPTS["minirag_query2kwd"].format(query=question,
                                                        TYPE_POOL=json.dumps(ty2ents, ensure_ascii=False, indent=2))
    result = self._chat(llm, hint_prompt, [{"role": "user", "content": "Output:"}], {"temperature": .5})

    keywords_data = json_repair.loads(result)
    type_keywords = keywords_data.get("answer_type_keywords", [])
    entities_from_query = keywords_data.get("entities_from_query", [])[:5]
    return type_keywords, entities_from_query

async def get_entity_type2sampels(idxnms, kb_ids: list):
    es_res = await trio.to_thread.run_sync(lambda: settings.retrievaler.search({"knowledge_graph_kwd": "ty2ents", "kb_id": kb_ids,
                                       "size": 10000,
                                       "fields": ["content_with_weight"]},
                                      idxnms, kb_ids))
def _chat(self, llm_bdl, system, history, gen_conf):
    response = get_llm_cache(llm_bdl.llm_name, system, history, gen_conf)
    if response:
        return response
    response = llm_bdl.chat(system, history, gen_conf)
    if response.find("**ERROR**") >= 0:
        raise Exception(response)
    set_llm_cache(llm_bdl.llm_name, system, response, history, gen_conf)
    return response
```



## æ£€ç´¢ç›¸å…³å®ä½“

ä» `query_rewrite()` æˆ‘ä»¬å¾—åˆ°äº† `ty_kwdsæŸ¥è¯¢é—®é¢˜çš„ç±»å‹` + `entsæŸ¥è¯¢é—®é¢˜ç›¸å…³è”çš„å®ä½“`

ç„¶åè§¦å‘`get_relevants_by_keywords()` å’Œ `get_relevant_ents_by_types()`
```python
ents_from_query = self.get_relevant_ents_by_keywords(ents, filters, idxnms, kb_ids, emb_mdl, ent_sim_threshold)
ents_from_types = self.get_relevant_ents_by_types(ty_kwds, filters, idxnms, kb_ids, 10000)
```

> æœ¬è´¨è¿˜æ˜¯é€šè¿‡ `ES` é…ç½®å‚æ•°å»æ£€ç´¢å¯¹åº”çš„å®ä½“

**é€šè¿‡å…³é”®è¯æ£€ç´¢ç›¸å…³å®ä½“**ï¼šget_relevant_ents_by_keywords

æ‹¿åˆ° `"entities_from_query": ["capital of the United States", "Washington", "New York"]` çš„ç›¸å…³å®ä½“

```python
filters["knowledge_graph_kwd"] = "entity"
matchDense = self.get_vector(", ".join(keywords), emb_mdl, 1024, sim_thr)
matchExprs = [matchDense]
# ESè¯·æ±‚...
```


**é€šè¿‡é—®é¢˜ç±»å‹æ£€ç´¢ç›¸å…³å®ä½“**ï¼šget_relevant_ents_by_types

æ‹¿åˆ° `"answer_type_keywords": ["LOCATION"]` çš„ç›¸å…³å®ä½“

```python
filters["knowledge_graph_kwd"] = "entity"
filters["entity_type_kwd"] = types
matchDense = []
# ESè¯·æ±‚...
```


## æ£€ç´¢ç›¸å…³å…³ç³»

> æœ¬è´¨è¿˜æ˜¯é€šè¿‡ `ES` é…ç½®å‚æ•°å»æ£€ç´¢å¯¹åº”çš„å®ä½“

æ‹¿åˆ°å½“å‰é—®é¢˜queryç›¸å…³çš„å…³ç³»è·¯å¾„ï¼Œæ¯”å¦‚ `"relations": [{"From": "Washington", "To": "United States", "Score": 1.08}]`

```python
qst = question
rels_from_txt = self.get_relevant_relations_by_txt(qst, filters, idxnms, kb_ids, emb_mdl, rel_sim_threshold)
```

```python
filters["knowledge_graph_kwd"] = "relation"
matchDense = self.get_vector(txt, emb_mdl, 1024, sim_thr)
matchExprs = [matchDense]
# ESè¯·æ±‚...
```



## æ•´åˆã€æ’åºç»“æœ


### è¿‡æ»¤ä½åˆ†ä¿¡æ¯

é€šè¿‡ `get_relevants_by_keywords()` å’Œ `get_relevant_ents_by_types()` æ–¹æ³•æ£€ç´¢ESï¼Œä½¿ç”¨ `_ent_info_from_()` è¿›è¡Œ `sim_thr` çš„è¿‡æ»¤ä»¥åŠå…³é”®ä¿¡æ¯çš„æå–

```python
def _ent_info_from_(self, es_res, sim_thr=0.3):
    res = {}
    flds = ["content_with_weight", "_score", "entity_kwd", "rank_flt", "n_hop_with_weight"]
    es_res = self.dataStore.getFields(es_res, flds)
    for _, ent in es_res.items():
        for f in flds:
            if f in ent and ent[f] is None:
                del ent[f]
        if get_float(ent.get("_score", 0)) < sim_thr:
            continue
        if isinstance(ent["entity_kwd"], list):
            ent["entity_kwd"] = ent["entity_kwd"][0]
        res[ent["entity_kwd"]] = {
            "sim": get_float(ent.get("_score", 0)),
            "pagerank": get_float(ent.get("rank_flt", 0)),
            "n_hop_ents": json.loads(ent.get("n_hop_with_weight", "[]")),
            "description": ent.get("content_with_weight", "{}")
        }
    return res
```


é€šè¿‡ `get_relevant_relations_by_txt()` æ–¹æ³•æ£€ç´¢ESï¼Œä½¿ç”¨ `_relation_info_from_()`è¿›è¡Œ `sim_thr` çš„è¿‡æ»¤ä»¥åŠå…³é”®ä¿¡æ¯çš„æå–

```python
def _relation_info_from_(self, es_res, sim_thr=0.3):
    res = {}
    es_res = self.dataStore.getFields(es_res, ["content_with_weight", "_score", "from_entity_kwd", "to_entity_kwd",
                                                "weight_int"])
    for _, ent in es_res.items():
        if get_float(ent["_score"]) < sim_thr:
            continue
        f, t = sorted([ent["from_entity_kwd"], ent["to_entity_kwd"]])
        if isinstance(f, list):
            f = f[0]
        if isinstance(t, list):
            t = t[0]
        res[(f, t)] = {
            "sim": get_float(ent["_score"]),
            "pagerank": get_float(ent.get("weight_int", 0)),
            "description": ent["content_with_weight"]
        }
    return res
```


### å¤šè·³è·¯å¾„æ•´åˆï¼šä»ents_from_queryä¸­è·å–


å‡è®¾ `ent[sim]` = 0.92ï¼Œé‚£ä¹ˆ
- è·¯å¾„é•¿åº¦ä¸º 1 æ—¶ï¼Œæƒé‡ = 0.92 / (2+0) = 0.46
- è·¯å¾„é•¿åº¦ä¸º 2 æ—¶ï¼Œæƒé‡ = 0.92 / (2+1) = 0.31

> å¤šè·³è¡°å‡

```python
for _, ent in ents_from_query.items():
    nhops = ent.get("n_hop_ents", [])
    for nbr in nhops:
        path = nbr["path"]
        wts = nbr["weights"]
        for i in range(len(path) - 1):
            f, t = path[i], path[i + 1]
            if (f, t) in nhop_pathes:
                nhop_pathes[(f, t)]["sim"] += ent["sim"] / (2 + i)
            else:
                nhop_pathes[(f, t)]["sim"] = ent["sim"] / (2 + i)
            nhop_pathes[(f, t)]["pagerank"] = wts[i]
```

### ç±»å‹åŒ¹é…åˆ°çš„å®ä½“æƒé‡ç¿»å€

```python
for ent in ents_from_types.keys():
    if ent not in ents_from_query:
        continue
    ents_from_query[ent]["sim"] *= 2
```

### å…³ç³»relationså’Œç±»å‹å¾—åˆ°çš„å®ä½“entityè¿›è¡Œæ··åˆè¯„åˆ†

å¦‚æœé€šè¿‡ESå¾—åˆ°çš„relationså…³ç³»ä¸­çš„ `ä¸¤ç«¯å®ä½“` éƒ½åœ¨ `ents_from_types` ä¸­ï¼Œåˆ™å¢å¼ºè¯¥æƒé‡

```python
for (f, t) in rels_from_txt.keys():
    pair = tuple(sorted([f, t]))
    s = 0
    if pair in nhop_pathes:
        s += nhop_pathes[pair]["sim"]
        del nhop_pathes[pair]
    if f in ents_from_types:
        s += 1
    if t in ents_from_types:
        s += 1
    rels_from_txt[(f, t)]["sim"] *= s + 1
```

### å°†ä»ents_from_queryä¸­è·å–çš„å¤šè·³è·¯å¾„å¹¶å…¥åˆ°rels_from_txtå…³ç³»è·¯å¾„ä¸­

```python
for (f, t) in nhop_pathes.keys():
    s = 0
    if f in ents_from_types:
        s += 1
    if t in ents_from_types:
        s += 1
    rels_from_txt[(f, t)] = {
        "sim": nhop_pathes[(f, t)]["sim"] * (s + 1),
        "pagerank": nhop_pathes[(f, t)]["pagerank"]
    }
```


### æ ¹æ®simå’Œpagerankè¿›è¡Œæ’åº

```python
ents_from_query = sorted(ents_from_query.items(), key=lambda x: x[1]["sim"] * x[1]["pagerank"], reverse=True)[
                    :ent_topn]
rels_from_txt = sorted(rels_from_txt.items(), key=lambda x: x[1]["sim"] * x[1]["pagerank"], reverse=True)[
                :rel_topn]
```


## æ„å»ºentså’Œrelas + ç¤¾åŒºæŠ¥å‘Šæ£€ç´¢


ä¼ å…¥ä»æŸ¥è¯¢é—®é¢˜ä¸­å¾—åˆ°çš„å®ä½“ï¼Œæ„å»ºå¯¹åº”çš„å‚æ•°è¿›è¡ŒESçš„æœç´¢ï¼Œä»ESæŸ¥è¯¢ç»“æœä¸­æ„å»ºå¯¹åº”çš„ç¤¾åŒºæŠ¥å‘Š

æ‹¼æ¥åˆ° `content_with_weight` è¿›è¡Œè¿”å›

```python
# çœç•¥...ents_from_query=>æ„å»ºentsçš„é€»è¾‘
# çœç•¥...rels_from_txt=>æ„å»ºrelasçš„é€»è¾‘
return {
        "chunk_id": get_uuid(),
        "content_ltks": "",
        "content_with_weight": ents + relas + self._community_retrival_([n for n, _ in ents_from_query], filters, kb_ids, idxnms, comm_topn, max_token),
        "doc_id": "",
        "docnm_kwd": "Related content in Knowledge Graph",
        "kb_id": kb_ids,
        "important_kwd": [],
        "image_id": "",
        "similarity": 1.,
        "vector_similarity": 1.,
        "term_similarity": 0,
        "vector": [],
        "positions": [],
    }

def _community_retrival_(self, entities, condition, kb_ids, idxnms, topn, max_token):
    ## Community retrieval
    fields = ["docnm_kwd", "content_with_weight"]
    odr = OrderByExpr()
    odr.desc("weight_flt")
    fltr = deepcopy(condition)
    fltr["knowledge_graph_kwd"] = "community_report"
    fltr["entities_kwd"] = entities
    comm_res = self.dataStore.search(fields, [], fltr, [],
                                        OrderByExpr(), 0, topn, idxnms, kb_ids)
    comm_res_fields = self.dataStore.getFields(comm_res, fields)
    txts = []
    for ii, (_, row) in enumerate(comm_res_fields.items()):
        obj = json.loads(row["content_with_weight"])
        txts.append("# {}. {}\n## Content\n{}\n## Evidences\n{}\n".format(
            ii + 1, row["docnm_kwd"], obj["report"], obj["evidences"]))
        max_token -= num_tokens_from_string(str(txts[-1]))

    if not txts:
        return ""
    return "\n---- Community Report ----\n" + "\n".join(txts)
```