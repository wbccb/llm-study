> 由于RAGFlow的代码更新非常频繁，因此强调本文所展示的源码和分析是基于2025/4/2 13:56的代码进行，随着时间的推移可能有所改变

# 多模态检索策略

**相关请求API：**
- POST /v1/chunk/retrieval_test
- POST http://localhost:11434/api/chat 
- POST /v1/conversation/ask
- POST /v1/conversation/related_questions

**核心步骤：**
用户问题 -> 关键词扩展 -> 向量检索 -> 重排序 -> 知识图谱补充 -> 结果融合


## 关键技术简述

**1.混合召回层**
| 技术  |  算法 |    所在的文件 |
|---|---|---|
| 关键词召回  |  BM25 算法 |    conf/retrieval/bm25 |
|  向量召回 | 双塔模型（sentence-BERT）  | models/embedding/ |
|  全文检索 | Elasticsearch DSL  | service_conf.yaml |

**2.重排序机制**
- 多路分数融合（线性加权/动态调整）
- 基于 Cohere Rerank 的语义相关性优化
- 时效性权重调节（对新闻类文档特殊处理）

**3.缓存优化**
- Redis 缓存高频查询结果（docker-compose-base.yml 包含 Redis 服务）
- 异步预取热点文档向量


## 检索的整体流程分析

初始化数据，包括各种权重配置以及对应的模型，包括
- question: 检索的问题
- kb_ids: 检索的知识库
- similarity_threshold: 相似度阀值，默认为0.2，可以用来过滤低质量结果
- vector_similarity_weight: 向量权重，默认为0.3，调整语义与关键词检索的混合权重
- embd_mdl: 向量模型
- rerank_mdl: 重排序模型
- chat_mdl: 聊天模型（比如deepseek-r1:7b）


### 关键词扩展
通过LLM提取出问题question的关键词，提高召回率
> 召回率是衡量RAG系统性能的关键指标之一，它表示系统能够检索到的相关文档占所有相关文档的比例
```python
if req.get("keyword", False):
    chat_mdl = LLMBundle(kb.tenant_id, LLMType.CHAT)
    question += keyword_extraction(chat_mdl, question)
```

### 向量检索

封装了混合搜索的逻辑
- 向量检索
- 重排序优化

> 具体逻辑在后续小点中展开

```python
ranks = settings.retrievaler.retrieval(
    question,
    embd_mdl,
    ...
    similarity_threshold,
    vector_similarity_weight,
    top,
    doc_ids,
    rerank_mdl=rerank_mdl,
    highlight=highlight,
    rank_feature=label_question(question, kbs)
)
```

### 知识图谱检索增强

将知识图谱检索结果插入到 `ranks`的最前面位置

> 知识图谱领域知识补充，为了解决LLM的幻觉问题


```python
if use_kg:
    ck = settings.kg_retrievaler.retrieval(...)
    ranks["chunks"].insert(0, ck)

    if ck["content_with_weight"]:
        ranks["chunks"].insert(0, ck)
```


### 检索后的数据处理

- 移除 `vector` 字段
- 重命名名称，返回标准化的格式

```python
for c in ranks["chunks"]:
    c.pop("vector", None)
##rename keys
renamed_chunks = []
for chunk in ranks["chunks"]:
    key_mapping = {
        "chunk_id": "id",
        "content_with_weight": "content",
        "doc_id": "document_id",
        "important_kwd": "important_keywords",
        "question_kwd": "questions",
        "docnm_kwd": "document_keyword",
        "kb_id":"dataset_id"
    }
    rename_chunk = {}
    for key, value in chunk.items():
        new_key = key_mapping.get(key, key)
        rename_chunk[new_key] = value
    renamed_chunks.append(rename_chunk)
ranks["chunks"] = renamed_chunks
return get_result(data=ranks)

def get_result(code=settings.RetCode.SUCCESS, message="", data=None):
    if code == 0:
        if data is not None:
            response = {"code": code, "data": data}
        else:
            response = {"code": code}
    else:
        response = {"code": code, "message": message}
    return jsonify(response)
```


## 混合检索核心实现分析

```python
ranks = settings.retrievaler.retrieval()
```

由上面的分析可以知道，我们使用了 `retrieval()` 进行混合检索


在 `rag/nlp/search.py` 的 `retrieval()` 代码逻辑也非常简单：

- **self.search()** ：进行 `向量搜索` 与 `关键词匹配（BM25改进版）` 的混合检索
- **self.rerank_by_model()** / **self.rerand()**：对结果进行重排序
- 重排序后的结果处理：`similarity_threshold` 过滤低质量答案，格式化高亮文本，构建docnm_kwd统计匹配次数，`sorted`根据`count`进行重新排序优化，然后返回处理后的 `ranks`（检索的数据主要放在 `ranks["chunks"]` 数组中

### search()检索

先进行了 `question` 的获取


> self.dataStore = ES/Infinity
**如果question为空**
直接进行全字段搜索（即 `matchExprs` 为空）

**如果question不为空**
- 先使用 `self.qryr.question` 对问题进行处理
- 然后判断向量模型是否存在
  - 如果不存在，`matchExprs` 拼接问题数据，直接对问题进行 `ES/Infinity` 检索
  - 如果存在，则使用 `self.get_vector()` 将问题转化为向量，然后进行 `matchExprs` 的构建（问题、问题向量、权重参数配置），然后进行 `ES/Infinity` 检索
    * 如果结果为空，则放宽 `min_match=0.1` （关键词阈值）和降低 `similarity=0.7`（向量相似度要求）再度进行检索
- 对提取出来的问题关键字 `keywork` 匹配搜索出来的内容进行高亮显示
- 按照 `docnm_kwd` 字段统计文档命中次数
- 返回 `return self.SearchResult(query_vector=q_vec, ...)` 格式化好的object数据

> 向量与关键词权重设为0.05:0.95，侧重语义匹配

```python
def search()
    qst = req.get("question", "")
    if not qst:
        res = self.dataStore.search(src, [], filters, [], orderBy, offset, limit, idx_names, kb_ids)
        total = self.dataStore.getTotal(res)
    else:
        highlightFields = ["content_ltks", "title_tks"] if highlight else []
        matchText, keywords = self.qryr.question(qst, min_match=0.3)
        if emb_mdl is None:
            matchExprs = [matchText]
            res = self.dataStore.search(src, highlightFields, filters, matchExprs, orderBy, offset, limit,
                                idx_names, kb_ids, rank_feature=rank_feature)
        else:
            matchDense = self.get_vector(qst, emb_mdl, topk, req.get("similarity", 0.1))
            q_vec = matchDense.embedding_data
            src.append(f"q_{len(q_vec)}_vec")
            fusionExpr = FusionExpr("weighted_sum", topk, {"weights": "0.05, 0.95"})
            matchExprs = [matchText, matchDense, fusionExpr]

            res = self.dataStore.search(src, highlightFields, filters, matchExprs, orderBy, offset, limit,
                                        idx_names, kb_ids, rank_feature=rank_feature)
            if total == 0:
                matchText, _ = self.qryr.question(qst, min_match=0.1)
                matchDense.extra_options["similarity"] = 0.17
                res = self.dataStore.search(src, highlightFields, filters, [matchText, matchDense, fusionExpr],
                                    orderBy, offset, limit, idx_names, kb_ids, rank_feature=rank_feature)
```


#### self.qryr.question扩展问题

传入查询的问题文本，进行一系列的处理：
- 输入预处理：`移除标点、换行符等噪声，全角转半角` + `去除中英文常见虚词和语气词`
- 存在非中文词语时处理
- 只存在中文词语时处理

##### 输入预处理

标准化输入格式（全角转半角、停用词过滤）

```python
txt = re.sub(
    r"[ :|\r\n\t,，。？?/`!！&^%%()\[\]{}<>]+",
    " ",
    rag_tokenizer.tradi2simp(rag_tokenizer.strQ2B(txt.lower())),
).strip()
txt = FulltextQueryer.rmWWW(txt)
```

停用词过滤 `rmWWW()` ：
- 中文停用词过滤："请问什么是深度学习" -> "深度学习"
- 英语疑问词过滤："whatis" -> "is"
- 英文停用词清除："The book" -> "book"

```python
@staticmethod
def rmWWW(txt):
    patts = [
        (
            r"是*(什么样的|哪家|一下|那家|请问|啥样|咋样了|什么时候|何时|何地|何人|是否|是不是|多少|哪里|怎么|哪儿|怎么样|如何|哪些|是啥|啥是|啊|吗|呢|吧|咋|什么|有没有|呀|谁|哪位|哪个)是*",
            "",
        ),
        (r"(^| )(what|who|how|which|where|why)('re|'s)? ", " "),
        (
            r"(^| )('s|'re|is|are|were|was|do|does|did|don't|doesn't|didn't|has|have|be|there|you|me|your|my|mine|just|please|may|i|should|would|wouldn't|will|won't|done|go|for|with|so|the|a|an|by|i'm|it's|he's|she's|they|they're|you're|as|by|on|in|at|up|out|down|of|to|or|and|if) ",
            " ")
    ]
    otxt = txt
    for r, p in patts:
        txt = re.sub(r, p, txt, flags=re.IGNORECASE)
    if not txt:
        txt = otxt
    return txt
```

通过非英文词汇占比是否超过0.7来判断是否是中文
```python
def question(self, txt, tbl="qa", min_match: float = 0.6):
    txt = re.sub(
        r"[ :|\r\n\t,，。？?/`!！&^%%()\[\]{}<>]+",
        " ",
        rag_tokenizer.tradi2simp(rag_tokenizer.strQ2B(txt.lower())),
    ).strip()
    txt = FulltextQueryer.rmWWW(txt)

    if not self.isChinese(txt):
        #...

def isChinese(line):
    arr = re.split(r"[ \t]+", line)
    if len(arr) <= 3:
        return True
    e = 0
    for t in arr:
        if not re.match(r"[a-zA-Z]+$", t):
            e += 1
    return e * 1.0 / len(arr) >= 0.7
```

接下来会区分两种状态，进行**中文**和**非中文**的区分处理

##### 存在非中文时

**词频权重计算：** 通过`rag_tokenizer.tokenize(txt)`进行分词，通过`tw.weights`获取 `TF-IDF`值

> TF（词频）：某个词在文章中出现次数 / 文章的总词数，也就是出现词的次数越多，则该值越大
>
> IDF（逆文档频率）：与 `语料库的文档总数 / 包含该词的文档数` 的值成正比，也就是出现词的次数越多，则该值越小

```python
tks = rag_tokenizer.tokenize(txt).split()
tks_w = self.tw.weights(tks, preprocess=False)
```

然后对 `tks_w` 进行：
- 删除特殊字符：空格、反斜杠（\）、双引号（"）、单引号（'）、脱字符（^）
- 删除单字符的小写字母或者数字，比如 "a" -> "" ， "3" -> "" , "an" -> "an" 
- 删除开头的加号或者减号
- 删除首尾空格 + 去除内容为空字符串的数据

```python
tks_w = [(re.sub(r"[ \\\"'^]", "", tk), w) for tk, w in tks_w]
tks_w = [(re.sub(r"^[a-z0-9]$", "", tk), w) for tk, w in tks_w if tk]
tks_w = [(re.sub(r"^[\+-]", "", tk), w) for tk, w in tks_w if tk]
tks_w = [(tk.strip(), w) for tk, w in tks_w if tk.strip()]
```


**对获取到的词频权重进行遍历**
- 使用 `self.syn.lookup()` 获取同义词
- 对获取到的同义词进行分词，并且加入到 `keywords` 中
- 同义词的权重降低为原来词语的 `w/4`，即 `1/4`：`syn = ["\"{}\"^{:.4f}".format(s, w / 4.) for s in syn if s.strip()]`
- 最终将同义词组成字符格式放入到 `syns` 中
- 原始词（`tk`)、权重（`w`)、同义词（`syn`)组合成查询表达式
- 使用 `q.append(...)` 进行相邻短语的生成
- `q.append(txt)` 加入原始完整词语 `txt`
- `query = " ".join(q)` 处理最终结果，然后返回一个对象数据 `MatchTextExpr` 和 `keywords`


```python
txt = FulltextQueryer.rmWWW(txt)
tks = rag_tokenizer.tokenize(txt).split()
keywords = [t for t in tks if t]
syns = []

for tk, w in tks_w[:256]:
    syn = self.syn.lookup(tk)
    syn = rag_tokenizer.tokenize(" ".join(syn)).split()
    keywords.extend(syn)
    syn = ["\"{}\"^{:.4f}".format(s, w / 4.) for s in syn if s.strip()]
    syns.append(" ".join(syn))

q = ["({}^{:.4f}".format(tk, w) + " {})".format(syn) for (tk, w), syn in zip(tks_w, syns) if
        tk and not re.match(r"[.^+\(\)-]", tk)]

for i in range(1, len(tks_w)):
    q.append(
        '"%s %s"^%.4f'
        % (
            tks_w[i - 1][0],
            tks_w[i][0],
            max(tks_w[i - 1][1], tks_w[i][1]) * 2,
        )
    )
if not q:
    q.append(txt)
query = " ".join(q)
return MatchTextExpr(
    self.query_fields, query, 100
), keywords
```

> 上面流程有一些细节方法还是比较复杂，下面我们将针对上面的细节小点具体展开分析

**self.syn.lookup()：获取同义词**
如果匹配纯小字母的 `tk`，则
- 通过 `wordnet.synsets()` 获取跨语言同义词集
- `syn.name().split(".")[0]` 提取出基础词性，比如 `word.a.01` -> `word`
- 下划线替换为空格：`re.sub("_", " ", xx)`
- set集合去除原来的词语 `tk`
- 返回当前处理好的同义词数组

> omw-1.4是自然语言处理工具库NLTK中一个重要的多语言语料库，通过 `wordnet.synsets()` 获取跨语言同义词集（例如中文"汽车"对应英语"car"）

```python
def lookup(self, tk, topn=8):
    if re.match(r"[a-z]+$", tk):
        res = list(set([re.sub("_", " ", syn.name().split(".")[0]) for syn in wordnet.synsets(tk)]) - set([tk]))
        return [t for t in res if t]

    self.lookup_num += 1
    self.load()
    res = self.dictionary.get(re.sub(r"[ \t]+", " ", tk.lower()), [])
    if isinstance(res, str):
        res = [res]
    return res[:topn]
```

如果不是纯小字母的 `tk`，则通过 `self.dictionary.get()` 从同义词文件中加载出对应的数据，如果超过了 `topn`，则进行结果截断

> `re.sub(r"[ \t]+", " ", tk.lower())` 合并连续空白符


![img.png](img.png)

-------

**原始词（`tk`)、权重（`w`)、同义词（`syn`)组合成查询表达式：**

- 通过 `re.match(r"[.^+\(\)-]", tk)` 排除包含特殊字符的词项（包括 `.`、`^`、`+`、`()`、`-`)
- 第一个 `format` 保留4位小数，如 `0.63232323` -> `0.6323`，并且使用 `^` 拼接字符串
- 第二个 `format` 实现 字符串的空格 拼接

> 整体流程如下面代码中的实例所示

```python
q = ["({}^{:.4f}".format(tk, w) + " {})".format(syn) for (tk, w), syn in zip(tks_w, syns) if
        tk and not re.match(r"[.^+\(\)-]", tk)]

# 示例
tks_w = [("data", 0.6), 
         ("analysis", 0.9), 
         ("machine+learning", 0.85)]  # 原始词项及权重
syns = ["dataset", "statistics", "AI"]  # 对应的同义词
q = [
    "(data^0.6000 dataset)",
    "(analysis^0.9000 statistics)",
    # "machine+learning"存在非法符号+被排除
]
```

-------

**组合相邻短语：**
继续使用上面的示例，从下面示例我们可以看出，就是从 `index=1` 开始遍历，然后不断组合 `前后两个单词` + `两个单词最大权重的数字 * 2`

```python
for i in range(1, len(tks_w)):
    q.append(
        '"%s %s"^%.4f'
        % (
            tks_w[i - 1][0],
            tks_w[i][0],
            max(tks_w[i - 1][1], tks_w[i][1]) * 2,
        )
    )

# 示例
result = [
    '"data analysis"^1.8000',  # max(0.6, 0.9)*2=1.8
    '"analysis machine_learning"^1.7000'  # max(0.9, 0.85)*2=1.7
]
```


##### 只存在中文时


由于这个流程比较复杂，因此我们先熟悉两个方法的具体逻辑
- self.tw.split(txt)
- fine_grained_tokenize()

**self.tw.split(txt)：** 分割文本时保留复合词或者实体，同时避免破坏函数名等关键标识符

合并连续空格/制表符尾 -> 单个空格 => split(空格)进行切分
- 如果前一个单词以字母结尾 + 当前单词以字母结尾 + 当前单词不是函数名称 + 前一个单词名称不是函数名称，则进行合并，比如"Hello World" => "Hello World"
- 如果不符合上面的情况，则需要切割，形成两个字符存放在 `thk` 中，比如"Hello functionNameOne" => '["Hello", "functionNameOne"]'

> `self.ne.get(t, "")` 也是一个文件的JSON数据的解析

```python
def split(self, txt):
    tks = []
    for t in re.sub(r"[ \t]+", " ", txt).split():
        if tks and re.match(r".*[a-zA-Z]$", tks[-1]) and \
            re.match(r".*[a-zA-Z]$", t) and tks and \
            self.ne.get(t, "") != "func" and self.ne.get(tks[-1], "") != "func":
            tks[-1] = tks[-1] + " " + t
        else:
            tks.append(t)
    return tks

# self.ne的来源
fnm = os.path.join(get_project_base_directory(), "rag/res")
self.ne, self.df = {}, {}
try:
    self.ne = json.load(open(os.path.join(fnm, "ner.json"), "r"))
```


**fine_grained_tokenize()：**

首先判断不需要分词的情况，直接返回
- 如果中文占比小于0.2：`if zh_num < len(tks) * 0.2`
- 如果长度小于3+纯数字加逗号、横线、点(比如"12,345.67-89") 或者 长度大于10：`if len(tk) < 3 or re.match(r"[0-9,\.-]+$", tk)` + `if len(tk) > 10`

当遇到需要分词的情况时
- 递归生成所有可能的分词结果：`self.dfs_(tk, 0, [], tkslist)` => `dfs_`使用`self.trie`（也就是 `rag/res/huqie.txt.trie`预定义词库）快速匹配
- 对所有结果进行评分：`B / len(tks) + L + F`，即基于词频权重 + 词长进行评分
- 优先选择高频词组合
- 最终进行英文数据的标准化，比如 "cats"-> "cat"等等


```python
def fine_grained_tokenize(self, tks):
    tks = tks.split()
    zh_num = len([1 for c in tks if c and is_chinese(c[0])])
    if zh_num < len(tks) * 0.2:
        res = []
        for tk in tks:
            res.extend(tk.split("/"))
        return " ".join(res)

    res = []
    for tk in tks:
        if len(tk) < 3 or re.match(r"[0-9,\.-]+$", tk):
            res.append(tk)
            continue
        tkslist = []
        if len(tk) > 10:
            tkslist.append(tk)
        else:
            self.dfs_(tk, 0, [], tkslist)
        if len(tkslist) < 2:
            res.append(tk)
            continue
        stk = self.sortTks_(tkslist)[1][0]
        if len(stk) == len(tk):
            stk = tk
        else:
            if re.match(r"[a-z\.-]+$", tk):
                for t in stk:
                    if len(t) < 3:
                        stk = tk
                        break
                else:
                    stk = " ".join(stk)
            else:
                stk = " ".join(stk)

        res.append(stk)

    return " ".join(self.english_normalize_(res))

def sortTks_(self, tkslist):
    res = []
    for tfts in tkslist:
        tks, s = self.score_(tfts)
        res.append((tks, s))
    return sorted(res, key=lambda x: x[1], reverse=True)
def score_(self, tfts):
    B = 30
    F, L, tks = 0, 0, []
    for tk, (freq, tag) in tfts:
        F += freq
        L += 0 if len(tk) < 2 else 1
        tks.append(tk)
    #F /= len(tks)
    L /= len(tks)
    logging.debug("[SC] {} {} {} {} {}".format(tks, len(tks), L, F, B / len(tks) + L + F))
    return tks, B / len(tks) + L + F
```

回到我们对中文问题的流程讨论

首先对文本进行了切割：`for tt in self.tw.split(txt)[:256]:`

然后遍历所有切割出来的单词 `tt`
- 记录目前切割的数据：`keywords.append(tt)`
- 获取该词的权重数组：`twts = self.tw.weights([tt])`
- 获取该词的同义词：`syns = self.syn.lookup(tt)`
- 添加同义词数组 `syns` 到 `keyworks`

----

- 然后按照 `权重降序` 排序（因为切割出来的可能是多个词），然后对目前的词 `tk` 再进行细粒度分词：`rag_tokenizer.fine_grained_tokenize(tk).split()` 得到 `sm` 数组
- 对 `sm` 数组去除特殊字符，然后添加到 `keyworks` 中
  
----


- 对当前的词 `tk` 获取它的同义词：`tk_syns = self.syn.lookup(tk)`
- 去除特殊字符后，加入到  `keyworks` 中
  

----

- 遍历所有同义词 `tk_syns`，进行细粒度分词：`tk_syns = [rag_tokenizer.fine_grained_tokenize(tk_syns的item).split()]`
- 遍历所有`tk_syns`，如果某一个 `item` 是有空格的，比如 `Hello World` -> `"Hello World"`，则加上引号 ==> 提升精确匹配
- 使用 `OR` 连接原始词+同义词（权重^0.2）：`tk = f"({tk} OR (%s)^0.2)" % " ".join(tk_syns)`
- 然后将 `tk` 存放在 `tms` 中
-----

最终进行数据的拼接：各种权重的配置拼接

```python
tms = " ".join([f"({t})^{w}" for t, w in tms])
if len(twts) > 1:
    tms += ' ("%s"~2)^1.5' % rag_tokenizer.tokenize(tt)
syns = " OR ".join(
            [
                '"%s"'
                % rag_tokenizer.tokenize(FulltextQueryer.subSpecialChar(s))
                for s in syns
            ]
        )
if syns and tms:
    tms = f"({tms})^5 OR ({syns})^0.7"

qs.append(tms)

if qs:
    query = " OR ".join([f"({t})" for t in qs if t])
    return MatchTextExpr(
        self.query_fields, query, 100, {"minimum_should_match": min_match}
    ), keywords
```

#### self.get_vector转化原始问题为向量

直接使用向量模型转化问题为向量，检查维度 `shape` 是否为 1，然后将向量数据转化为 `float` 数据数组，动态生成向量列名：`q_{向量长度}_vec`

```python
def get_vector(self, txt, emb_mdl, topk=10, similarity=0.1):
    qv, _ = emb_mdl.encode_queries(txt)
    shape = np.array(qv).shape
    if len(shape) > 1:
        raise Exception(
            f"Dealer.get_vector returned array's shape {shape} doesn't match expectation(exact one dimension).")
    embedding_data = [get_float(v) for v in qv]
    vector_column_name = f"q_{len(embedding_data)}_vec"
    return MatchDenseExpr(vector_column_name, embedding_data, 'float', 'cosine', topk, {"similarity": similarity})
```

#### self.dataStore.search使用原始问题向量和扩展问题后的数据进行检索

`this.dataStore`本质就是 `ES/Infinity`，直接调用了对应的 `search()` 方法进行检索

```python
if lower_case_doc_engine == "elasticsearch":
    docStoreConn = rag.utils.es_conn.ESConnection()
elif lower_case_doc_engine == "infinity":
    docStoreConn = rag.utils.infinity_conn.InfinityConnection()
else:
    raise Exception(f"Not supported doc engine: {DOC_ENGINE}")

retrievaler = search.Dealer(docStoreConn)
```

-----


### rerank_by_model()重排序

### rerank()重排序




## 知识图谱增强检索核心实现分析