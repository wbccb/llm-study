> 由于RAGFlow的代码更新非常频繁，因此强调本文所展示的源码和分析是基于2025/4/2 13:56的代码进行，随着时间的推移可能有所改变

# 多模态检索策略

**相关请求API：**
- POST /v1/chunk/retrieval_test
- POST http://localhost:11434/api/chat 
- POST /v1/conversation/ask
- POST /v1/conversation/related_questions

**核心步骤：**
用户问题 -> 关键词扩展 -> 向量检索 -> 重排序 -> 知识图谱补充 -> 结果融合


## 关键技术简述

**1.混合召回层**
| 技术  |  算法 |    所在的文件 |
|---|---|---|
| 关键词召回  |  BM25 算法 |    conf/retrieval/bm25 |
|  向量召回 | 双塔模型（sentence-BERT）  | models/embedding/ |
|  全文检索 | Elasticsearch DSL  | service_conf.yaml |

**2.重排序机制**
- 多路分数融合（线性加权/动态调整）
- 基于 Cohere Rerank 的语义相关性优化
- 时效性权重调节（对新闻类文档特殊处理）

**3.缓存优化**
- Redis 缓存高频查询结果（docker-compose-base.yml 包含 Redis 服务）
- 异步预取热点文档向量


## 检索的整体流程分析

初始化数据，包括各种权重配置以及对应的模型，包括
- question: 检索的问题
- kb_ids: 检索的知识库
- similarity_threshold: 相似度阀值，默认为0.2，可以用来过滤低质量结果
- vector_similarity_weight: 向量权重，默认为0.3，调整语义与关键词检索的混合权重
- embd_mdl: 向量模型
- rerank_mdl: 重排序模型
- chat_mdl: 聊天模型（比如deepseek-r1:7b）


### 关键词扩展
通过LLM提取出问题question的关键词，提高召回率
> 召回率是衡量RAG系统性能的关键指标之一，它表示系统能够检索到的相关文档占所有相关文档的比例
```python
if req.get("keyword", False):
    chat_mdl = LLMBundle(kb.tenant_id, LLMType.CHAT)
    question += keyword_extraction(chat_mdl, question)
```

### 向量检索

封装了混合搜索的逻辑
- 向量检索
- 重排序优化

> 具体逻辑在后续小点中展开

```python
ranks = settings.retrievaler.retrieval(
    question,
    embd_mdl,
    ...
    similarity_threshold,
    vector_similarity_weight,
    top,
    doc_ids,
    rerank_mdl=rerank_mdl,
    highlight=highlight,
    rank_feature=label_question(question, kbs)
)
```

### 知识图谱检索增强

将知识图谱检索结果插入到 `ranks`的最前面位置

> 知识图谱领域知识补充，为了解决LLM的幻觉问题


```python
if use_kg:
    ck = settings.kg_retrievaler.retrieval(...)
    ranks["chunks"].insert(0, ck)

    if ck["content_with_weight"]:
        ranks["chunks"].insert(0, ck)
```


### 检索后的数据处理

- 移除 `vector` 字段
- 重命名名称，返回标准化的格式

```python
for c in ranks["chunks"]:
    c.pop("vector", None)
##rename keys
renamed_chunks = []
for chunk in ranks["chunks"]:
    key_mapping = {
        "chunk_id": "id",
        "content_with_weight": "content",
        "doc_id": "document_id",
        "important_kwd": "important_keywords",
        "question_kwd": "questions",
        "docnm_kwd": "document_keyword",
        "kb_id":"dataset_id"
    }
    rename_chunk = {}
    for key, value in chunk.items():
        new_key = key_mapping.get(key, key)
        rename_chunk[new_key] = value
    renamed_chunks.append(rename_chunk)
ranks["chunks"] = renamed_chunks
return get_result(data=ranks)

def get_result(code=settings.RetCode.SUCCESS, message="", data=None):
    if code == 0:
        if data is not None:
            response = {"code": code, "data": data}
        else:
            response = {"code": code}
    else:
        response = {"code": code, "message": message}
    return jsonify(response)
```


## 混合检索核心实现分析

```python
ranks = settings.retrievaler.retrieval()
```

由上面的分析可以知道，我们使用了 `retrieval()` 进行混合检索


在 `rag/nlp/search.py` 的 `retrieval()` 代码逻辑也非常简单：

- **self.search()** ：进行 `向量搜索` 与 `关键词匹配（BM25改进版）` 的混合检索
- **self.rerank_by_model()** / **self.rerand()**：对结果进行重排序
- 重排序后的结果处理：`similarity_threshold` 过滤低质量答案，格式化高亮文本，构建docnm_kwd统计匹配次数，`sorted`根据`count`进行重新排序优化，然后返回处理后的 `ranks`（检索的数据主要放在 `ranks["chunks"]` 数组中

### search()

先进行了 `question` 的获取


> self.dataStore = ES/Infinity
**如果question为空**
直接进行全字段搜索（即 `matchExprs` 为空）

**如果question不为空**
- 先使用 `self.qryr.question` 对问题进行处理
- 然后判断向量模型是否存在
  - 如果不存在，`matchExprs` 拼接问题数据，直接对问题进行 `ES/Infinity` 检索
  - 如果存在，则使用 `self.get_vector()` 将问题转化为向量，然后进行 `matchExprs` 的构建（问题、问题向量、权重参数配置），然后进行 `ES/Infinity` 检索
    * 如果结果为空，则放宽 `min_match=0.1` （关键词阈值）和降低 `similarity=0.7`（向量相似度要求）再度进行检索
- 对提取出来的问题关键字 `keywork` 匹配搜索出来的内容进行高亮显示
- 按照 `docnm_kwd` 字段统计文档命中次数
- 返回 `return self.SearchResult(query_vector=q_vec, ...)` 格式化好的object数据

> 向量与关键词权重设为0.05:0.95，侧重语义匹配

```python
def search()
    qst = req.get("question", "")
    if not qst:
        res = self.dataStore.search(src, [], filters, [], orderBy, offset, limit, idx_names, kb_ids)
        total = self.dataStore.getTotal(res)
    else:
        highlightFields = ["content_ltks", "title_tks"] if highlight else []
        matchText, keywords = self.qryr.question(qst, min_match=0.3)
        if emb_mdl is None:
            matchExprs = [matchText]
            res = self.dataStore.search(src, highlightFields, filters, matchExprs, orderBy, offset, limit,
                                idx_names, kb_ids, rank_feature=rank_feature)
        else:
            matchDense = self.get_vector(qst, emb_mdl, topk, req.get("similarity", 0.1))
            q_vec = matchDense.embedding_data
            src.append(f"q_{len(q_vec)}_vec")
            fusionExpr = FusionExpr("weighted_sum", topk, {"weights": "0.05, 0.95"})
            matchExprs = [matchText, matchDense, fusionExpr]

            res = self.dataStore.search(src, highlightFields, filters, matchExprs, orderBy, offset, limit,
                                        idx_names, kb_ids, rank_feature=rank_feature)
            if total == 0:
                matchText, _ = self.qryr.question(qst, min_match=0.1)
                matchDense.extra_options["similarity"] = 0.17
                res = self.dataStore.search(src, highlightFields, filters, [matchText, matchDense, fusionExpr],
                                    orderBy, offset, limit, idx_names, kb_ids, rank_feature=rank_feature)
```


#### self.qryr.question

传入查询的问题文本，进行一系列的处理：
- 输入预处理：`移除标点、换行符等噪声，全角转半角` + `去除中英文常见虚词和语气词`
- 意图解析
- 语义扩展

##### 输入预处理

标准化输入格式（全角转半角、停用词过滤）

```python
txt = re.sub(
    r"[ :|\r\n\t,，。？?/`!！&^%%()\[\]{}<>]+",
    " ",
    rag_tokenizer.tradi2simp(rag_tokenizer.strQ2B(txt.lower())),
).strip()
txt = FulltextQueryer.rmWWW(txt)
```

停用词过滤 `rmWWW()` ：
- 中文停用词过滤："请问什么是深度学习" -> "深度学习"
- 英语疑问词过滤："whatis" -> "is"
- 英文停用词清除："The book" -> "book"

```python
@staticmethod
def rmWWW(txt):
    patts = [
        (
            r"是*(什么样的|哪家|一下|那家|请问|啥样|咋样了|什么时候|何时|何地|何人|是否|是不是|多少|哪里|怎么|哪儿|怎么样|如何|哪些|是啥|啥是|啊|吗|呢|吧|咋|什么|有没有|呀|谁|哪位|哪个)是*",
            "",
        ),
        (r"(^| )(what|who|how|which|where|why)('re|'s)? ", " "),
        (
            r"(^| )('s|'re|is|are|were|was|do|does|did|don't|doesn't|didn't|has|have|be|there|you|me|your|my|mine|just|please|may|i|should|would|wouldn't|will|won't|done|go|for|with|so|the|a|an|by|i'm|it's|he's|she's|they|they're|you're|as|by|on|in|at|up|out|down|of|to|or|and|if) ",
            " ")
    ]
    otxt = txt
    for r, p in patts:
        txt = re.sub(r, p, txt, flags=re.IGNORECASE)
    if not txt:
        txt = otxt
    return txt
```

通过非英文词汇占比是否超过0.7来判断是否是中文
```python
def question(self, txt, tbl="qa", min_match: float = 0.6):
    txt = re.sub(
        r"[ :|\r\n\t,，。？?/`!！&^%%()\[\]{}<>]+",
        " ",
        rag_tokenizer.tradi2simp(rag_tokenizer.strQ2B(txt.lower())),
    ).strip()
    txt = FulltextQueryer.rmWWW(txt)

    if not self.isChinese(txt):
        #...

def isChinese(line):
    arr = re.split(r"[ \t]+", line)
    if len(arr) <= 3:
        return True
    e = 0
    for t in arr:
        if not re.match(r"[a-zA-Z]+$", t):
            e += 1
    return e * 1.0 / len(arr) >= 0.7
```

接下来会区分两种状态，进行**中文**和**非中文**的区分处理

##### 存在非中文时

**词频权重计算：** 通过`rag_tokenizer.tokenize(txt)`进行分词，通过`tw.weights`获取 `TF-IDF`值

> TF（词频）：某个词在文章中出现次数 / 文章的总词数，也就是出现词的次数越多，则该值越大
>
> IDF（逆文档频率）：与 `语料库的文档总数 / 包含该词的文档数` 的值成正比，也就是出现词的次数越多，则该值越小

```python
tks = rag_tokenizer.tokenize(txt).split()
tks_w = self.tw.weights(tks, preprocess=False)
```

然后对 `tks_w` 进行：
- 删除特殊字符：空格、反斜杠（\）、双引号（"）、单引号（'）、脱字符（^）
- 删除单字符的小写字母或者数字，比如 "a" -> "" ， "3" -> "" , "an" -> "an" 
- 删除开头的加号或者减号
- 删除首尾空格 + 去除内容为空字符串的数据

```python
tks_w = [(re.sub(r"[ \\\"'^]", "", tk), w) for tk, w in tks_w]
tks_w = [(re.sub(r"^[a-z0-9]$", "", tk), w) for tk, w in tks_w if tk]
tks_w = [(re.sub(r"^[\+-]", "", tk), w) for tk, w in tks_w if tk]
tks_w = [(tk.strip(), w) for tk, w in tks_w if tk.strip()]
```


**对获取到的词频权重进行遍历**
- 使用 `self.syn.lookup()` 获取同义词
- 对获取到的同义词进行分词，并且加入到 `keywords` 中
- 同义词的权重降低为原来词语的 `w/4`，即 `1/4`：`syn = ["\"{}\"^{:.4f}".format(s, w / 4.) for s in syn if s.strip()]`
- 最终将同义词组成字符格式放入到 `syns` 中
- 原始词（`tk`)、权重（`w`)、同义词（`syn`)组合成查询表达式
- 使用 `q.append(...)` 进行相邻短语的生成
- `q.append(txt)` 加入原始完整词语 `txt`
- `query = " ".join(q)` 处理最终结果，然后返回一个对象数据 `MatchTextExpr` 和 `keywords`


```python
txt = FulltextQueryer.rmWWW(txt)
tks = rag_tokenizer.tokenize(txt).split()
keywords = [t for t in tks if t]
syns = []

for tk, w in tks_w[:256]:
    syn = self.syn.lookup(tk)
    syn = rag_tokenizer.tokenize(" ".join(syn)).split()
    keywords.extend(syn)
    syn = ["\"{}\"^{:.4f}".format(s, w / 4.) for s in syn if s.strip()]
    syns.append(" ".join(syn))

q = ["({}^{:.4f}".format(tk, w) + " {})".format(syn) for (tk, w), syn in zip(tks_w, syns) if
        tk and not re.match(r"[.^+\(\)-]", tk)]

for i in range(1, len(tks_w)):
    q.append(
        '"%s %s"^%.4f'
        % (
            tks_w[i - 1][0],
            tks_w[i][0],
            max(tks_w[i - 1][1], tks_w[i][1]) * 2,
        )
    )
if not q:
    q.append(txt)
query = " ".join(q)
return MatchTextExpr(
    self.query_fields, query, 100
), keywords
```

> 上面流程有一些细节方法还是比较复杂，下面我们将针对上面的细节小点具体展开分析

**self.syn.lookup()：**


```python
def lookup(self, tk, topn=8):
    if re.match(r"[a-z]+$", tk):
        res = list(set([re.sub("_", " ", syn.name().split(".")[0]) for syn in wordnet.synsets(tk)]) - set([tk]))
        return [t for t in res if t]

    self.lookup_num += 1
    self.load()
    res = self.dictionary.get(re.sub(r"[ \t]+", " ", tk.lower()), [])
    if isinstance(res, str):
        res = [res]
    return res[:topn]
```


![img.png](img.png)


**原始词（`tk`)、权重（`w`)、同义词（`syn`)组合成查询表达式：**

**组合相邻短语：**


------

**意图解析：**







------

**语义扩展：**

------


##### 只存在中文时


#### self.get_vector


#### self.dataStore.search



### rerank_by_model()重排序

### rerank()重排序




## 知识图谱增强检索核心实现分析