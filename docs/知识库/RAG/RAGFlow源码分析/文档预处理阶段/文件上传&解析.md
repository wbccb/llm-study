# 文件上传&解析整体流程分析

## 文件上传

> api/apps/file_app.py：接口的操作文件

> api/db/services/file_service.py： 数据库的文件操作文件

遍历上传的文件，形成 blob 数据后，并且插入到数据库中

```python
file_objs = request.files.getlist('file')
for file_obj in file_objs:
    blob = file_obj.read()
    file = {
        "id": get_uuid(),
        "parent_id": last_folder.id,
        "tenant_id": current_user.id,
        "created_by": current_user.id,
        "type": filetype,
        "name": filename,
        "location": location,
        "size": len(blob),
    }
    file = FileService.insert(file)
```

## 文件解析

- 17:48:34 Task has been received.
- 17:48:38 Page(1~13): Start to parse.
- 17:48:38 Page(1~13): OCR started
- 17:48:42 Page(1~13): OCR finished (4.30s)
- 17:48:59 Page(1~13): Layout analysis (16.26s)
- 17:48:59 Page(1~13): Table analysis (0.14s)
- 17:48:59 Page(1~13): Text merged (0.00s)
- 17:48:59 Page(1~13): Finish parsing.
- 17:49:09 Page(1~13): Generate 29 chunks
- 17:49:15 Page(1~13): Embedding chunks (5.96s)
- 17:49:18 Page(1~13): Indexing done (2.88s). Task done (43.87s)
- 17:48:34 Reused previous task's chunks.

> 接下来我们将按照目前前端提示的顺序进行整体流程的分析

### Task has been received

> `handle_task()` -> `get_task()`中打印 `Task has been received`

上传完成后，点击解析按钮，触发文件解析

> web/src/hooks/document-hooks.ts

```ts
export const useRunNextDocument = () => {
  const queryClient = useQueryClient();

  const {
    data,
    isPending: loading,
    mutateAsync,
  } = useMutation({
    mutationKey: ["runDocumentByIds"],
    mutationFn: async ({
      documentIds,
      run,
      shouldDelete,
    }: {
      documentIds: string[];
      run: number;
      shouldDelete: boolean;
    }) => {
      const ret = await kbService.document_run({
        doc_ids: documentIds,
        run,
        delete: shouldDelete,
      });
      const code = get(ret, "data.code");
      if (code === 0) {
        queryClient.invalidateQueries({ queryKey: ["fetchDocumentList"] });
        message.success(i18n.t("message.operated"));
      }

      return code;
    },
  });

  return { runDocumentByIds: mutateAsync, loading, data };
};
```

最终触发 POST 方法

```ts
document_run: `${api_host}/document/run`;
```

也就是 `api/apps/document_app.py` -> `api/db/services/task_service.py` 的 `queue_tasks(doc)`处理解析

> 而这里的 `doc` 是什么呢？

```python
doc = DocumentService.get_by_id(id)

class CommonService:
    @classmethod
    @DB.connection_context()
    def get_by_id(cls, pid):
        # Get a record by ID
        # Args:
        #     pid: Record ID
        # Returns:
        #     Tuple of (success, record)
        obj = cls.model.get_or_none(cls.model.id == pid)
        if obj:
            return True, obj
        return False, None

    @classmethod
    @DB.connection_context()
    def get_or_none(cls, **kwargs):
        """Get a single record or None if not found.

        This method attempts to retrieve a single record matching the given criteria,
        returning None if no match is found instead of raising an exception.

        Args:
            **kwargs: Filter conditions as keyword arguments.

        Returns:
            Model instance or None: Matching record if found, None otherwise.
        """
        try:
            return cls.model.get(**kwargs)
        except peewee.DoesNotExist:
            return None

class DocumentService(CommonService):
    model = Document
```

`DocumentService` 继承 `CommonService`，本质就是调用了 `Document.get()`

```python
class Document(DataBaseModel):

class DataBaseModel(BaseModel):

class BaseModel(Model):
```

而 `Model` 就是 `peewee.py`：Peewee 是一个轻量级、直观且功能完备的 Python ORM（对象关系映射）库，专门用于简化数据库操作

`Document.get()` 就是从数据库中执行 `SELECT * FROM <table> WHERE <conditions> LIMIT 1`：精确匹配，要求查询条件必须对应唯一结果

#### queue_tasks()

创建并排队文档处理任务。

此函数根据文档的类型和配置为其创建处理任务。

它以不同的方式处理不同的文档类型（PDF、Excel 等），并管理任务分块和配置。它还通过检查以前完成的任务来实现任务重用优化。

参数：

- doc (dict)：包含元数据和配置的文档字典。
- bucket (str)：存储文档的存储桶名称。
- name (str)：文档的文件名。
- priority (int，可选)：任务排队的优先级（默认值为 0）。

注意：

- 对于 PDF 文档，根据配置按页面范围创建任务
- 对于 Excel 文档，按行范围创建任务
- 计算任务摘要以进行优化和重用
- 如果可用，可以重用以前的任务块

比如下面的 pdf 的相关处理，根据页数构建出不同的 `task` 存入到 `parse_task_array` 中

```python
def queue_tasks(doc: dict, bucket: str, name: str, priority: int):
   if doc["type"] == FileType.PDF.value:
        file_bin = STORAGE_IMPL.get(bucket, name)
        do_layout = doc["parser_config"].get("layout_recognize", "DeepDOC")
        pages = PdfParser.total_page_number(doc["name"], file_bin)
        page_size = doc["parser_config"].get("task_page_size", 12)
        if doc["parser_id"] == "paper":
            page_size = doc["parser_config"].get("task_page_size", 22)
        if doc["parser_id"] in ["one", "knowledge_graph"] or do_layout != "DeepDOC":
            page_size = 10 ** 9
        page_ranges = doc["parser_config"].get("pages") or [(1, 10 ** 5)]
        for s, e in page_ranges:
            s -= 1
            s = max(0, s)
            e = min(e - 1, pages)
            for p in range(s, e, page_size):
                task = new_task()
                task["from_page"] = p
                task["to_page"] = min(p + page_size, e)
                parse_task_array.append(task)


   DocumentService.update_by_id(doc["id"], {"chunk_num": ck_num})
   bulk_insert_into_db(Task, parse_task_array, True)
   DocumentService.begin2parse(doc["id"])
```

在 `task_executor.py` 的 `main()`中

**初始化阶段**

- 创建主 nursery 作为任务容器，立即启动 report_status 监控任务（例如周期上报系统指标）。

**任务处理循环**

- 通过 while True 持续监听新任务（如消息队列消费）
- 每次迭代前需通过 task_limiter 获取执行配额（避免突发流量冲击）
- 每次成功获取配额后，启动 handle_task 处理实际业务逻辑

**终止条件**

- 外部取消信号（如 Ctrl+C 触发 trio.Cancelled 异常）
- report_status 或 handle_task 内部抛出未捕获异常

```python
async def main():
    async with trio.open_nursery() as nursery:
        nursery.start_soon(report_status)
        while True:
            async with task_limiter:
                nursery.start_soon(handle_task)

async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        await do_handle_task(task)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    #...
    redis_msg.ack()
```

在 `handle_task()` 中，我们使用 `task = await collect()` 获取任务，如下面代码所示，本质就是 `TaskService.get_task`

```python
async def collect():
    #...
    task = TaskService.get_task(msg["id"])
    if task:
        _, doc = DocumentService.get_by_id(task["doc_id"])
    return redis_msg, task
```

从 `get_task()` 中，我们可以得到第一步的打印信息 `Task has been received`

```python
@classmethod
@DB.connection_context()
def get_task(cls, task_id):
    fields = [
        #...
    ]
    docs = (
        cls.model.select(*fields)
            .join(Document, on=(cls.model.doc_id == Document.id))
            .join(Knowledgebase, on=(Document.kb_id == Knowledgebase.id))
            .join(Tenant, on=(Knowledgebase.tenant_id == Tenant.id))
            .where(cls.model.id == task_id)
    )
    docs = list(docs.dicts())
    #...
    msg = f"\n{datetime.now().strftime('%H:%M:%S')} Task has been received."
    cls.model.update(
        progress_msg=cls.model.progress_msg + msg,
        progress=prog,
        retry_count=docs[0]["retry_count"] + 1,
    ).where(cls.model.id == docs[0]["id"]).execute()
    #...
    return docs[0]
```

#### 总结

- 我们上传文件并且点击解析按钮后，会创建对应的任务存入对应的数据库
- 然后不断轮询触发任务的处理
- 当我们获取到当前的任务时，会打印出 `Task has been received`

### Start to parse

> 在 `build_chunks()` -> ``中打印 `Start to parse`

当我们拿到对应的 task 后，我们会触发 `do_handle_task()`

```python
async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        await do_handle_task(task)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    #...
    redis_msg.ack()
```


在 `do_handle_task()`中，我们先进行了一些数据的初始化、引擎的检查以及是否有 task_canceled

然后进行 `embedding_model` 的加载验证，知识库的初始化

根据不同条件，主要分为
- `task.get("task_type", "") == "raptor"`：执行 `run_raptor(task, chat_model, embedding_model, vector_size, progress_callback)`
- `task.get("task_type", "") == "graphrag"`：执行 `run_graphrag(task, task_language, with_resolution, with_community, chat_model, embedding_model, progress_callback)`
- 标准处理chunk处理方式：`build_chunks()` + `embedding()` + 批量插入chunks到 `Elasticsearch/Infinity`

> RAPTOR 一种基于树的RAG方法，准确率提高 20%
>
> RAG 是当前使用LLM的标准方法，大多数现有方法仅从检索语料库中检索短的连续块，限制了对整个文档上下文的整体理解。
>
> 最近，一种名为 RAPTOR （Recursive Abstractive Processing for Tree-Organized Retrieval）方法提出来，该方法核心思想是将doc构建为一棵树，然后逐层递归的查询


> GraphRAG是微软提出来的RAG+graph框架，使用 LLM 生成的知识图谱，在对复杂信息进行文档分析时显著提高问答性能。
> 
> 基础RAG的缺点：
>
> - 基础 RAG 难以将各个点连接起来。当回答问题需要通过共享属性遍历不同的信息片段以提供新的综合见解时，就会发生这种情况。
> - 当被要求全面理解大型数据集合甚至单个大型文档中的总结语义概念时，基础 RAG 的表现不佳。（比如让模型回答未训练过的《西游记》一书的内容，若问孙悟空被天庭封为什么官，那么RAG到相似的文档即可回答。若问西游戏整本书在讲什么，那么RAG的效果就很差，取任何chunk都没办法总结全书的内容）



> 接下来我们将针对 `build_chunks()` 和 `embedding()` 进行详细的分析



#### build_chunks

主要分为：
- 分块器初始化
- 异步获取文件内容
- 同步分块操作转为异步任务
- chunks生成完成后，执行一系列工作
  - 关键词生成
  - 问题生成
  - 标签生成


##### 分块器初始化

从 `FACTORY` 中根据类型获取对应的分块器 `chunker`，支持动态适配不同的分块策略

```python
chunker = FACTORY[task["parser_id"].lower()]
```

```python
FACTORY = {
    "general": naive,
    ParserType.NAIVE.value: naive,
    ParserType.PAPER.value: paper,
    ParserType.BOOK.value: book,
    ParserType.PRESENTATION.value: presentation,
    ParserType.MANUAL.value: manual,
    ParserType.LAWS.value: laws,
    ParserType.QA.value: qa,
    ParserType.TABLE.value: table,
    ParserType.RESUME.value: resume,
    ParserType.PICTURE.value: picture,
    ParserType.ONE.value: one,
    ParserType.AUDIO.value: audio,
    ParserType.EMAIL.value: email,
    ParserType.KG.value: naive,
    ParserType.TAG.value: tag
}
```

##### 异步获取文件内容

通过 `File2DocumentService` 获取文件存储地址（MinIO)，使用 `get_storage_binary()` 获取该文件的二进制数据

```python
bucket, name = File2DocumentService.get_storage_address(doc_id=task["doc_id"])
binary = await get_storage_binary(bucket, name)
logging.info("From minio({}) {}/{}".format(timer() - st, task["location"], task["name"]))
```

##### 同步分块操作转为异步任务

使用 `chunk_limiter` 限制并发量，通过 `trio.to_thread.run_sync` 将同步方法 `chunker.chunk()` 包装为异步任务，分块参数包括
- 页码范围
- 语言配置
- ....

```python
async with chunk_limiter:
    cks = await trio.to_thread.run_sync(lambda: chunker.chunk(task["name"], binary=binary, from_page=task["from_page"],
                        to_page=task["to_page"], lang=task["language"], callback=progress_callback,
                        kb_id=task["kb_id"], parser_config=task["parser_config"], tenant_id=task["tenant_id"]))
logging.info("Chunking({}) {}/{} done".format(timer() - st, task["location"], task["name"]))
```

##### 关键词生成

检测缓存，如果没有缓存，则调用大模型从所有分块中进行关键词的提取，然后存入缓存中

```python
if task["parser_config"].get("auto_keywords", 0):
    cached = await trio.to_thread.run_sync(lambda: keyword_extraction(chat_mdl, d["content_with_weight"], topn))

def keyword_extraction(chat_mdl, content, topn=3):
    prompt = f"""
Role: You're a text analyzer.
Task: extract the most important keywords/phrases of a given piece of text content.
Requirements:
  - Summarize the text content, and give top {topn} important keywords/phrases.
  - The keywords MUST be in language of the given piece of text content.
  - The keywords are delimited by ENGLISH COMMA.
  - Keywords ONLY in output.

### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.2})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        return ""
    return kwd
```

##### 问题生成

跟上面 `关键词生成` 类似，调用大模型进行问题的生成，然后存入缓存中

```python
if task["parser_config"].get("auto_questions", 0):
    cached = await trio.to_thread.run_sync(lambda: question_proposal(chat_mdl, d["content_with_weight"], topn))


def question_proposal(chat_mdl, content, topn=3):
    prompt = f"""
Role: You're a text analyzer.
Task:  propose {topn} questions about a given piece of text content.
Requirements:
  - Understand and summarize the text content, and propose top {topn} important questions.
  - The questions SHOULD NOT have overlapping meanings.
  - The questions SHOULD cover the main content of the text as much as possible.
  - The questions MUST be in language of the given piece of text content.
  - One question per line.
  - Question ONLY in output.

### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.2})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        return ""
    return kwd

```


##### 标签生成

跟上面 `关键词生成` 类似，调用大模型进行标签的生成，然后存入缓存中



```python
if task["kb_parser_config"].get("tag_kb_ids", []):
    cached = await trio.to_thread.run_sync(lambda: content_tagging(chat_mdl, d["content_with_weight"], all_tags, picked_examples, topn=topn_tags))


def content_tagging(chat_mdl, content, all_tags, examples, topn=3):
    prompt = f"""
Role: You're a text analyzer.

Task: Tag (put on some labels) to a given piece of text content based on the examples and the entire tag set.

Steps::
  - Comprehend the tag/label set.
  - Comprehend examples which all consist of both text content and assigned tags with relevance score in format of JSON.
  - Summarize the text content, and tag it with top {topn} most relevant tags from the set of tag/label and the corresponding relevance score.

Requirements
  - The tags MUST be from the tag set.
  - The output MUST be in JSON format only, the key is tag and the value is its relevance score.
  - The relevance score must be range from 1 to 10.
  - Keywords ONLY in output.

# TAG SET
{", ".join(all_tags)}

"""
    for i, ex in enumerate(examples):
        prompt += """
# Examples {}
### Text Content
{}

Output:
{}

        """.format(i, ex["content"], json.dumps(ex[TAG_FLD], indent=2, ensure_ascii=False))

    prompt += f"""
# Real Data
### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.5})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        raise Exception(kwd)

    try:
        return json_repair.loads(kwd)
    except json_repair.JSONDecodeError:
        try:
            result = kwd.replace(prompt[:-1], '').replace('user', '').replace('model', '').strip()
            result = '{' + result.split('{')[1].split('}')[0] + '}'
            return json_repair.loads(result)
        except Exception as e:
            logging.exception(f"JSON parsing error: {result} -> {e}")
            raise e

```

### chunker.chunk()

在上面的分析中，我们知道  `build_chunks()` -> `chunker.chunk()` 中打印 `Start to parse`

> 而 `chunker.chunk()` 具体执行了什么内容呢？


`chunker.chunk()`主要根据扩展名选择对应的解析器进行chunk的构建，包括：
- DOCX文件
- PDF文件
- EXCEL文件（CSV/XLSX）
- 纯文本文件（TXT/代码文件）
- Markdown/HTML/JSON文件
- 其它格式：DOC
- 未支持的格式，抛出错误
----------------

接下来我们针对 `PDF文件` 的解析展开分析

> 针对打印信息进行小点的划分

先获取配置数据，得到 `PDF解析策略`：
- DeepDOC
- Plain Text
- 其它

```python
layout_recognizer = parser_config.get("layout_recognize", "DeepDOC")
```

#### DeepDOC模式

使用可视化模型提取 
- sections段落
- tables表格
- figures图表
```python
pdf_parser = Pdf()
sections, tables, figures = pdf_parser(filename if not binary else binary, from_page=from_page, to_page=to_page, callback=callback, separate_tables_figures=True)
```

------------------

而 `Pdf()` 以及 `pdf_parser()`，如下面代码所示，会依次执行
- OCR started
- Layout analysis
- Table analysis
- Text merged

```python
class Pdf(PdfParser):
    def __init__(self):
        super().__init__()

    def __call__(self, filename, binary=None, from_page=0,
                 to_page=100000, zoomin=3, callback=None, separate_tables_figures=False):
        start = timer()
        first_start = start
        callback(msg="OCR started")
        self.__images__(
            filename if not binary else binary,
            zoomin,
            from_page,
            to_page,
            callback
        )
        callback(msg="OCR finished ({:.2f}s)".format(timer() - start))
        logging.info("OCR({}~{}): {:.2f}s".format(from_page, to_page, timer() - start))

        start = timer()
        self._layouts_rec(zoomin)
        callback(0.63, "Layout analysis ({:.2f}s)".format(timer() - start))

        start = timer()
        self._table_transformer_job(zoomin)
        callback(0.65, "Table analysis ({:.2f}s)".format(timer() - start))

        start = timer()
        self._text_merge()
        callback(0.67, "Text merged ({:.2f}s)".format(timer() - start))

        if separate_tables_figures:
            tbls, figures = self._extract_table_figure(True, zoomin, True, True, True)
            self._concat_downward()
            logging.info("layouts cost: {}s".format(timer() - first_start))
            return [(b["text"], self._line_tag(b, zoomin)) for b in self.boxes], tbls, figures
        else:
            tbls = self._extract_table_figure(True, zoomin, True, True)
            # self._naive_vertical_merge()
            self._concat_downward()
            # self._filter_forpages()
            logging.info("layouts cost: {}s".format(timer() - first_start))
            return [(b["text"], self._line_tag(b, zoomin)) for b in self.boxes], tbls
```
------------------

然后再使用可视化图表模型对 `figures图表` 进行再度提取存入到 `tables` 中
```python
pdf_vision_parser = VisionFigureParser(vision_model=vision_model, figures_data=figures, **kwargs)
boosted_figures = pdf_vision_parser(callback=callback)
tables.extend(boosted_figures)
```

然后再对 `tables表格` 数据进行处理
- 空值过滤
- 单行表格处理
- 多行表格批处理

对表格数据进行结构化处理，转化为适合机器学习模型处理的Token化文档，同时保留多模态信息（如图像、位置坐标等）
- `tokenize`：HTML标签清理、rag_tokenizer.tokenize进行粗粒度分词、rag_tokenizer.fine_grained_tokenize进行细粒度分词
- `d["image"]`：图像绑定，支持跨模态检索（图文联合检索）
- `add_positions(d, poss)`：记录单元格位置，可用于高亮布局或者布局还原

```python
res = tokenize_table(tables, doc, is_english)

def tokenize_table(tbls, doc, eng, batch_size=10):
    res = []
    # add tables
    for (img, rows), poss in tbls:
        if not rows:
            continue
        if isinstance(rows, str):
            d = copy.deepcopy(doc)
            tokenize(d, rows, eng)
            d["content_with_weight"] = rows
            if img:
                d["image"] = img
            if poss:
                add_positions(d, poss)
            res.append(d)
            continue
        de = "; " if eng else "； "
        for i in range(0, len(rows), batch_size):
            d = copy.deepcopy(doc)
            r = de.join(rows[i:i + batch_size])
            tokenize(d, r, eng)
            d["image"] = img
            add_positions(d, poss)
            res.append(d)
    return res
    
def tokenize(d, t, eng):
    d["content_with_weight"] = t
    t = re.sub(r"</?(table|td|caption|tr|th)( [^<>]{0,12})?>", " ", t)
    d["content_ltks"] = rag_tokenizer.tokenize(t)
    d["content_sm_ltks"] = rag_tokenizer.fine_grained_tokenize(d["content_ltks"])
```

最终打印出
- Finish parsing
结束解析




#### Generate 29 chunks


### Embedding chunks

### Indexing done & Task done

------------

> `Pdf()` 以及 `pdf_parser()`，如下面代码所示，会依次执行
> - OCR started
> - Layout analysis
> - Table analysis
> - Text merged

## OCR
