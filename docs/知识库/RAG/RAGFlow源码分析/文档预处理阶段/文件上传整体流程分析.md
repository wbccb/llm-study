# 文件上传&解析整体流程分析

## 文件上传

> api/apps/file_app.py：接口的操作文件

> api/db/services/file_service.py： 数据库的文件操作文件

遍历上传的文件，形成 blob数据后，并且插入到数据库中

```python
file_objs = request.files.getlist('file')
for file_obj in file_objs:
    blob = file_obj.read()
    file = {
        "id": get_uuid(),
        "parent_id": last_folder.id,
        "tenant_id": current_user.id,
        "created_by": current_user.id,
        "type": filetype,
        "name": filename,
        "location": location,
        "size": len(blob),
    }
    file = FileService.insert(file)
```

## 文件解析

* 17:48:34 Task has been received.
* 17:48:38 Page(1~13): Start to parse.
* 17:48:38 Page(1~13): OCR started
* 17:48:42 Page(1~13): OCR finished (4.30s)
* 17:48:59 Page(1~13): Layout analysis (16.26s)
* 17:48:59 Page(1~13): Table analysis (0.14s)
* 17:48:59 Page(1~13): Text merged (0.00s)
* 17:48:59 Page(1~13): Finish parsing.
* 17:49:09 Page(1~13): Generate 29 chunks
* 17:49:15 Page(1~13): Embedding chunks (5.96s)
* 17:49:18 Page(1~13): Indexing done (2.88s). Task done (43.87s)
* 17:48:34 Reused previous task's chunks.

> 接下来我们将按照目前前端提示的顺序进行整体流程的分析


### Task has been received

上传完成后，点击解析按钮，触发文件解析
> web/src/hooks/document-hooks.ts

```ts
export const useRunNextDocument = () => {
  const queryClient = useQueryClient();

  const {
    data,
    isPending: loading,
    mutateAsync,
  } = useMutation({
    mutationKey: ['runDocumentByIds'],
    mutationFn: async ({
      documentIds,
      run,
      shouldDelete,
    }: {
      documentIds: string[];
      run: number;
      shouldDelete: boolean;
    }) => {
      const ret = await kbService.document_run({
        doc_ids: documentIds,
        run,
        delete: shouldDelete,
      });
      const code = get(ret, 'data.code');
      if (code === 0) {
        queryClient.invalidateQueries({ queryKey: ['fetchDocumentList'] });
        message.success(i18n.t('message.operated'));
      }

      return code;
    },
  });

  return { runDocumentByIds: mutateAsync, loading, data };
};
```

最终触发 POST 方法
```ts
document_run: `${api_host}/document/run`
```


也就是 `api/apps/document_app.py` -> `api/db/services/task_service.py` 的 `queue_tasks(doc)`处理解析


> 而这里的 `doc` 是什么呢？

```python
doc = DocumentService.get_by_id(id)

class CommonService:
    @classmethod
    @DB.connection_context()
    def get_by_id(cls, pid):
        # Get a record by ID
        # Args:
        #     pid: Record ID
        # Returns:
        #     Tuple of (success, record)
        obj = cls.model.get_or_none(cls.model.id == pid)
        if obj:
            return True, obj
        return False, None
    
    @classmethod
    @DB.connection_context()
    def get_or_none(cls, **kwargs):
        """Get a single record or None if not found.
    
        This method attempts to retrieve a single record matching the given criteria,
        returning None if no match is found instead of raising an exception.
    
        Args:
            **kwargs: Filter conditions as keyword arguments.
    
        Returns:
            Model instance or None: Matching record if found, None otherwise.
        """
        try:
            return cls.model.get(**kwargs)
        except peewee.DoesNotExist:
            return None

class DocumentService(CommonService):
    model = Document
```

`DocumentService` 继承 `CommonService`，本质就是调用了 `Document.get()`


```python
class Document(DataBaseModel):

class DataBaseModel(BaseModel):

class BaseModel(Model):
```

而 `Model` 就是 `peewee.py`：Peewee 是一个轻量级、直观且功能完备的 Python ORM（对象关系映射）库，专门用于简化数据库操作

`Document.get()` 就是从数据库中执行 `SELECT * FROM <table> WHERE <conditions> LIMIT 1`：精确匹配，要求查询条件必须对应唯一结果


#### queue_tasks()

创建并排队文档处理任务。

此函数根据文档的类型和配置为其创建处理任务。

它以不同的方式处理不同的文档类型（PDF、Excel 等），并管理任务分块和配置。它还通过检查以前完成的任务来实现任务重用优化。

参数：
* doc (dict)：包含元数据和配置的文档字典。
* bucket (str)：存储文档的存储桶名称。
* name (str)：文档的文件名。
* priority (int，可选)：任务排队的优先级（默认值为 0）。

注意：
- 对于 PDF 文档，根据配置按页面范围创建任务
- 对于 Excel 文档，按行范围创建任务
- 计算任务摘要以进行优化和重用
- 如果可用，可以重用以前的任务块

比如下面的 pdf 的相关处理，根据页数构建出不同的 `task` 存入到 `parse_task_array` 中
```python
def queue_tasks(doc: dict, bucket: str, name: str, priority: int):
   if doc["type"] == FileType.PDF.value:
        file_bin = STORAGE_IMPL.get(bucket, name)
        do_layout = doc["parser_config"].get("layout_recognize", "DeepDOC")
        pages = PdfParser.total_page_number(doc["name"], file_bin)
        page_size = doc["parser_config"].get("task_page_size", 12)
        if doc["parser_id"] == "paper":
            page_size = doc["parser_config"].get("task_page_size", 22)
        if doc["parser_id"] in ["one", "knowledge_graph"] or do_layout != "DeepDOC":
            page_size = 10 ** 9
        page_ranges = doc["parser_config"].get("pages") or [(1, 10 ** 5)]
        for s, e in page_ranges:
            s -= 1
            s = max(0, s)
            e = min(e - 1, pages)
            for p in range(s, e, page_size):
                task = new_task()
                task["from_page"] = p
                task["to_page"] = min(p + page_size, e)
                parse_task_array.append(task)


   DocumentService.update_by_id(doc["id"], {"chunk_num": ck_num})
   bulk_insert_into_db(Task, parse_task_array, True)
   DocumentService.begin2parse(doc["id"])
```


在 `task_executor.py` 的 `main()`中

**初始化阶段**
- 创建主nursery作为任务容器，立即启动report_status监控任务（例如周期上报系统指标）。

**任务处理循环**
- 通过while True持续监听新任务（如消息队列消费）
- 每次迭代前需通过task_limiter获取执行配额（避免突发流量冲击）
- 每次成功获取配额后，启动handle_task处理实际业务逻辑

**终止条件**
- 外部取消信号（如Ctrl+C触发trio.Cancelled异常）
- report_status或handle_task内部抛出未捕获异常

```python
async def main():
    async with trio.open_nursery() as nursery:
        nursery.start_soon(report_status)
        while True:
            async with task_limiter:
                nursery.start_soon(handle_task)

async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        await do_handle_task(task)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    #...
    redis_msg.ack()
```

### Start to parse

### OCR started

### Layout analysis

### Table analysis

### Text merged

### Finish parsing

### Generate 29 chunks

### Embedding chunks

### Indexing done & Task done




