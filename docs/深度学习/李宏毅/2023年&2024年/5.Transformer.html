<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer | 大模型相关学习电子书</title>
    <meta name="description" content="A VitePress site">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/llm-study/assets/style.D1XloW-X.css" as="style">
    <link rel="preload stylesheet" href="/llm-study/vp-icons.css" as="style">
    
    <script type="module" src="/llm-study/assets/app.C8K0wDnH.js"></script>
    <link rel="preload" href="/llm-study/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/llm-study/assets/chunks/theme.BY3UI_tb.js">
    <link rel="modulepreload" href="/llm-study/assets/chunks/framework.OqxnQCTf.js">
    <link rel="modulepreload" href="/llm-study/assets/docs_深度学习_李宏毅_2023年_2024年_5.Transformer.md.DYa_PQr0.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-ba1e04ff><!--[--><!--]--><!--[--><span tabindex="-1" data-v-97ea0100></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-97ea0100> Skip to content </a><!--]--><!----><header class="VPNav" data-v-ba1e04ff data-v-b5170f91><div class="VPNavBar" data-v-b5170f91 data-v-c5a70330><div class="wrapper" data-v-c5a70330><div class="container" data-v-c5a70330><div class="title" data-v-c5a70330><div class="VPNavBarTitle has-sidebar" data-v-c5a70330 data-v-c6706323><a class="title" href="/llm-study/" data-v-c6706323><!--[--><!--]--><!----><span data-v-c6706323>大模型相关学习电子书</span><!--[--><!--]--></a></div></div><div class="content" data-v-c5a70330><div class="content-body" data-v-c5a70330><!--[--><!--]--><div class="VPNavBarSearch search" data-v-c5a70330><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-c5a70330 data-v-a30a5015><span id="main-nav-aria-label" class="visually-hidden" data-v-a30a5015> Main Navigation </span><!--[--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://github.com/wbccb/llm-study" target="_blank" rel="noreferrer" tabindex="0" data-v-a30a5015 data-v-11bf523d><!--[--><span data-v-11bf523d>Home</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-c5a70330 data-v-6f47f4f1><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6f47f4f1 data-v-94ce7627 data-v-ef45cb02><span class="check" data-v-ef45cb02><span class="icon" data-v-ef45cb02><!--[--><span class="vpi-sun sun" data-v-94ce7627></span><span class="vpi-moon moon" data-v-94ce7627></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-c5a70330 data-v-e81dec13 data-v-14929940><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wbccb" aria-label="github" target="_blank" rel="noopener" data-v-14929940 data-v-75a89838><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-c5a70330 data-v-2cdb8833 data-v-a8d23b86><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-a8d23b86><span class="vpi-more-horizontal icon" data-v-a8d23b86></span></button><div class="menu" data-v-a8d23b86><div class="VPMenu" data-v-a8d23b86 data-v-7cf740b3><!----><!--[--><!--[--><!----><div class="group" data-v-2cdb8833><div class="item appearance" data-v-2cdb8833><p class="label" data-v-2cdb8833>Appearance</p><div class="appearance-action" data-v-2cdb8833><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-2cdb8833 data-v-94ce7627 data-v-ef45cb02><span class="check" data-v-ef45cb02><span class="icon" data-v-ef45cb02><!--[--><span class="vpi-sun sun" data-v-94ce7627></span><span class="vpi-moon moon" data-v-94ce7627></span><!--]--></span></span></button></div></div></div><div class="group" data-v-2cdb8833><div class="item social-links" data-v-2cdb8833><div class="VPSocialLinks social-links-list" data-v-2cdb8833 data-v-14929940><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wbccb" aria-label="github" target="_blank" rel="noopener" data-v-14929940 data-v-75a89838><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-c5a70330 data-v-2f88690c><span class="container" data-v-2f88690c><span class="top" data-v-2f88690c></span><span class="middle" data-v-2f88690c></span><span class="bottom" data-v-2f88690c></span></span></button></div></div></div></div><div class="divider" data-v-c5a70330><div class="divider-line" data-v-c5a70330></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-ba1e04ff data-v-b8c7cf07><div class="container" data-v-b8c7cf07><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-b8c7cf07><span class="vpi-align-left menu-icon" data-v-b8c7cf07></span><span class="menu-text" data-v-b8c7cf07>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-b8c7cf07 data-v-239c32ad><button data-v-239c32ad>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-ba1e04ff data-v-119ab7f8><div class="curtain" data-v-119ab7f8></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-119ab7f8><span class="visually-hidden" id="sidebar-aria-label" data-v-119ab7f8> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-f81fd976><section class="VPSidebarItem level-0" data-v-f81fd976 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h2 class="text" data-v-b849b590>python</h2><!----></div><div class="items" data-v-b849b590><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/python/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>基础知识</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-f81fd976><section class="VPSidebarItem level-0" data-v-f81fd976 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h2 class="text" data-v-b849b590>深度学习</h2><!----></div><div class="items" data-v-b849b590><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>基本概念</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86/DeepSeek.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>DeepSeek</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-f81fd976><section class="VPSidebarItem level-0" data-v-f81fd976 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h2 class="text" data-v-b849b590>深度学习-李宏毅</h2><!----></div><div class="items" data-v-b849b590><!--[--><section class="VPSidebarItem level-1" data-v-b849b590 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h3 class="text" data-v-b849b590>2021年 & 2022年</h3><!----></div><div class="items" data-v-b849b590><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2021%E5%B9%B4&amp;2022%E5%B9%B4/1.%E4%BA%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>了解线性模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2021%E5%B9%B4&amp;2022%E5%B9%B4/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>机器学习框架</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1" data-v-b849b590 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h3 class="text" data-v-b849b590>2023年&2024年</h3><!----></div><div class="items" data-v-b849b590><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/1.chatGPT.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>chatGPT</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/2.%E7%94%9F%E6%88%90%E5%BC%8FAI.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>生成式AI</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/3.%E4%B8%8D%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B=%3E%E5%BC%BA%E5%8C%96%E6%A8%A1%E5%9E%8B.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>不训练模型=>强化模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%AD%A5%E9%AA%A4.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>训练模型步骤</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/5.Transformer.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>Transformer</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/6.%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B&amp;%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%89%E5%85%A8%E6%80%A7.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>评估模型能力&模型的安全性</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/7.%E7%94%9F%E6%88%90%E7%AD%96%E7%95%A5.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>生成策略</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E5%AE%8F%E6%AF%85/2023%E5%B9%B4&amp;2024%E5%B9%B4/8.Video%E7%9B%B8%E5%85%B3%E7%9A%84%E7%94%9F%E6%88%90%E5%BC%8FAI%E6%8A%80%E6%9C%AF.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>Video相关的生成式AI技术</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><div class="no-transition group" data-v-f81fd976><section class="VPSidebarItem level-0" data-v-f81fd976 data-v-b849b590><div class="item" role="button" tabindex="0" data-v-b849b590><div class="indicator" data-v-b849b590></div><h2 class="text" data-v-b849b590>知识库</h2><!----></div><div class="items" data-v-b849b590><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b849b590 data-v-b849b590><div class="item" data-v-b849b590><div class="indicator" data-v-b849b590></div><a class="VPLink link link" href="/llm-study/docs/%E7%9F%A5%E8%AF%86%E5%BA%93/NLP+%E5%A4%A7%E6%A8%A1%E5%9E%8B=%3E%E9%97%AE%E7%AD%94.html" data-v-b849b590><!--[--><p class="text" data-v-b849b590>NLP+大模型=>问答</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-ba1e04ff data-v-58605e9b><div class="VPDoc has-sidebar has-aside" data-v-58605e9b data-v-f9254922><!--[--><!--]--><div class="container" data-v-f9254922><div class="aside" data-v-f9254922><div class="aside-curtain" data-v-f9254922></div><div class="aside-container" data-v-f9254922><div class="aside-content" data-v-f9254922><div class="VPDocAside" data-v-f9254922 data-v-da6ed907><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-da6ed907 data-v-d887b119><div class="content" data-v-d887b119><div class="outline-marker" data-v-d887b119></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-d887b119>On this page</div><ul class="VPDocOutlineItem root" data-v-d887b119 data-v-43146e3a><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-da6ed907></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-f9254922><div class="content-container" data-v-f9254922><!--[--><!--]--><main class="main" data-v-f9254922><div style="position:relative;" class="vp-doc _llm-study_docs_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%E6%9D%8E%E5%AE%8F%E6%AF%85_2023%E5%B9%B4&amp;2024%E5%B9%B4_5_Transformer" data-v-f9254922><div><h1 id="transformer" tabindex="-1">Transformer <a class="header-anchor" href="#transformer" aria-label="Permalink to &quot;Transformer&quot;">​</a></h1><ol><li>文字转化为 token</li><li>理解 token：包括语意、位置、上下文 =&gt; 向量</li><li>Transform Block N：多个向量 =&gt;</li><li>Output</li></ol><h2 id="类神经网络技术使用历史" tabindex="-1">类神经网络技术使用历史 <a class="header-anchor" href="#类神经网络技术使用历史" aria-label="Permalink to &quot;类神经网络技术使用历史&quot;">​</a></h2><p><code>N-gram</code> -&gt; <code>Feed-forward Network</code> -&gt; <code>RNN</code> -&gt; <code>Transformer</code></p><h2 id="transformer核心步骤" tabindex="-1">Transformer核心步骤 <a class="header-anchor" href="#transformer核心步骤" aria-label="Permalink to &quot;Transformer核心步骤&quot;">​</a></h2><p><img src="https://github.com/user-attachments/assets/b836b98b-2c5b-4d89-9921-d63e97a85de1" alt="Image"></p><h3 id="根据输入内容生成token" tabindex="-1">根据输入内容生成token <a class="header-anchor" href="#根据输入内容生成token" aria-label="Permalink to &quot;根据输入内容生成token&quot;">​</a></h3><p>每一个大模型都有一个 <code>token list</code>，可以人工生成，也可以通过一定方式生成 <code>token list</code>，但是这些 <code>token list</code>都是大模型自己决定的，并不都相同</p><blockquote><p>可能中文中，每一个字就是一个 <code>token</code> ，在英语中，可能一个单词就是一个 <code>token</code> ，也有可能一个单词就是2个 <code>token</code>，数字也有可能一个单词就是2个 <code>token</code>，比如在 <code>GPT-3.5&amp;GPT-4</code>中，<code>1980</code>可能会被拆为 <code>198</code> 和 <code>0</code> 两个 token</p></blockquote><p><img src="https://github.com/user-attachments/assets/3b62a8b2-b210-4d06-ba7c-55819a157cf1" alt="Image"></p><h3 id="理解每一个token" tabindex="-1">理解每一个token <a class="header-anchor" href="#理解每一个token" aria-label="Permalink to &quot;理解每一个token&quot;">​</a></h3><h3 id="语意" tabindex="-1">语意 <a class="header-anchor" href="#语意" aria-label="Permalink to &quot;语意&quot;">​</a></h3><p>每一个 <code>token</code> 都可以变成 <code>向量</code>，<code>向量</code> 可以得到token之间的关联性和语意，为后面做准备</p><blockquote><p>本质就是通过一个训练好的模型得到对应的向量，但是向量是没考虑上下文的，也就是苹果（苹果手机还是苹果这种水果都是同一个向量）</p></blockquote><p><img src="https://github.com/user-attachments/assets/7f1095a2-c5fe-421f-948e-9a27f507e321" alt="Image"></p><h3 id="位置" tabindex="-1">位置 <a class="header-anchor" href="#位置" aria-label="Permalink to &quot;位置&quot;">​</a></h3><p>可以人工决定规则/训练得到一定的规则，相当于在原来加上语意的基础上 加上 位置的向量，为后面做准备</p><p><img src="https://github.com/user-attachments/assets/f99ceab2-49a0-4b20-9504-6680ae4545a4" alt="Image"></p><h3 id="上下文" tabindex="-1">上下文 <a class="header-anchor" href="#上下文" aria-label="Permalink to &quot;上下文&quot;">​</a></h3><p>根据一定的方式计算出 <code>当前token</code> 与 <code>其它token</code> 的相关性，然后加起来就是 <code>新的向量</code></p><blockquote><p>实际只会考虑 <code>当前token</code> 与 <code>前面的其它token</code> 的相关性</p></blockquote><p><img src="https://github.com/user-attachments/assets/b842bbf9-c946-4fa0-b6fc-e337fa9b46fb" alt="Image"></p><hr><p><img src="https://github.com/user-attachments/assets/2b2a2128-bb5b-4592-8789-bf0a45a08bcf" alt="Image"></p><hr><h3 id="transformer-block" tabindex="-1">Transformer Block <a class="header-anchor" href="#transformer-block" aria-label="Permalink to &quot;Transformer Block&quot;">​</a></h3><p>实际上不会只用一个 <code>Attention</code> _计算出相关性，因为可能有多种相关性！！这些向量是互相独立的</p><p><img src="https://github.com/user-attachments/assets/5dd1f7b2-c743-447c-aa13-8ba6502bfd69" alt="Image"></p><hr><p>最终多个向量需要合并为一个向量</p><p><img src="https://github.com/user-attachments/assets/85b9272b-2333-4408-9992-d6a10867e1bb" alt="Image"></p><p>上面这种流程就是 <code>Transformer Block</code></p><hr><p>但是实际是不止一个 <code>Transformer Block</code>的！</p><p><img src="https://github.com/user-attachments/assets/e8123814-bfc5-4703-947d-490a9b79ee93" alt="Image"></p><hr><h3 id="output" tabindex="-1">Output <a class="header-anchor" href="#output" aria-label="Permalink to &quot;Output&quot;">​</a></h3><p>经过多个 <code>Transformer Block</code> 的转化，最终我们得到一个输出！</p><blockquote><p>下图是一个简化过程，实际还不止下面的流程</p></blockquote><p><img src="https://github.com/user-attachments/assets/ea79342e-f234-4181-bed3-4ed966bf0946" alt="Image"></p><h3 id="其它细节总结" tabindex="-1">其它细节总结 <a class="header-anchor" href="#其它细节总结" aria-label="Permalink to &quot;其它细节总结&quot;">​</a></h3><blockquote><p>为什么我们只需要考虑 <code>当前token</code> 与 <code>前面的其它token</code> 的相关性</p></blockquote><p>因为答案是下图的一个流程，当我们生成 <code>w1</code> 时，下一次我们会将 <code>w1</code> 也作为输入去生成 <code>w2</code>，因此我们只需要考虑 <code>w1</code> 和 <code>它前面的token</code> 的相关性，因为我们后面的token都还没出来</p><blockquote><p>而对于 <code>w1</code> 前面的 token，需不需要计算跟 <code>w1</code> 的相关性呢？</p></blockquote><p>通过实践证明，<code>w1</code> 前面的 token，计不计算跟 <code>w1</code> 的相关性，其实效果都差不多，因此就直接不计算了！</p><p><img src="https://github.com/user-attachments/assets/c6d768b6-32cc-4e4d-b453-591189c94a1c" alt="Image"></p><hr><p>因此一个大模型的出现，总是会强调它能支持多长的 token，那是因为每一个 token 的增加，都需要更大算力的支持！</p><p><img src="https://github.com/user-attachments/assets/940e0898-0bcd-44e8-ae18-0ee2bd55b5b6" alt="Image"></p><h2 id="未来的研究方向-大模型每个流程的内容" tabindex="-1">未来的研究方向-大模型每个流程的内容 <a class="header-anchor" href="#未来的研究方向-大模型每个流程的内容" aria-label="Permalink to &quot;未来的研究方向-大模型每个流程的内容&quot;">​</a></h2><p><img src="https://github.com/user-attachments/assets/f4debcf5-1109-46a1-acec-9a9f4f17d198" alt="Image"></p><h3 id="分析每一个流程" tabindex="-1">分析每一个流程 <a class="header-anchor" href="#分析每一个流程" aria-label="Permalink to &quot;分析每一个流程&quot;">​</a></h3><ol><li>找出影响输出的输入：比如通过屏蔽输入的某一个字去验证对某一个输出的影响</li><li>找出影响输出的训练资料，到底是哪一篇文章或者多篇文章导致目前的输出内容（占比较大的训练资料）</li><li>分析<code>Embedding</code>有什么信息：比如有没有词性？每一个Transformer输出的<code>Embedding</code>到底是什么内容？</li></ol><blockquote><p>目的：当我们了解每一个流程蕴含的信息，我们就可以对这个模型进行底层的优化，加速推理/节省算力等等</p></blockquote><blockquote><p>依赖于将整个训练流程都开源的大模型信息 =&gt; 研究推出论文，不断深化理解复杂流程</p></blockquote><h3 id="直接问大模型" tabindex="-1">直接问大模型 <a class="header-anchor" href="#直接问大模型" aria-label="Permalink to &quot;直接问大模型&quot;">​</a></h3><ol><li>直接问影响输出的训练资料是什么</li><li>直接问得到的输出的信心概率有多少</li><li>直接问每一个输入影响输出的比重</li></ol></div></div></main><footer class="VPDocFooter" data-v-f9254922 data-v-3a03286c><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-3a03286c><span class="visually-hidden" id="doc-footer-aria-label" data-v-3a03286c>Pager</span><div class="pager" data-v-3a03286c><!----></div><div class="pager" data-v-3a03286c><a class="VPLink link pager-link next" href="/llm-study/docs/python/1.%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" data-v-3a03286c><!--[--><span class="desc" data-v-3a03286c>Next page</span><span class="title" data-v-3a03286c>基础知识</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"docs_ai_agents_readme.md\":\"CBRIdwO7\",\"docs_python_1.基础知识.md\":\"DU1EMxie\",\"docs_python_django框架-5.1_基本概念.md\":\"BUdNc-pt\",\"docs_深度学习_readme.md\":\"EsTASKxp\",\"docs_深度学习_sebastian raschka_1.理解大模型.md\":\"Sxw863VH\",\"docs_深度学习_主流大模型原理_deepseek.md\":\"v612l4xq\",\"docs_深度学习_基本概念.md\":\"CBDmzcye\",\"docs_深度学习_李宏毅_2021年_2022年_1.了解线性模型.md\":\"DMySFK1b\",\"docs_深度学习_李宏毅_2021年_2022年_2.机器学习框架.md\":\"DFt5zSjd\",\"docs_深度学习_李宏毅_2023年_2024年_1.chatgpt.md\":\"C1lsTu7H\",\"docs_深度学习_李宏毅_2023年_2024年_2.生成式ai.md\":\"CwmcMwtE\",\"docs_深度学习_李宏毅_2023年_2024年_3.不训练模型__强化模型.md\":\"DtZn0AnH\",\"docs_深度学习_李宏毅_2023年_2024年_4.训练模型步骤.md\":\"CJmMQZv3\",\"docs_深度学习_李宏毅_2023年_2024年_5.transformer.md\":\"DYa_PQr0\",\"docs_深度学习_李宏毅_2023年_2024年_6.评估模型能力_模型的安全性.md\":\"B-SkLoX0\",\"docs_深度学习_李宏毅_2023年_2024年_7.生成策略.md\":\"BEWQm0al\",\"docs_深度学习_李宏毅_2023年_2024年_8.video相关的生成式ai技术.md\":\"CdTS_AJ_\",\"docs_深度学习_李宏毅_readmd.md\":\"CSmrZeEY\",\"docs_知识库_nlp_大模型__问答.md\":\"Dnug9v_J\",\"docs_知识库_rag_ragflow源码分析_1.核心概念_文档解析_1.检索与生成的交互机制.md\":\"ClwwR1fb\",\"docs_知识库_rag_ragflow源码分析_1.核心概念_文档解析_2.文档解析技术.md\":\"DslU1_dp\",\"docs_知识库_rag_ragflow源码分析_1.核心概念_文档解析_3.文本分块算法.md\":\"DacJOtRV\",\"docs_知识库_rag_ragflow源码分析_2.向量_关键词混合检索_1.向量索引构建.md\":\"Ythes0dP\",\"docs_知识库_rag_ragflow源码分析_2.向量_关键词混合检索_2.混合检索策略.md\":\"TXGbRFcR\",\"docs_知识库_rag_ragflow源码分析_2.向量_关键词混合检索_3.重排序算法.md\":\"CGCx6Kof\",\"docs_知识库_rag_ragflow源码分析_3.集成到现有系统_1.工作流引擎.md\":\"CN7PXw_J\",\"docs_知识库_rag_ragflow源码分析_3.集成到现有系统_2.大模型适配层.md\":\"jKUvZcOw\",\"docs_知识库_rag_ragflow源码分析_3.集成到现有系统_3.权限控制系统.md\":\"BmpF1pDd\",\"docs_知识库_rag_ragflow源码分析_readme.md\":\"MPixwGrS\",\"docs_知识库_rag_ragflow源码分析_study.md\":\"C4IX3xa7\",\"docs_知识库_rag_readme.md\":\"CwfgFghX\",\"index.md\":\"BhNUKPKk\",\"projects_minirag_readmd.md\":\"CW1Ib-0F\",\"readme.md\":\"DvvCb5zT\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"大模型相关学习电子书\",\"description\":\"A VitePress site\",\"base\":\"/llm-study/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"https://github.com/wbccb/llm-study\"}],\"sidebar\":[{\"text\":\"python\",\"items\":[{\"text\":\"基础知识\",\"link\":\"/docs/python/1.基础知识.md\"}]},{\"text\":\"深度学习\",\"items\":[{\"text\":\"基本概念\",\"link\":\"/docs/深度学习/基本概念.md\"},{\"text\":\"DeepSeek\",\"link\":\"/docs/深度学习/主流大模型原理/DeepSeek.md\"}]},{\"text\":\"深度学习-李宏毅\",\"items\":[{\"text\":\"2021年 & 2022年\",\"items\":[{\"text\":\"了解线性模型\",\"link\":\"docs/深度学习/李宏毅/2021年&2022年/1.了解线性模型.md\"},{\"text\":\"机器学习框架\",\"link\":\"docs/深度学习/李宏毅/2021年&2022年/2.机器学习框架.md\"}]},{\"text\":\"2023年&2024年\",\"items\":[{\"text\":\"chatGPT\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/1.chatGPT.md\"},{\"text\":\"生成式AI\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/2.生成式AI.md\"},{\"text\":\"不训练模型=>强化模型\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/3.不训练模型=>强化模型.md\"},{\"text\":\"训练模型步骤\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/4.训练模型步骤.md\"},{\"text\":\"Transformer\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/5.Transformer.md\"},{\"text\":\"评估模型能力&模型的安全性\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/6.评估模型能力&模型的安全性.md\"},{\"text\":\"生成策略\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/7.生成策略.md\"},{\"text\":\"Video相关的生成式AI技术\",\"link\":\"docs/深度学习/李宏毅/2023年&2024年/8.Video相关的生成式AI技术.md\"}]}]},{\"text\":\"知识库\",\"items\":[{\"text\":\"NLP+大模型=>问答\",\"link\":\"docs/知识库/NLP+大模型=>问答.md\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wbccb\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>