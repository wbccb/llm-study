---
outline: [1, 6]
---

# 评估模型的能力

1. 使用人工构件的比较复杂难以回答的问题集
2. 只做选择题（也不能确定大模型是在选择 A、B、C、D还是根据答案选择A、B、C、D)

## 评判标准

1. 其它大模型进行评分
2. 人工评分

## 评判存在问题

1. 人工评判可能具备主观性，不一定正确
2. 其它大模型的评判也不一定正确，可能具备偏向性（比如偏向于大文本的回答）

## 总结

目前的评判还是有待讨论的，并不是完善的

# 模型的安全性

1. 大模型说错话：进行资料的校验（只能保证大模型输出能找到出处，不能保证出处说的就一定是对的）
2. 大模型自带偏见：可能因为训练资料的问题导致回答有一定的倾向性，比如软件工程师，女性是比较适合的！在各个阶段（预训练、微调、输出之前、输出之后）进行偏见的修正
   ![Image](https://github.com/user-attachments/assets/0411e401-7f78-4771-9143-902bcd058c64)

3. 检验一些答案是否是大模型输出的：可能会随着版本的升级而导致原来训练的校验器失效，而且能否真的能准确校验也是一个问题
   ![Image](https://github.com/user-attachments/assets/f3d24f34-1fd4-4eda-9f5f-64dfcc7f35be)

3. 突破大模型的限制，输出一些错误的数据
   ![Image](https://github.com/user-attachments/assets/4a744294-4ed3-441a-a106-e67ac4456699)

## Jailbreak
1. 使用特殊的文字触发
2. 使用一些比较少见的语言（可能它只是训练了英语的防御模式）
3. 给予冲突的指令
4. 试图说服语言模型：先跟大模型说一个虚构的故事，让它忽略我们接下来想要提问的问题