---
outline: [1, 6]
---

# 训练模型=>强化模型

## 找参数的挑战

### 训练失败

使用`一组超参数` + `训练资料` 去训练一个 `函数`，然后通过 `函数` 去验证测试数据是否符合预期来判断这个`函数`是否正确

并不是每次训练都能得到正确的 `函数`，因此需要算力！不断进行训练

> 所谓的调参数，其实就是调 `超参数`，因为最终得到的 `函数` 的参数太多了，根本不知道哪里有问题，因此每次都是换一组`超参数` 去训练

![Image](https://github.com/user-attachments/assets/c2e73344-149a-4874-b76b-62d2e402f3d9)

### 训练成果，但是测试失败（代入新的数据失败）

可能训练出来的  `函数`（模型）只是根据颜色去区分动物...因此这个  `函数` 还是失败的

![Image](https://github.com/user-attachments/assets/0341e57b-6dbe-45df-aff1-f9df167435a3)

### 优化训练结果的方法

- 增加训练资料的多样性
- 调整超参数
- 设置正确的初始参数（本质随机设置）

> 那如何才能获取比较合适的初始参数呢？请看下面的分析

## 训练的主要步骤

1. 阶段1：【自督导式学习】不需要太多人工介入获取训练资料的方式，从网络上获取大量资料进行自我训练
2. 阶段2：【督导式学习】耗费大量人力=>资料标注，使用阶段1得到的参数作为 `原始参数
> 为了避免阶段2得到的参数跟阶段1参数过于不同，可以使用一些小技巧`Adapter`在阶段1参数的基础上增加一些少量参数，使得两个阶段得到的参数相差不大！

![Image](https://github.com/user-attachments/assets/84f8f622-f813-4136-a3c9-3f6bceb7adc1)

> 在阶段2分为两条路线：打造一堆专才（使用特定领域的人工资料训练出解决特定领域的问题，比如翻译）+ 通才（使用多种多样的标注资料，涵盖多种领域）


-------
目前新的大模型的产生方式：
1. 利用开源的力量，拿到阶段1的参数作为阶段2的初始参数，直接省略预训练过程
2. 使用其它大模型（比如ChatGPT得到一些类似人工标注的资料：有输入和输出）的数据进行微调

![Image](https://github.com/user-attachments/assets/46dec504-38c8-461d-b9c2-ed5b13b874ff)


------

3. 阶段3：【增强式学习】通过收集人类的反馈，进行参数的调整（提高正确答案的概率，降低错误答案的概率）

![Image](https://github.com/user-attachments/assets/9b6c8102-c563-4811-a86f-615526eee034)

> 但是人工是非常贵的，专门给你反馈的数量还是很少的，有没有什么方法模仿人工反馈呢？

大模型直接向 `回馈模型` 进行学习（先要训练一个`回馈模型`）

> 过度向 `虚拟人类` 学习会导致准确率降低，因此目前在开发新的模式改进 `回馈模型` 进行学习的模式，但是还不成熟，比如用其它模型来评价你的模型产出，或者用同一个模型来评价你的模型产出

![Image](https://github.com/user-attachments/assets/e499fb34-ab04-4c23-a250-f41e208e34ee)

----------

【增强式学习】的难题：可以解决问题但是会伤害其他人的反馈 / 人类都无法判断对错的反馈 => 会导致整个模型走向极端

----------


注：`BERT` 模型预训练后，学习一种语言，可以同时举一反三，学会其它语言