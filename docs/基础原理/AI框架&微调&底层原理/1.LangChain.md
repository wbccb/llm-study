# RAG
1. 数据获取
2. 数据切分 
3. 数据切分后转化为向量存储
4. 根据存储的向量进行RAG检索


## RAG外部数据获取

### 本地文件解析为Document对象

Document对象：
* pageContent 文本内容，即文档对象对应的文本数据
* metadata 元数据，文本数据对应的元数据，例如 原始文档的标题、页数等信息

1. TextLoader：解析txt文件
2. PDFLoader：解析pdf文件
3. DirectoryLoader：解析多种格式的文件夹
```js
import { DirectoryLoader } from "langchain/document_loaders/fs/directory";

const loader = new DirectoryLoader(
  "./data",
  {
    ".pdf": (path) => new PDFLoader(path, { splitPages: false }),
    ".txt": (path) => new TextLoader(path),
  }
);
const docs = await loader.load();
```

### Web Loader
1. Github loader：使用`github API`获取某一个开源项目的代码
2. WebLoader：使用 `Cheerio` 用来提取和处理 html 内容，类似于 python 中的 BeautifulSoup
3. Search API：使用`SearchApiLoader`可以进行google搜索，并且会爬取每个结果的汇总和信息放在pageContent


## RAG切分数据

切分函数最核心的两个参数是 chunkSize 和 chunkOverlap
> 先设定为默认的 1000 和 200，然后使用 ChunkViz 去检查部分结果是否符合预期，然后根据人类对语意的理解去调整到一个合适的值
> 
> https://chunkviz.up.railway.app/

## RAG的向量操作类Vector store

### 存储向量
```js
const run = async () => {
  const loader = new TextLoader("../data/kong.txt");
  const docs = await loader.load();

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 100,
    chunkOverlap: 20,
  });
  const splitDocs = await splitter.splitDocuments(docs);

  const embeddings = new OpenAIEmbeddings();
  const vectorStore = await FaissStore.fromDocuments(splitDocs, embeddings);

  const directory = "../db/xxx";
  await vectorStore.save(directory);
};

run();
```

### 读取向量并提问
```js
// 读取存储的向量数据
const directory = "../db/xxx";
const embeddings = new OpenAIEmbeddings();
const vectorstore = await FaissStore.load(directory, embeddings);

// 提问
const retriever = vectorstore.asRetriever(2);
const res = await retriever.invoke("茴香豆是做什么用的");
```

返回值
```js
[
    {
        pageContent: '有喝酒的人便都看着他笑，有的叫道，“孔乙己，你脸上又添上新伤疤了！”他不回答，对柜里说，“温两碗酒，要一碟茴香豆。”便排出九文大钱。他们又故意的高声嚷道，“你一定又偷了人家的东西了！”孔乙己睁大眼睛说',
        metadata: { source: '../data/kong.txt', loc: [Object] }
    },
    {
        pageContent: '有几回，邻居孩子听得笑声，也赶热闹，围住了孔乙己。他便给他们一人一颗。孩子吃完豆，仍然不散，眼睛都望着碟子。孔乙己着了慌，伸开五指将碟子罩住，弯腰下去说道，“不多了，我已经不多了。”直起身又看一看豆',
        metadata: { source: '../data/kong.txt', loc: [Object] }
    }
]
```


## 优化RAG质量
1. MultiQueryRetriever：将用户的提问使用llm转化为多个相似的提问词，对每一个 query 调用 vector store 的 retriever，也就是，比如1个提问生成3个相似的 query ，会生成 3 * 3 共九个文档结果。 然后咱其中去重，并返回
2. Document Compressor：RAG检索出来很多内容，针对内容先进行llm预处理，根据用户提问从文档中提取出最相关的部分，并且强调不要让 LLM 去改动提取出来的部分，来避免 LLM 发挥自己的幻想改动原文，然后将提取精简后的相关文档再进行llm提问时的附带资料
3. ScoreThresholdRetriever：设置一定分数进行RAG检索出来的内容的限制，避免输出过多的内容（比如最小的相似度阈值、一次最多返回多少条数据）


# Agent
1. Function calling: `OpenAI`如何使用`Function calling`
2. Lang Smith打印信息追踪`Agent`的运行情况
3. ReAct框架

## Function calling
> `Function calling`后面OpenAI改名为`tools`

在 langchain 中使用 tools，通过 zod 减少了我们编写 schema 的繁琐

在 langchain 也可以使用 tools 对数据进行打标签和数据提取

## Lang Smith打印信息追踪

## ReAct框架:流行的agent框架-推理（reasoning）和行动（acting）
跟OpenAI相比较还是比较粗糙，但是值得学习的框架

# 示例
## 1. RAG-实现一个小说的chatbot
> projects/LangChain/1.RAG_novel

* 使用 Prompt Template 来构建可复用的 prompt 模板
* 根据私域数据的类型来对数据进行分块（splitter）
* 构建私域数据的 vector db
* 根据相似性去查询 vector db 中最相关的上下文

## 2. RAG-实现完整的chain: 自动的提问改写、数据检索、聊天记录 + 部署成API提供给第三方使用
> projects/LangChain/2.RAG_novel_pro


## 3. Agents-实现MBTI问答
> projects/LangChain/3.Agents-MBTI