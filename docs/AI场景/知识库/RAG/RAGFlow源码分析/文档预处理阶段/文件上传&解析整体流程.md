---
outline: [1, 6]
---

> 由于RAGFlow的代码更新非常频繁，因此强调本文所展示的源码和分析是基于2025/3/8 16:03的代码进行，随着时间的推移可能有所改变

# 文件上传&解析整体流程分析


## 文件上传

> api/apps/file_app.py：接口的操作文件

> api/db/services/file_service.py： 数据库的文件操作文件

遍历上传的文件，形成 blob 数据后，并且插入到数据库中

```python
file_objs = request.files.getlist('file')
for file_obj in file_objs:
    blob = file_obj.read()
    file = {
        "id": get_uuid(),
        "parent_id": last_folder.id,
        "tenant_id": current_user.id,
        "created_by": current_user.id,
        "type": filetype,
        "name": filename,
        "location": location,
        "size": len(blob),
    }
    file = FileService.insert(file)
```

## 文件解析

- 17:48:34 Task has been received.
- 17:48:38 Page(1~13): Start to parse.
- 17:48:38 Page(1~13): OCR started
- 17:48:42 Page(1~13): OCR finished (4.30s)
- 17:48:59 Page(1~13): Layout analysis (16.26s)
- 17:48:59 Page(1~13): Table analysis (0.14s)
- 17:48:59 Page(1~13): Text merged (0.00s)
- 17:48:59 Page(1~13): Finish parsing.
- 17:49:09 Page(1~13): Generate 29 chunks
- 17:49:15 Page(1~13): Embedding chunks (5.96s)
- 17:49:18 Page(1~13): Indexing done (2.88s). Task done (43.87s)
- 17:48:34 Reused previous task's chunks.

> 接下来我们将按照目前前端提示的顺序进行整体流程的分析

### 上传文件创建Task

> `handle_task()` -> `get_task()`中打印 `Task has been received`

上传完成后，点击解析按钮，触发文件解析

> web/src/hooks/document-hooks.ts

```ts
export const useRunNextDocument = () => {
  const queryClient = useQueryClient();

  const {
    data,
    isPending: loading,
    mutateAsync,
  } = useMutation({
    mutationKey: ["runDocumentByIds"],
    mutationFn: async ({
      documentIds,
      run,
      shouldDelete,
    }: {
      documentIds: string[];
      run: number;
      shouldDelete: boolean;
    }) => {
      const ret = await kbService.document_run({
        doc_ids: documentIds,
        run,
        delete: shouldDelete,
      });
      const code = get(ret, "data.code");
      if (code === 0) {
        queryClient.invalidateQueries({ queryKey: ["fetchDocumentList"] });
        message.success(i18n.t("message.operated"));
      }

      return code;
    },
  });

  return { runDocumentByIds: mutateAsync, loading, data };
};
```

最终触发 POST 方法

```ts
document_run: `${api_host}/document/run`;
```

也就是 `api/apps/document_app.py` -> `api/db/services/task_service.py` 的 `queue_tasks(doc)`处理解析

> 而这里的 `doc` 是什么呢？

```python
doc = DocumentService.get_by_id(id)

class CommonService:
    @classmethod
    @DB.connection_context()
    def get_by_id(cls, pid):
        # Get a record by ID
        # Args:
        #     pid: Record ID
        # Returns:
        #     Tuple of (success, record)
        obj = cls.model.get_or_none(cls.model.id == pid)
        if obj:
            return True, obj
        return False, None

    @classmethod
    @DB.connection_context()
    def get_or_none(cls, **kwargs):
        """Get a single record or None if not found.

        This method attempts to retrieve a single record matching the given criteria,
        returning None if no match is found instead of raising an exception.

        Args:
            **kwargs: Filter conditions as keyword arguments.

        Returns:
            Model instance or None: Matching record if found, None otherwise.
        """
        try:
            return cls.model.get(**kwargs)
        except peewee.DoesNotExist:
            return None

class DocumentService(CommonService):
    model = Document
```

`DocumentService` 继承 `CommonService`，本质就是调用了 `Document.get()`

```python
class Document(DataBaseModel):

class DataBaseModel(BaseModel):

class BaseModel(Model):
```

而 `Model` 就是 `peewee.py`：Peewee 是一个轻量级、直观且功能完备的 Python ORM（对象关系映射）库，专门用于简化数据库操作

`Document.get()` 就是从数据库中执行 `SELECT * FROM <table> WHERE <conditions> LIMIT 1`：精确匹配，要求查询条件必须对应唯一结果

#### queue_tasks()

创建并排队文档处理任务。

此函数根据文档的类型和配置为其创建处理任务。

它以不同的方式处理不同的文档类型（PDF、Excel 等），并管理任务分块和配置。它还通过检查以前完成的任务来实现任务重用优化。

参数：

- doc (dict)：包含元数据和配置的文档字典。
- bucket (str)：存储文档的存储桶名称。
- name (str)：文档的文件名。
- priority (int，可选)：任务排队的优先级（默认值为 0）。

注意：

- 对于 PDF 文档，根据配置按页面范围创建任务
- 对于 Excel 文档，按行范围创建任务
- 计算任务摘要以进行优化和重用
- 如果可用，可以重用以前的任务块

比如下面的 pdf 的相关处理，根据页数构建出不同的 `task` 存入到 `parse_task_array` 中

```python
def queue_tasks(doc: dict, bucket: str, name: str, priority: int):
   if doc["type"] == FileType.PDF.value:
        file_bin = STORAGE_IMPL.get(bucket, name)
        do_layout = doc["parser_config"].get("layout_recognize", "DeepDOC")
        pages = PdfParser.total_page_number(doc["name"], file_bin)
        page_size = doc["parser_config"].get("task_page_size", 12)
        if doc["parser_id"] == "paper":
            page_size = doc["parser_config"].get("task_page_size", 22)
        if doc["parser_id"] in ["one", "knowledge_graph"] or do_layout != "DeepDOC":
            page_size = 10 ** 9
        page_ranges = doc["parser_config"].get("pages") or [(1, 10 ** 5)]
        for s, e in page_ranges:
            s -= 1
            s = max(0, s)
            e = min(e - 1, pages)
            for p in range(s, e, page_size):
                task = new_task()
                task["from_page"] = p
                task["to_page"] = min(p + page_size, e)
                parse_task_array.append(task)


   DocumentService.update_by_id(doc["id"], {"chunk_num": ck_num})
   bulk_insert_into_db(Task, parse_task_array, True)
   DocumentService.begin2parse(doc["id"])
```

在 `task_executor.py` 的 `main()`中

**初始化阶段**

- 创建主 nursery 作为任务容器，立即启动 report_status 监控任务（例如周期上报系统指标）。

**任务处理循环**

- 通过 while True 持续监听新任务（如消息队列消费）
- 每次迭代前需通过 task_limiter 获取执行配额（避免突发流量冲击）
- 每次成功获取配额后，启动 handle_task 处理实际业务逻辑

**终止条件**

- 外部取消信号（如 Ctrl+C 触发 trio.Cancelled 异常）
- report_status 或 handle_task 内部抛出未捕获异常

```python
async def main():
    async with trio.open_nursery() as nursery:
        nursery.start_soon(report_status)
        while True:
            async with task_limiter:
                nursery.start_soon(handle_task)

async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        await do_handle_task(task)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    #...
    redis_msg.ack()
```

在 `handle_task()` 中，我们使用 `task = await collect()` 获取任务，如下面代码所示，本质就是 `TaskService.get_task`

```python
async def collect():
    #...
    task = TaskService.get_task(msg["id"])
    if task:
        _, doc = DocumentService.get_by_id(task["doc_id"])
    return redis_msg, task
```

从 `get_task()` 中，我们可以得到第一步的打印信息 `Task has been received`

```python
@classmethod
@DB.connection_context()
def get_task(cls, task_id):
    fields = [
        #...
    ]
    docs = (
        cls.model.select(*fields)
            .join(Document, on=(cls.model.doc_id == Document.id))
            .join(Knowledgebase, on=(Document.kb_id == Knowledgebase.id))
            .join(Tenant, on=(Knowledgebase.tenant_id == Tenant.id))
            .where(cls.model.id == task_id)
    )
    docs = list(docs.dicts())
    #...
    msg = f"\n{datetime.now().strftime('%H:%M:%S')} Task has been received."
    cls.model.update(
        progress_msg=cls.model.progress_msg + msg,
        progress=prog,
        retry_count=docs[0]["retry_count"] + 1,
    ).where(cls.model.id == docs[0]["id"]).execute()
    #...
    return docs[0]
```

#### 总结

- 我们上传文件并且点击解析按钮后，会创建对应的任务存入对应的数据库
- 然后不断轮询触发任务的处理
- 当我们获取到当前的任务时，会打印出 `Task has been received`

### Task执行do_handle_task()

> 在 `build_chunks()` -> ``中打印 `Start to parse`

当我们拿到对应的 task 后，我们会触发 `do_handle_task()`

```python
async def handle_task():
    global DONE_TASKS, FAILED_TASKS
    redis_msg, task = await collect()
    try:
        logging.info(f"handle_task begin for task {json.dumps(task)}")
        await do_handle_task(task)
        logging.info(f"handle_task done for task {json.dumps(task)}")
    #...
    redis_msg.ack()
```


在 `do_handle_task()`中，我们先进行了一些数据的初始化、引擎的检查以及是否有 task_canceled

然后进行 `embedding_model` 的加载验证，知识库的初始化

根据不同条件，主要分为
- `task.get("task_type", "") == "raptor"`：执行 `run_raptor(task, chat_model, embedding_model, vector_size, progress_callback)`
- `task.get("task_type", "") == "graphrag"`：执行 `run_graphrag(task, task_language, with_resolution, with_community, chat_model, embedding_model, progress_callback)`
- 标准处理chunk处理方式：`build_chunks()` + `embedding()` + 批量插入chunks到 `Elasticsearch/Infinity`

> RAPTOR 一种基于树的RAG方法，准确率提高 20%
>
> RAG 是当前使用LLM的标准方法，大多数现有方法仅从检索语料库中检索短的连续块，限制了对整个文档上下文的整体理解。
>
> 最近，一种名为 RAPTOR （Recursive Abstractive Processing for Tree-Organized Retrieval）方法提出来，该方法核心思想是将doc构建为一棵树，然后逐层递归的查询


> GraphRAG是微软提出来的RAG+graph框架，使用 LLM 生成的知识图谱，在对复杂信息进行文档分析时显著提高问答性能。
> 
> 基础RAG的缺点：
>
> - 基础 RAG 难以将各个点连接起来。当回答问题需要通过共享属性遍历不同的信息片段以提供新的综合见解时，就会发生这种情况。
> - 当被要求全面理解大型数据集合甚至单个大型文档中的总结语义概念时，基础 RAG 的表现不佳。（比如让模型回答未训练过的《西游记》一书的内容，若问孙悟空被天庭封为什么官，那么RAG到相似的文档即可回答。若问西游戏整本书在讲什么，那么RAG的效果就很差，取任何chunk都没办法总结全书的内容）



> 接下来我们将针对 `build_chunks()` 和 `embedding()` 进行详细的分析



#### build_chunks

主要分为：
- 分块器初始化
- 异步获取文件内容
- 同步分块操作转为异步任务
- chunks生成完成后，执行一系列工作
  - 关键词生成
  - 问题生成
  - 标签生成


##### 分块器初始化

从 `FACTORY` 中根据类型获取对应的分块器 `chunker`，支持动态适配不同的分块策略

```python
chunker = FACTORY[task["parser_id"].lower()]
```

```python
FACTORY = {
    "general": naive,
    ParserType.NAIVE.value: naive,
    ParserType.PAPER.value: paper,
    ParserType.BOOK.value: book,
    ParserType.PRESENTATION.value: presentation,
    ParserType.MANUAL.value: manual,
    ParserType.LAWS.value: laws,
    ParserType.QA.value: qa,
    ParserType.TABLE.value: table,
    ParserType.RESUME.value: resume,
    ParserType.PICTURE.value: picture,
    ParserType.ONE.value: one,
    ParserType.AUDIO.value: audio,
    ParserType.EMAIL.value: email,
    ParserType.KG.value: naive,
    ParserType.TAG.value: tag
}
```

##### 异步获取文件内容

通过 `File2DocumentService` 获取文件存储地址（MinIO)，使用 `get_storage_binary()` 获取该文件的二进制数据

```python
bucket, name = File2DocumentService.get_storage_address(doc_id=task["doc_id"])
binary = await get_storage_binary(bucket, name)
logging.info("From minio({}) {}/{}".format(timer() - st, task["location"], task["name"]))
```

##### 同步分块操作转为异步任务

使用 `chunk_limiter` 限制并发量，通过 `trio.to_thread.run_sync` 将同步方法 `chunker.chunk()` 包装为异步任务，分块参数包括
- 页码范围
- 语言配置
- ....

```python
async with chunk_limiter:
    cks = await trio.to_thread.run_sync(lambda: chunker.chunk(task["name"], binary=binary, from_page=task["from_page"],
                        to_page=task["to_page"], lang=task["language"], callback=progress_callback,
                        kb_id=task["kb_id"], parser_config=task["parser_config"], tenant_id=task["tenant_id"]))
logging.info("Chunking({}) {}/{} done".format(timer() - st, task["location"], task["name"]))
```

##### 关键词生成

检测缓存，如果没有缓存，则调用大模型从所有分块中进行关键词的提取，然后存入缓存中

```python
if task["parser_config"].get("auto_keywords", 0):
    cached = await trio.to_thread.run_sync(lambda: keyword_extraction(chat_mdl, d["content_with_weight"], topn))

def keyword_extraction(chat_mdl, content, topn=3):
    prompt = f"""
Role: You're a text analyzer.
Task: extract the most important keywords/phrases of a given piece of text content.
Requirements:
  - Summarize the text content, and give top {topn} important keywords/phrases.
  - The keywords MUST be in language of the given piece of text content.
  - The keywords are delimited by ENGLISH COMMA.
  - Keywords ONLY in output.

### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.2})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        return ""
    return kwd
```

##### 问题生成

跟上面 `关键词生成` 类似，调用大模型进行问题的生成，然后存入缓存中

```python
if task["parser_config"].get("auto_questions", 0):
    cached = await trio.to_thread.run_sync(lambda: question_proposal(chat_mdl, d["content_with_weight"], topn))


def question_proposal(chat_mdl, content, topn=3):
    prompt = f"""
Role: You're a text analyzer.
Task:  propose {topn} questions about a given piece of text content.
Requirements:
  - Understand and summarize the text content, and propose top {topn} important questions.
  - The questions SHOULD NOT have overlapping meanings.
  - The questions SHOULD cover the main content of the text as much as possible.
  - The questions MUST be in language of the given piece of text content.
  - One question per line.
  - Question ONLY in output.

### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.2})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        return ""
    return kwd

```

##### 标签生成

跟上面 `关键词生成` 类似，调用大模型进行标签的生成，然后存入缓存中



```python
if task["kb_parser_config"].get("tag_kb_ids", []):
    cached = await trio.to_thread.run_sync(lambda: content_tagging(chat_mdl, d["content_with_weight"], all_tags, picked_examples, topn=topn_tags))


def content_tagging(chat_mdl, content, all_tags, examples, topn=3):
    prompt = f"""
Role: You're a text analyzer.

Task: Tag (put on some labels) to a given piece of text content based on the examples and the entire tag set.

Steps::
  - Comprehend the tag/label set.
  - Comprehend examples which all consist of both text content and assigned tags with relevance score in format of JSON.
  - Summarize the text content, and tag it with top {topn} most relevant tags from the set of tag/label and the corresponding relevance score.

Requirements
  - The tags MUST be from the tag set.
  - The output MUST be in JSON format only, the key is tag and the value is its relevance score.
  - The relevance score must be range from 1 to 10.
  - Keywords ONLY in output.

# TAG SET
{", ".join(all_tags)}

"""
    for i, ex in enumerate(examples):
        prompt += """
# Examples {}
### Text Content
{}

Output:
{}

        """.format(i, ex["content"], json.dumps(ex[TAG_FLD], indent=2, ensure_ascii=False))

    prompt += f"""
# Real Data
### Text Content
{content}

"""
    msg = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": "Output: "}
    ]
    _, msg = message_fit_in(msg, chat_mdl.max_length)
    kwd = chat_mdl.chat(prompt, msg[1:], {"temperature": 0.5})
    if isinstance(kwd, tuple):
        kwd = kwd[0]
    kwd = re.sub(r"<think>.*</think>", "", kwd, flags=re.DOTALL)
    if kwd.find("**ERROR**") >= 0:
        raise Exception(kwd)

    try:
        return json_repair.loads(kwd)
    except json_repair.JSONDecodeError:
        try:
            result = kwd.replace(prompt[:-1], '').replace('user', '').replace('model', '').strip()
            result = '{' + result.split('{')[1].split('}')[0] + '}'
            return json_repair.loads(result)
        except Exception as e:
            logging.exception(f"JSON parsing error: {result} -> {e}")
            raise e

```


> 在 `do_handle_task()`执行完成 `build_chunks()`后
> 会打印：
> - Generate 29 chunks
>
> 然后对生成的 `chunks` 进行向量化！也就是触发 `embedding()`

#### embedding

在 `do_handle_task()` 中，先使用 `build_chunks()` 构建出多个 `chunks`，然后使用 `embedding()` 转化为向量进行存储


遍历所有文档`for d in docs:`
- 提取标题：`tts.append(d.get("docnm_kwd", "Title"))`
- 提取内容：`c = "\n".join(d.get("question_kwd", []))`
- 对内容进行清洗，去除HTML表格标签：`c = re.sub(r"</?(table|td|caption|tr|th)( [^<>]{0,12})?>", " ", c)`
- 执行到这一步，如果内容 `c` 为空，则设置为 `c = "None"`
- 将内容添加到 `cnts` 中：`cnts.append(c)`


> mdl: embedding_model = LLMBundle(task_tenant_id, LLMType.EMBEDDING, llm_name=task_embedding_id, lang=task_language)

对标题进行 `mdl.encode` 生成对应的向量 `vts` 和 token计数 `c`，然后使用 `np.concatenate` 复制多次（与目前文档数量一样），对所有文档都是用第一个标题的向量，然后更新总体的 `tk_count` （token计数）
```python
vts, c = await trio.to_thread.run_sync(lambda: mdl.encode(tts[0: 1]))
tts = np.concatenate([vts for _ in range(len(tts))], axis=0)
tk_count += c
```

对内容进行批量编码为向量，按照 `batch_size` 的长度分批处理，不断调用 `callback` 更新当前进度
> 如果每一次处理文本超过了当前模型的 `mdl.max_length-10`，则进行截断
然后将转化的向量存放在 `cnts_`中，最终 `cnts = cnts_`

```python 
cnts_ = np.array([])
for i in range(0, len(cnts), batch_size):
    vts, c = await trio.to_thread.run_sync(lambda: mdl.encode([truncate(c, mdl.max_length-10) for c in cnts[i: i + batch_size]]))
    if len(cnts_) == 0:
        cnts_ = vts
    else:
        cnts_ = np.concatenate((cnts_, vts), axis=0)
    tk_count += c
    callback(prog=0.7 + 0.2 * (i + 1) / len(cnts), msg="")
cnts = cnts_

```

对标题和内容进行加权向量计算，标题为 `0.1`，内容为 `0.9`，如果标题和内容向量的长度不一样（理论应该不会发生），则只使用内容向量
```python
title_w = float(parser_config.get("filename_embd_weight", 0.1))
vects = (title_w * tts + (1 - title_w) *
            cnts) if len(tts) == len(cnts) else cnts
```

最终将计算好的向量放入到 docs 的某一个字段中
```python
for i, d in enumerate(docs):
    v = vects[i].tolist()
    vector_size = len(v)
    d["q_%d_vec" % len(v)] = v
```

返回当前的 token数量 和 向量纬度 vector_size
```python
return tk_count, vector_size
```


> 回到 `do_handle_task()`，我们执行完成 `build_chunks()` -> `embedding()`后，我们打印出 `Embedding chunks`


#### embedding()->chunks存储

分批（`es_bulk_size`=4）将 4个chunks 插入到 `es/Infinity` 中
> 每更新128个chunks，则调用一次callback更新当前进度

```python
es_bulk_size = 4
for b in range(0, len(chunks), es_bulk_size):
    doc_store_result = await trio.to_thread.run_sync(lambda: settings.docStoreConn.insert(chunks[b:b + es_bulk_size], search.index_name(task_tenant_id), task_dataset_id))
    if b % 128 == 0:
        progress_callback(prog=0.8 + 0.1 * (b + 1) / len(chunks), msg="")
    chunk_ids = [chunk["id"] for chunk in chunks[:b + es_bulk_size]]
    chunk_ids_str = " ".join(chunk_ids)
    try:
        TaskService.update_chunk_ids(task["id"], chunk_ids_str)
    except DoesNotExist:
        #...
```

```python
lower_case_doc_engine = DOC_ENGINE.lower()
if lower_case_doc_engine == "elasticsearch":
    docStoreConn = rag.utils.es_conn.ESConnection()
elif lower_case_doc_engine == "infinity":
    docStoreConn = rag.utils.infinity_conn.InfinityConnection()
else:
    raise Exception(f"Not supported doc engine: {DOC_ENGINE}")
```


更新总体的chunk数量
```python
DocumentService.increment_chunk_num(task_doc_id, task_dataset_id, token_count, chunk_count, 0)
```

打印出 `Indexing done & Task done`









