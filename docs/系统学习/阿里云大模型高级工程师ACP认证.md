# 阿里云大模型高级工程师ACP认证
> https://github.com/AlibabaCloudDocs/aliyun_acp_learning


## 2.1 大模型基础知识

大模型的问答工作流程
- 输入文本Token化
- Token向量化
- 大模型推理
- 输出Token：影响大模型生成内容的参数`temperature`和`top_p`
- 输出文本


### 2.1.1 temperature 调整候选Token集合的概率分布

- 想要明确的答案：temperature设置越小
- 想要创意多样的答案：temperature设置越大


### 2.1.2 top_p 控制候选Token集合的采样范围

- top_p越大：候选范围越广，适合多样性答案
- top_p越小：候选范围越小，适合代码生成需要明确答案的场景
- top_p取极小值：理论上模型只选择概率最高的token，输出非常稳定

### 2.1.3 上下文工程（Context Engineering）

- RAG
- Prompt
- Tool
- Memory


## 2.3 Prompt

- 任务目标：明确要求大模型完成什么任务，让大模型专注具体目标
- 上下文：任务的背景信息，比如操作流程、任务场景等，明确大模型理解讨论的范围
- 角色：大模型扮演的角色，或者强调大模型应该使用的语气、写作风格等，明确大模型回应的预期情感
- 受众：明确大模型针对的特定受众，约束大模型的应答风格
- 样例：让大模型参考的具体案例，大模型会从中抽象出实现方案、需要注意的具体格式等信息
- 输出格式：明确指定输出的格式、输出类型、枚举值的范围。通常也会明确指出不需要输出的内容和不期望的信息，可以结合样例来进一步明确输出的格式和输出方法


## 2.4 RAG自动化评测


评估纬度：
- 召回质量：RAG系统是否检索到了正确且相关的文档片段
- 答案忠实度：生成的答案是否完全基于检索到的上下文，没有幻觉
- 答案相关性：生成的答案是否准确回答了用户的问题
- 上下文利用率/效率：大模型是否有效利用了所有提供给它的上下文信息

### 2.4.1 流行的RAG评测框架

#### 2.4.1.1 Ragas

在评测时，Ragas 会调用一个大模型作为评测专家来阅读你的问题、RAG检索到的上下文和生成的答案，然后根据预设的指标给出分数。Ragas的评估指标高度契合RAG系统的痛点，主要包括：
- 整体回答质量的评估：
  - Answer Correctness，用于评估 RAG 应用生成答案的准确度。
- 生成环节的评估：
  - Answer Relevancy，用于评估 RAG 应用生成的答案是否与问题相关。
  - Faithfulness，用于评估 RAG 应用生成的答案和检索到的参考资料的事实一致性。
- 召回阶段的评估：
  - Context Precision，用于评估 contexts 中与准确答案相关的条目是否排名靠前、占比高（信噪比）。
  - Context Recall，用于评估有多少相关参考资料被检索到，越高的得分意味着更少的相关参考资料被遗漏。


#### 2.4.1.2 TruLens

专注于评估的`可观测性`
- 告诉你RAG哪里可能有问题
- 还能帮你追溯RAG整个运行过程，包括每次运行的输入、输出、中间步骤、调用的大模型、检索召回的上下文等等；并提供一系列`反馈函数`来自动化评估这些运行


可以和`langChain`、`LlamaIndex`无缝集成，并提供一套可视化工具，展示每次RAG调用的详细过程

提供细粒度的反馈函数，从多个角度评估RAG性能，帮助你快速定位问题，评估指标包括：
- Context Relevance：评估召回的知识是否跟问题相关。
- Groundedness：生成的答案是否是来自于检索召回的知识
- Answer Relevance：生成的答案是否与问题相关

#### 2.4.1.3 DeepEval

将传统的软件开发测试理念引入到RAG应用中，在RAG功能开发前就编写评估测试，采用单元测试和测试驱动开发TDD的思想，提供一个专门的测试框架，让你能够编写单元测试一样，为RAG系统创建LLM评估测试

支持为每个测试用例设置明确的通过/失败的阀值，轻松集成到你的CI/CD流程，从而实现自动化回归测试


#### 2.4.1.4 其他的开源评测工具

比如LlamaIndex和LangChain这些主流RAG开发框架本身也有内置评估工具

#### 2.4.1.5 自定义评测框架


在某些特定场景下，你可能需要更灵活、更贴合业务的评估方式，这时你也可以选择自定义评测框架

> 请注意：领域专家的深度参与是评测系统乃至AI应用成功的关键。使用自动化评测框架并非要让机器彻底取代人工判断，而是旨在为评测提效。前面介绍的许多自动化框架（如Ragas）会利用大模型来充当“评委”，对RAG表现进行初步判断。然而，只有领域专家才能提供那些最宝贵的、真正反映业务需求的高价值“正确答案”（Ground Truth），并对用户提问和RAG回答的正确性进行权威性的审定。


案例一：当用户询问“报销的流程是什么？”，你们的业务专家可能会要求“好的报销答案，必须明确包含报销单链接和二级主管审批信息。”或者“好的答案，必须引导用户到费用系统提交报销申请。”，这样才能满足“流程合规”的要求。


#### 2.4.1.6 评测示例讲解

> https://github.com/AlibabaCloudDocs/aliyun_acp_learning/blob/main/%E5%A4%A7%E6%A8%A1%E5%9E%8BACP%E8%AE%A4%E8%AF%81%E6%95%99%E7%A8%8B/p2_%E6%9E%84%E9%80%A0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/2_4_%E8%87%AA%E5%8A%A8%E5%8C%96%E8%AF%84%E6%B5%8B%E7%AD%94%E7%96%91%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%A1%A8%E7%8E%B0.ipynb


## 2.5 评测不理想->优化RAG具体步骤

### 2.5.1. 切片方式的优化
- 如果你刚开始接触 RAG，建议先使用默认的句子切片方法，它在大多数场景下都能提供不错的效果
- 当你发现检索结果不够理想时，可以尝试：
  - 处理长文档且需要保持上下文？试试句子窗口切片
  - 文档逻辑性强、内容专业？语义切片可能会有帮助
  - 模型总是报 Token 超限？Token 切片可以帮你精确控制
  - 处理 Markdown 文档？别忘了有专门的 Markdown 切片

### 2.5.2. Embedding模型
- 新版本的Embedding模型通常能带来更好的效果（如text-embedding-v3比v2表现更好）
- 在实践中，单纯升级Embedding模型就可能显著提升检索质量

### 2.5.3. Embedding存储数据库
- 开发测试时使用内存向量存储
- 小规模应用可以使用本地向量数据库
- 生产环境推荐使用云服务，可根据具体需求选择合适的服务类型

### 2.5.4. 检索召回阶段
- 在执行检索前，很多用户问题描述是不完整、甚至有歧义的，你需要想办法还原用户真实意图，以便提升检索效果。
- 在执行检索后，你可能会发现存在一些无关的信息，需要想办法减少无关信息，避免干扰下一步的答案生成。

#### 2.5.4.1 问题改写
- 使用大模型扩充用户问题
- 将单一查询改写为多步骤查询
- 用假设文档来增强检索（HyDE）

> 在向量检索的基础上，我们还可以添加标签过滤来提升检索精度。这种方式类似于图书馆既有书名检索，又有分类编号系统，能让检索更精准

#### 2.5.4.2 标签提取有两个关键场景
- 建立索引时，从文档切片中提取结构化标签
- 检索时，从用户问题中提取对应的标签进行过滤

当我们建立索引时，可以将这些标签与文档切片一起存储。这样在检索时，比如用户问"张伟是哪个部门的"，我们可以：
- 从问题中提取人名标签 {"key": "人名", "value": "张伟"}
- 先用标签过滤出所有包含"张伟"的文档切片
- 再用向量相似度检索找出最相关的内容

这种"标签过滤+向量检索"的组合方式，能大幅提升检索的准确性。特别是在处理结构化程度较高的企业文档时，这个方法效果更好。

#### 2.5.4.3 重排序

增加召回数量，利用重排序获取分数更高的片段

#### 2.5.4.4 生成答案阶段

即使上面几个步骤都优化好了，你仍然会遇到：
- 没有检索到相关信息，大模型捏造答案。
- 检索到了相关信息，但是大模型没有按照要求生成答案。
- 检索到了相关信息，大模型也给出了答案，但是你希望 AI 给出更全面的答案。


这个能做的是：
- 选择合适的大模型
- 充分优化prompt内容
- 调整大模型参数
- 调优大模型


> GraphRAG 技术巧妙地结合了检索增强生成（RAG）和查询聚焦摘要（QFS）的优点，为处理大规模文本数据提供了一个强大的解决方案。它把两种技术的特长融合在一起：RAG 擅长找出精确的细节信息，而 QFS 则更善于理解和总结文章的整体内容。通过这种结合，GraphRAG 既能准确回答具体问题，又能处理需要深入理解的复杂查询，特别适合用来构建智能问答系统。
如果你想深入了解如何实际运用 GraphRAG，可以参考 LlamaIndex 提供的详细教程：使用 LlamaIndex 构建 GraphRAG 应用


## 2.6 Agent

智能体的工作原理：
1. 工具模块
2. 记忆模块
3. 计划能力
4. 行动能力


### 2.6.1 意图识别

对用户的问题进行意图识别后，你就可以让答疑机器人先识别问题的类型，再使用不同的提示词和工作流程来回答问题
- 节省资源：对于检查文档错误的问题，大模型其实可以直接回复，并不需要检索参考资料，之前的实现存在资源浪费。
- 避免误解：之前的实现每次会检索参考资料，这些被召回的相关文本段可能会干扰大模型理解问题，导致答非所问。

```python
def ask_llm_route(question):
    question_type = get_question_type(question)
    print(f'问题：{question}\n类型：{question_type}')
  
    reviewer_prompt = """
    【角色背景】
    你是文档纠错专家，负责找出文档中或网页内容的明显错误
    【任务要求】
    - 你需要言简意赅的回复。
    - 如果没有明显问题，请直接回复没有问题\n
    【输入如下】\n"""
  
    translator_prompt = """
   【任务要求】
    你是一名翻译专家，你要识别不同语言的文本，并翻译为中文。
    【输入如下】\n"""

    if question_type == '文档审查':
        return llm.invoke(reviewer_prompt + question)
    elif question_type == '公司内部文档查询':
        return rag.ask(question, query_engine=query_engine)
    elif question_type == '内容翻译':
        return llm.invoke(translator_prompt + question)
    else:
        return "未能识别问题类型，请重新输入。"

query_engine =rag.create_query_engine(index=rag.load_index())
```

### 2.6.2 多智能体

Multi-Agent系统有多种设计思路，本教程将介绍一个由
- 一个Planner Agent：根据用户的输入内容，选择要将任务分发给哪个Agent或Agent组合完成任务
- 若干个负责执行工具函数的Agent：根据Planner Agent分发的任务，执行属于自己的工具函数
- 一个Summary Agent组成的Multi-Agent系统：根据用户的输入，以及执行工具函数的Agent的输出，生成总结并返回给用户

组成的多智能体系统

### 2.7 微调

